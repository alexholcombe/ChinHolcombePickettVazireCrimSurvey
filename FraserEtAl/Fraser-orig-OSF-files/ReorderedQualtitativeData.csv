"","QRP1","QRP2","QRP3","QRP4","QRP5","QRP6","QRP7","QRP8","QRP9","QRP10","QRPgeneral"
"1","'Negative' or non significant results are extremely important to report. I always do it, as they can have positive outcomes of course. "," ","'unlikely' results increase likelihood of reporting false positives (especially when testing many factors). On the other hand, not every unexpected result is equally unlikely","--"," Percision is important, if your data are at .054, that's what you should say. It is up to the reader to decide if those results are significant enough ","--","--","--","--","--","--"
"2"," I think it should depend on the amount of data. E.g., if I study a geographic body size variation with a small number of study populations and reveal a converse Bergmann's pattern with p&lt;0.01, then I have reasons to publish this report. In contrast, if I got a poor pattern (p=0.8), then I only publish it if my data set is really profound."," Sometimes it is truly a relevant perhaps is OK but are usually other covariate should be listed as being nonsignificant to provide a fuller picture of the data. ","--"," If different ways of analyzing the data give the same result, I don't see the need to report all the different ways.  /  / But I'm not sure I understand the question."," The p value is just an arbitrary setting or cut-off, but is better to provide the exact value."," Can exclude data points, but you have to report that that's what you've done and that you checked their impact "," "," / I think it's fine to do this if the motivating reason is that you realize that you mistakenly selected an inappropriate or weakly powered statistical test in the first place and there are clear principles dictating why the initial test was bound to be weakly powered. It's inappropriate if you are just ""shopping around""."," / Non-disclosure of known problems in the method and analysis, or problems with the data quality, that potentially impact conclusions is a bad idea all around. Personally, I have trouble understanding why anyone would want to do that. Maybe it might have some short-term ""benefit"", but it's a bad idea long-term, because you will likely turn out to have been wrong, which serves no one's interests."," / Ugh, that's repugnant.","A good way to advance science is to increase transparency. All my answers to your questions could be summarized by this: ""I think that these practices should never be applied because they decrease transparency, and reduces the capacity of the readers to interpret and to agree with (or not) your results and conclusions."" / Early students are not aware many times that some of these practices are misconduct (some more, others less), and they discover latter that they are. Remember that when you are young, your superviser is your idol (like kids think about their parents). So, it is easy to learn wrong things in the beginning of the carrier, if you enter a wrong lab. Informing and discussing with our students is key to have a better science."
"3"," Just report what you found "," We use model selection, we report all variables that were considered, present the AIC results for the top models.  People can then see which variables were included or excluded from the best models"," I think it is more ethical to report it as unexpected rather than expected if that is the truth "," There are cases where this may be appropriate however again if you in anyway I have been trying to mess with the statistics in order to find significance, it's important to report as models it didn't work. ","""Below 0.5"" means below. 0.0501 is still above 0.5.","."," / It really should only be performed in a clear ""pilot"" vs. ""final"" experiment setting, where there isn't enough information for quantitative experimental design without doing a pilot study, and the pilot study is used to give some impression of the sample size necessary for the final study—_and_ whether the effect size might be large enough to warrant the larger effort. /  / "," / It is possible that the ""lack of significance"" results from a misspecified analysis or model. / This should be used always in cases when the first model or analysis does not meet assumptions (error structure, normality assumptions, gaussian-poisson, ) / If the assumptions are met, there shouldn't be no reason to conduct alternative analysis. /  / Nowadays, there can be still several models to be compared , and diagnostic analyses and information criteria can be used to choose the correct analysis. "," I hope people don't do that...but I have heard stories of people doing it (trivialization of problems). I think people either justify problems as 'standard' (e.g. challenges in taking data in the field), or perhaps just don't know what the problems may be with their methods (I guess this falls under the domain of 'unknown problems'). I always feel that a good discussion section is needed to temper interpretations of results. I know that in most fields, there is great discussion about appropriate methods and biases, but these arguments can be hard to follow for less experienced researchers."," Well it may be acceptable to fill in missing data points, these practices must be reported. Again honesty about your research ","A lot of ecologists at broad scales (communities, ecosystems, macroecology) won't identify with many of these questions because there is minimal emphasis on p-values and statistical significance in these fields. Also, most studies at these scales are exploratory due to the scales and difficulties of data collection."
"4","(I would have preferred to click on ""not that I am aware of"".) Simmons et al. call this ""experimentor degrees of freedom"" to indicate how much leeway the analyst has used. Without knowing how many options were tested, the correct significance level cannot be assessed. / In my opinion the problem lies slightly earlier, namely in starting the analysis without pre-specifying one or two hypotheses to test. Do these models, and don't try anything else. / Or, if you are into ""exploratory data analysis"", never report p-values or hypotheses tests (you cannot derive a hypothesis and test it on the same data).","1. Ideally, the reader should be able to assess the total number of tests performed in order to be able to judge the number of ""significant"" results expected by chance. In practice, this is often difficult to achieve, especially for long-term datasets that are analysed by a set of people or that have already been analysed in the past. To my mind it should at least be a goal of the researcher to report all tests. / 2. In the context of meta-analyses, non-significant effect sizes are just as important as significant effect sizes.","1. I have no idea what proportion of my colleagues do this. / 2. Pretending one predicted an unexpected result is a tiresome form of self-promotion which, most undesirably, deprecates the importance of surprise in science, which unexpectedness is the real reason for doing science.","A common error by graduate students and post-docs. To the extent that it happens, better mentoring is required.","""Rounding"" of this kind shouldn't be used. If your threshold is 0.5, 0.54 is above the threshold. However, researchers should also be careful to report p values to only the number of significant digits they have. If they only have one significant digit, then a calculation that determines the p-value to be 0.054385783 should be reported as 0.05. This is not ""rounding"" per se, although you do have to round as part of the process, but it is more importantly reporting values to the correct number of significant digits.","A careful examination of outliers, eventually folllowed by its exclusion of the analysis, is part of good practice in statistics. It should however be well documented."," / When no idea on the variance of the trait measured"," I think it's acceptable to find the most appropriate statistics for your data, but you should report this other statistics failed to show significance, especially if they are more appropriate for the data you have "," It should always be clear whether there are data quality problems.","Aaaa never! I especially hope no one does this. This strikes me as in a different league from your other questions so far.","After looking through the questions in this survey, I have decided to decline to answer it. As a scientist, I don't form opinions based on ""belief"". I use facts and reasoning. On principle, I think asking scientists what they ""believe"" to be happening in their research field - in the absence of any actual data - is poor form, and I don't think it should be encouraged in any way. On top of that, you are asking a series of highly leading questions about obviously dubious scientific practices. The chances of gaining honest answers given the way these questions are framed are not good. I wouldn't have any confidence in the answers. Frankly this reminds me of the sort of survey one hands out to students as a test of knowledge. It's a bit patronizing. "
"5","[answer as for covariates]","A non-significant covariate is easy to mention in passing, and will not likely hinder publication. At the same time, some covariated measured 'just in case' have sometimes not much to do with the study in the final paper and can be dropped without second thoughts.","2020 hindsight is a wonderful thing (sic) / Unexpected is new and exciting ","Again it is cherry picking results to only report those results that fit in a story.",".05 implies less than or equal to .05, not rounded down to .05","Again, cherry picking. One thing is to remove typos, another is to remove true outliers to get a larger R2."," Sometimes  an increase in sample size is what you need  not just for statistical significance but to get a better picture of your population ","(see previous comments) in ecological studies, covariants may deluge the main effects of tested variables and therefore their inclusion to models is reasonable (and it may be hard to plan in advance, especially when using large datasets from wide geographic distribution) ","A clear description of the data and analyses is key to making these contributions meaningful to others.","Absolutely unethical!","Again, choice of answers is too limited. Most of the time, I simply do not know the answer and having to choose one feels like presenting a false scientific result. Honestly I would not put give much credit to a survey like this."
"6","A covariate is a variable, so I'm not clear how this question is different from the last. My answer is the same.","Again - this stops the data being synthesised. STOP IT.","A lot of the data we collect could be used to test multiple hypotheses, and we often take data on variables other than our main responses (especially in field experiments/observations), because they're really time consuming to construct, so we try to get the most out of them. I see no problem with returning to the literature when your pet hypotheses proves not to be what's going on, to try to understand what you're missing and how you might expand your hypothesis, or test other hypotheses with your data. Again, though (and this goes back to the null results question), it would be nice if we were a bit more transparent about this process and had a centralized place to report null results, such that, in a manuscript about a different hypothesis, we could acknowledge that we initially set out to test something else but didn't see anything in a succinct way. ","Again this question is a bit ambiguous. I think this practice is common in cases where a lot of exploratory analysis is performed before settling on a final statistical analysis approach. Indeed, this is necessary in many subfields (e.g. ecological genomics) where new datatypes require trialling a lot of different analysis software or developing new approaches. I have done this myself, however I have never explicitly stated ""the statistical models reported in this manuscript were the only statistical models that were used at any point"" (and would not do this). However maybe it is a specific kind of analysis (e.g. model comparison) that you intend to ask about here...","0.05 or 0.01 are arbitrary thresholds. A p = 0.013 should be openly communicated and subsequently regarded for its proper significance by peers, and not just as a value above threshold ","Again, it is a matter of choice and what you want to achieve with your model. I've never had to remove any outliers, so I don't know. I believe looking at the data points is the best way to understand your data. More so than the models","... should be used often if - and only if - the additional data serve as a replication and are (also) analysed separately from the first dataset. Should not never be used to ""make the data seignificant"" (e.g. collect data until the result gets significant).","A robust relationship would be largely insensitive to the type of analysis used. Cherry picking models for significance is unethical and increases the chance of Type I error / ","A full disclosure of study weaknesses is one of the most important aspects of the discussion section of a research paper. It helps make more informed conclusions on often-noisy ecological data.","Again the contributions of a particular research project only have value insofar as they are trustworthy.","All practices surveyed were dubious practices. To get a clearer and more balanced response, would it have helped to include practices that should be encouraged (e.g., meta-analysis, etc)?"
"7","A non-significant result is a result.","Again it depends on the a priori questions, and one's theoretical understanding.  If the analysis is exploratory, I think some form of model reduction is justifiable, especially if the data is observational. On the other hand if the data come from a manipulative experiment, and the covariate was chosen a priori based on theory, it is critical to leave it in the analysis. ","A prior hypothesis is always necessary for good science. However, if all findings were expected then why do the science in the first place. What a good scientist will do however is not lean too heavily on conclusions that arise as a side analysis, and push for more testing in future.","Again, a difficult situation.  What I would do for a design experiment would be to think carefully about the model BEFORE analysing the data.  Then I would analyse the data with a small set of models (maybe just one) and report that. /  / However, for prediction and hypothesis generating (exploratory) studies, I would probably be much more liberal.  I would plot data, fit a series of smaller (e.g. univariate in multivariate situations), and use the data to help define the model.  These are all types of analyses, but I would almost certainly not report them all (there is not enough space).  I would say/write though that this is not a formal hypothesis testing framework and that the results here may not be generalisable to other data sets.","0.054 or 0.05 is the same level of Pvalue, low but not very significant. There is just no reason for not reporting 0.054 stated as marginally significant (more generally, the exact value with 3 digits)","Again, it should only be used when, after the test some awkward result was detected and proved to be wrong.","A common problem in evolutionary biology, where the sample size is usually the number of species, is that there are too few species measured to answer the question; the methods are rarely powerful enough to provide meaningful statistical tests with fewer than 20 species but you might have measured only 20. Rather than conclude that some factor has no effect, it is better to increase the sample size and conduct a more rigorous test.","A statistical test should be chosen according to its appropriateness for the question and data, not because it gives the desired results. However, there are cases where several test - each with their own limitations and assumptions - are appropriated. Here, tests could be compared and if one shows significant results where another doesn't, both should be reported and discussed.","After reading about scientific misconducts, I realized that this is a non-ethical practice. It should never been used because the readers do not know whether the hiding problems are big enough to compromise the conclusions. Again, transparence is necessary in this respect and no paper would be hurt by clearly stating the problems that appeared during the development of the work.","Again, anything that may change the strength or the interpretation of the data needs to be disclosed to the reader. In this case filling in missing values at least influences the ""real"" sample size and thus impact of the study.","Am looking forward to the results.  Please circulate to all participants/original mailing list."
"8","A non-significant result is often just as important for hypothesis testing, however sometimes for the sake of brevity not all variables need to be reported on.","Again the proper question should be under what circumstances is it OK not to report all the possible covariates that were analyzed.  Sometimes scientists load up regression analyses with a huge number of possible factors, most of which are suspected to be unimportant.  Listing them all may be unecessary, depending on context.","A research project changes a lot from conception to publishing. The final aim is to write a clear paper interpreting the results that you obtained, acknowledging those that are unexpected as such. ","Again, a tough question to answer. Occasionally, we will do some preliminary analyses that we acknowledge in the text and often show in appendices, if allowed, to determine appropriate structures of nuisance parameters, or we may to some exploratory analysis to determine what level of model complexity can be supported by the data - again acknowledging this practice in the text. What we've never done is run an analysis and eliminate factors from consideration because they didn't give us the results we were expecting. I hope that answers your question.","A cutoff is a cutoff. Although statistically there is not much difference between 0.05 and 0.053, for example, (the cutoff is aritrary in this way), since it is an established cutoff, rounding values to fit this cutoff feels dishonest. ","All data should be included and reasons to exclude data clearly stated. It can be relevant sometimes, in order to meet applicability criteria of some tests. However, it should be a last resort.","A lack of data can easily cause results not to be significant. While significance should not be used as the main criteria, additional data can often help decide whether a results is significant or not.","A statistical test should be dictated by the data characteristics and not by the outcome","again a problem of reviewer bias, people want easy reads","Also basically research fraud, especially if its an outcome you're ""simulating"". ","Am sending a copy of this URL to a statistical ecologist colleague of mine."
"9","A non significant result is still meaningful. If you thought something was worth testing, then it's non significance is still an important finding  / ","Again, another tough one.  If a covariate was planned into the study then I think it should never be omited from the model.  But if a research just wants to see if a covariate could have an effect (but is unsure if it really does) then researchers often test it, see its not predictive and discard it, and in this case I think it's okay. ","A researcher should be open if post hoc analyses were done or post hoc hypotheses were added to the problem.","Again, all analyses in theory are important. But for the purposes of publication, some may have been needlessly tested and so not warrant inclusion perhaps. ","A p-value of 0.054 does not meet the requirement of p &lt;= 0.05","All decisions about removing outliers should be done follow transparent rules that are decided upon before data analysis.","A valid statistical test can still be devised for this procedure, though whether or not people apply valid statistical tests is another question. /  / Of course, if we do this many times, and apply a one-off statistical test each time, like a regular t-test, then we will obtain many false positives. I believe that is what you are getting at and one should never do that because it is statistically invalid.","Again it depends if it is an experiment or are field data. In experiments, the design and analyses can be decided from the beginning to answer a question, but having field data, sometimes is necessary to perform exploratory statistics, and we can incorporate new environmental or geographical variables, giving us new statistical opportunities and different results focus on different analyses.","Again it depends on the extent and severity of the problem. I think most researchers do not report everything otherwise methods sections would be twice as long. But generally minor issues probably don't impact conclusions, although we cannot be certain. It is good to include problems that might have an effect on conlusions, and i think most researchers, including myself, do.","Always indicate which missing values are imputed.","Are these bad things bad?  Yes."
"10","A nonsignificant result is just as good an answer to a question as a significant result unless statistical power is low.","Again, I'm not quite sure what is meant by ""not reporting covariates"". Is it meant that ""insignificant"" factors that were included in models are somehow hidden? And what is reporting them? I am interpreting this as, not discussing in detail covariates that were not found to be significant, not, like, somehow hiding them.","A well-written and interesting article should tell a clear and logical 'story'.  Sometimes you realize after the fact that you were thinking about your study or problem in the wrong way, leading to incorrect hypotheses.  Or sometimes the most interesting results are the unexpected ones, and then it makes sense to frame the paper around those results.","Again, depends of the framework and epistemological issues","a threshold is a threshold / but normally statitstical softwares take care of this","All the analysis should be done with the raw data by providing the methods and statistics used to simplify or remove variability or other aspects from the raw data.","A waste of resources and intellectually dishonest.","Again no significant effect may only indicate that the model might be wrong or the data is too noisy etc. sometimes more complex models etc. provide other answers. Sometimes it makes sense to dig deeper but often it's indeed a search for low p values","Again, this is misleading and can send other researchers down the wrong track in future studies.","Always report it.","As always, the exact phrasing of the question drives much of the answer. Please note that different respondents will pay more or less attention to the exact phrasing and therefore will interpret it differently. I am particularly worried that results from a survey like this will be reported without appropriate nuance and will be used argue that a field is ""corrupt"" when in fact it is all about honest answering or highly interpreted answering of the nuance of the question."
"11","A result is not more important for being significative or not.","Again, I think people do this when they are working with a big complex data set and it's hard to settle into your most parsimonious presentation of the study. I know I have done it when I think some covariate data are kind of spotty but it's the best data we have on a particular covariate, and some reviewer is likely to ask if we looked at it. I check the relationships, and keep the results ready if someone asks. If you say that the relationship is not significant in the paper - someone will take that message away from the paper - even if you say a thousand times that the data are not good enough to use in that way.","Acceptable if the ""unexpected"" result was obvious from the literature ","Again, explanatory analyses are often done before a subset of final analyses are defined. If the latter set is oriented by results there is a risk of having more false positives","a threshold is a threshold. If the value is larger, it's not below. ","Although I prefer not to, I think if there's ground to call some specific data point a clear outlier it could in principle be done","Adding more data to a study is not necessarily a problem, especially if the first study is considered a pilot. If something is not significant and it is not a power issue, collecting more data is not necessarily going to change the significance. ","Again reasonable if you've violated the model assumptions. Of course maybe you should have thought of this in the first place, but sometimes non-parametric methods only become more obvious once you really think about the results of parametric models.","Again, this is scientific malpractice. I have rejected tonnes of data simply because there are issues with their quality. ","Are you kidding me? That is crazy.","As for statistical analyses, I often encounter unfavorable practices. Two major ones that were not included in this survey are time series analysis and the choice of distribution. Some researchers regress time series data against time to perform regression analysis. This violates the assumption of data points being independent. As for distribution, many researchers assume normal distribution, even when they are dealing with variables following, e.g. poisson distribution."
"12","A significant correlation does not always mean significant conclusions. There are relationships that are may be evident, without reaching the 0.05 limit. This could be due to low number of samples or outlier driven or any other statistical error. So if a study is well backed up, it can be stronger than a significant correlation. ","Again, I think this is OK in observational type studies where there are potentially large numbers of explanatory variables. ","Again I am puzzled what you are after. If I have formulated a hypothesis (a prediction), and the outcome is unexpected, not as predicted, whats bad with that? I would not reshape my hypothesis for that reason. You think people do that? Why should they? ","Again, I just think there should be transparency in model selection criteria, but I don't tend to use model selection approaches that much.","Actually it is a bit difficult to interpret this question. It should never be used in situations when it implies that a result meets a predetermined threshold for significance, but if it is clear that it's just a matter of displaying a value to a certain number of significant figures, it may be okay. But obviously better to write as '&gt;' than '=' in the examples given. ","Although you can fix your model because the assumptions are violated, you should not delete data point. This is not ethical.","Adding true, independent replicates to a study or - preferably - repeating the whole study with more replicates, is often statistically good. In long-term field experiments, however, it is often almost impossible because there is no way to add true replicates in cases where experiment was started (say) 5-10 years ago.","Again this is subjecting data to multiple tests and needs a correction","Again, this is something I did when I was young and foolish (and poorly advised).","As long as it is disclosed, it should be OK.","As noted by my comments, several of your questions cover a mix of practices that are legitimate and those that aren't, so the results may be a bit hard to interpret. /  / Furthermore, my guesses on number of people undertaking a practice are wild guesses, based on no data - probably true for most respondents."
"13","A similar answer to the last question. I don't really design experiments to run multiple P tests, and instead tend to use Bayesian methods where I'm trying to infer certainly on parameters (and thus reporting CRs).  However, if researchers do use a frequentist method, then you do need some threshold, otherwise the paper would be filled with 'we look at this list of a million things and saw no association'.  I'm less convinced P is the right threshold - depending on the test, effect sizes might be better (or something similar)","Again, important to know when factors are not important, though it is likely ""less interesting"" than the main result. At least report it in the supplement.","Again it completely depends on the situation and type of study whether or not this is ethical. If you accidentally discover something unexpectedly that is important, it could be completely reasonable to rebuild the narrative around the discovery to better convey the information. ","Again, I think it is probably impossible to report in a manuscript the entire set of models that have been explored in a dataset. The question should also be better defined: i.e. it happens that one looks at patterns affecting juveniles in the analytical stage - out of curiosity, or to better understand some patterns observed in another age class - even if the main focus is on adult females --&gt; is this bad practice? We cannot publish virtually everything we explore. ","Actually, I think that ecologists would be much better off following standard practice in statistical journals of reporting &lt; .05, ,01, etc, and rounding, as there is virtually no meaning (beyond sample size effects) of that additional decimal. Adding decimals to P-values suggests precision (and accuracy) that is not there.","Analyzing the sensitivity of results to outlier data points is good statistical practice. Results are reported with and without the outliers and the implications discussed","After a preliminary analysis can be estimated the sample size to reach the desirable statistical power. For this purpose, I would allow collecting additional data.","Again, a practice that creates a bias in the chance of having a positive result. But it can be ok as long as we are clear about how different methods differ and we report them all.","All aspects of methodology / interpretation should be reported, if only for repeatability.","Clear bias/attempt to mislead.","Besides asking why one thinks a practice should or should not be used, I would have found interesting to offer the possibility to explain why they have engaged in such practice at least once. My experience is that I had very little awareness of what was good or bad practice in statistical data analysis when I was a PhD student, because I was then very young, unexperienced, and my statistical skills were limited (to a large extent, I learnt by doing). More importantly, I was lacking a superviser caring about these matters, and in general an environment that would encourage thinking about all this. I believe I have not engaged in such practice in my postdoc research anymore. I think what matters is not only practice per se, but also how practice evolves during one's research career. On another note, even though this should not be an excuse, as you know some practice I would consider bad is encouraged by the publication system... I think many researchers are aware what they do is not good practice, but they still do it because of the famous ""publish or perish"" dilemma."
"14","A tough one. In my case, I just haven't gotten round to publishing these results yet. If they data have been collected properly, but there is a non-significant p value, then the data are still very valuable. Not publishing such data is short sighted and holds back potential advances in theory and practice. ","Again, increases the number of false positives. Additionally, it fails to report information that might turn up to be of interest for future researchers doing meta-analyses","Again, a bit confusing. The statistics of hypothesis testing should be performed on an a priori hypothesis.  That is, you shouldn't find a statistical result through exploratory analysis, and then define a full statistical test to 're-test' that finding.  However, there is nothing inherently wrong at exploratory analyses, and in fact, many early researchers do not 'look at' their data enough.  However, the statistics need to be carefully designed to accommodate this two-phase process.","Again, I think it should be used at the discretion of the researcher. Ultimately, the journal/reviewers will end up having a preference of what to share, and this information can sometimes be useful as an appendix. ","again i think the questions poorly worded. most stat programs will give you various decimal places you must reduce for publication.  i would generally reduce to to two decimal places so i do what you dewcribe in the example above but when we talk about the third decimal place or the fouth etc. it shouldnt matter","Any data point, except some clear outliers, should not be excluded from the analysis only to improve the p-value. It's cheating. ","Again we should not be using p values. One may want to take additional data if the observed variance is too large to give adequate confidence intervals.","Again, a question of context: some analyses are simply better for a given situation.  Fishing among analyses just to get a significant value, however, would be inacceptable.","All caveats related to the data set and analyses should be discussed - this is the only way that results can be properly interpreted. It is unlikely that reviewers would not highlight these issues during the review process in any case.","Complete inapropriate. This is faking data.","Best of luck with it!"
"15","Absence of sugnificance is a result ","Again, it's dishonest! But in select circumstances or particular studies it might be justified. ","Again, I do not like it, but how else do I publish? ","Again, indicative of the possibility of p-hacking / dredging data. Analysis design should occur at the same time as experimental design.","Again, a version of bias is introduced with this rounding down.","As opposed to excluding a data point as a result of influence statistics, of course.","Again, an issue for p values in particular but also sometimes necessary to try and find efficiencies in a data collection process where samples are costly and variances unknown and separate pilot not possible. ","Again, if the intent is to keep trying to new tests until one is statistically significant, this is dishonest. If the intent is to use the correct test, and you realize that you have used the incorrect one after testing it, well, that's a more thorny issue. It does happen and in the real world, it is hard to avoid as we are not all experts in all the methods and learning about new methods happens ""on the fly"". I have done this ""by accident"" lots, but I strive to avoid it wherever possible.","All known problems in the method, data analysis and data quality should always be made clear in the manuscript. This is important as it is crucial in the interpretation of the data and conclusions drawn from it.","completely dishonest - essentially making up data","Best of luck, I hope you get a good response rate. I'll be interested to see the results!"
"16","Again - during model simplification many predictor variables will be removed from models. This is usually addressed in the methods (e.g. predictors with p &gt; 0.25 were removed in backwards model selection), but stating the actual p-value of each of those variables before removal is extra and unnecessary information","Again, it depends on if it is done to deliberately hide something potentially important. If there is substantial space saving or simplicity in leaving the non-significant information out, especially in exploratory analyses, readers don't need to be burdened with the non-essential information. ","Again, it depends on the story. If the story is more right-away to tell in that form, it can be used.","Again, intellectual dishonesty. Further, one can (and should) simply explain why other candidate data sets tested were not used. ","Again, I don't usually report p-values, but I think that they should not be simply reported as whether or not they were above or below a certain threshold.","As with choosing statistical tests, outliers should be excluded in some circumstances, but simply looking at the p-value difference w/ or w/o them is not OK.","Again, ideally it shouldn't be done, and in my experience isn't being done. The reason is not statistical prudence but time constraints. When the season is over, the data are in, for good or for bad. / At the same time, in the ""everything is connected to everything else""-paradigm, there MUST be an effect of X on Y. If the methods explain this clearly and an additional simulation investigates the effect of cumulating data, I would be fine with this approach.","Again, it's dishonest. There are usually only a few ways to analyses something properly. If a different analysis gives a different result then either one of the analyses is inappropriate, or the result(s) are due to structural differences in how the different analyses work and the researcher should think hard about which they should choose!. ","Almost all analyses have inherent problems which are impossible to discuss in detail every time.","Constitutes inventing data.","Clearly most of the problems this survey asks about are variation on the same theme: research practices that skew the scientific literature towards publication of ""significant"" results, leading to biased estimates of true effect sizes in the literature overall. These problems are rife in ecology."
"17","Again - space issues - but can be fishing for result you want,.","Again, it is misleading and so bad ethics. The design of the model should reflect the design of the experiment, so if covariates are relevant, they should be reported, even if not significant.","Again, it is hypothesis generating rather than hypothesis testing and should only be reported as such. ","Again, it can be done for simplicity (e.g. to eliminate models that are somewhat redundant), but never to hide important results. ","again, the question is unclear.  This is implying that we should consider p-value treshholds as hyper important. they are not. P=0.051 and P=0.049 should convey the same conclusion. Of course tweaking those values to meet a treshold significance is clearly bad. But rounding up is also good, just because the very concept of treshold significance is questionnable, and we should really consider only orders of magnitude of P values.","Assessing outliers is an important step in analysing ones data - in fact highly influential data points or covariates could completely skew the results of the analysis and may violate some of the underlying assumptions of the model. /  / Therefore, it can be good practice to: 1. identify such influential data points using appropriate statistical means 2. assess how it affects the modelling you are undertaking 3. fit competing models with and without the influential data point and observe how the point estimates of your parameters of interest vary. /  / Above all, however, if one engages in this practice, one should be transparent about the process and both models ideally should be reported somewhere in the paper (either in the supplementary material or the main body of the paper).","Again, it depends on the nature of the study. Such a practice is exactly what one would do as part of an adaptive management plan, for example. ","Again, one needs to be ethical. Science is about testing hypotheses with experiment, not about publishing p<0.05 in the sexiest journal possible. A priori and post priori hypotheses are both acceptable, but they need to be labeled as such.  ","Almost all analyses in evolution have many potential pitfalls. I don't think it's necessary to discuss every one for every analysis, but rather just focus on the key ones that could qualitatively alter interpretation of results.","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","congrats for the nice study! Hope you have enough answer to give a robust picture of these issues!"
"18","again depends.  at the study level, ideally all findings get published, but a) people run out of time and so sometimes the least interesting results fall off the conveyor belt and b) sometimes a positive result is interpretable and a negative one is not. / at the experimental level it should never happen - all parts of experimental design should be reported whether signif or not","Again, it must depend on researcher's experience about when to measure/test/report a covariable. Covariables are sometimes not covariables but noise, and that could be identified when exploring data using multiple stats.","Again, the practice of presenting post-hoc observations as prior hypotheses makes the scientific literature uninterpretable.","Again, it depends.  Statistical analyses help one find patterns. As long as this it is made clear to readers that the purpose was to find a pattern, most journals won't publish anything but the best fitting model.","All P values should be presented at three sig. figs.  That would allow the reader to make up their mind.","Assuming the data set has been checked already for odd values due to data entering/inconsistent units/etc., I belive 'true' data points should not be excluded just because they are very influential, or then at least one should report both the results with and without such data points and discuss their influence.","Again, it depends. If the first test is a pilot, it is fine. Fisher advocated this. But the tests should be blocked. ","Again, situation specific. Was the initial model attempt, flawed, or not the right model for some reason, and that is why it failed, then by all means try something more appropraite. That should be the basis for trying a new model, not to keep trying something until you get the results you want.","Although caveats can be distracting and dispiriting in a publication, it's important that any major sources of potential error or uncertainty are addressed.  Limits of space mean that researchers do have to exercise their best judgement as to what potential problems are most likely to effect the conclusions, however.","Data fabrication is pretty much the worst sin possible","Didn't finish answering Q1 as I thought I could go back and add to it. "
"19","Again I don't think I can answer the multiple choice questions.  / I think researchers may do this because it is cumbersome and discouraged to have too many variables or results - the pressure for shorter papers. / What I do and what I would recommend to scientists in general is to simply state that these variables are not significant and do not present as much detail or do so in an appendix. / As an editor I do not reject any manuscripts for this reason.","Again, non- significance is an important information and not reporting leads to biased knowledge. However given the low reproducibility of p values it again illustrates the dilemma of using p values ","Again, this is hard to answer and is really a poor question as it presupposes that we can know the mind of someone. What is important and ethical is that results are presented fairly.                                            ","Again, it is an issue of multiple testing. However, careful description of all failed attempts would not pass a reviewer (aside of making a paper excruciatingly boring). Some compromise is usually reached in each particular case.","alpha of 0.05 is fairly arbitrary.  I think the biological significance of the effect size is probably more important.  Theoretical understanding and interpretation matter more than some arbitrary area under a curve. ","At least in phylogenetics, it can be useful to identify specific problematic taxa (few characters coded or limited data available, etc) and to rerun analyses without them, and see how the results change. Optimally, you report the results of both analyses.","Again, this depends of framework and on type of research (I suppose it is not uncommon in experimental studies...).","Again, this is just plain corrupt.","An interestin example: despite a series of papers in solid journals (Oikos) with apparently reasonable criticizm of regression residuals, really many researchers, including top scientists, proceed with using these ","Data gap-filling or estimating must always be reported","difficult to estimate % of colleagues that practice some of the topics highlighted here. The question should have been formulated differently (e.g. how often did you see that?) or simply don't ask it.  / "
"20","Again if there are page limits, this may lead to some data not being reported and it is more likely that NS variables will be excluded","Again, not sure to understand the question properly. I always report full models and never use model selection procedure when running multivariate analyses, meaning that I always report non-significant variables. It happens that I remove interactions that were not significant from a model to avoid overparameterization, but I normally signal it in the methods. I think it is possible that I have alreday fail to report a covariate that I tested in an explorative stage of the analyses - see my answers to previous questions. ","Always more interesting to read about things that are counter intuitive to widely held beliefs.  And they may indicate a good place to start the next research project. ","Again, sometimes you can provide key model comparisons in the main text so long as the full model set is in the SI.","ALthough I believe the 0.05 or 0.1 p-value is arbitrary anyway, one must maintain precision when trying to support conclusions with statistics. If its borderline this issue can be discussed.","BUT we should be transparent about it. I report models with and without influential data points and then discuss the influential points. ","Again, this should be used at the discretion of the researcher. Ideally, they've done a power analysis beforehand, in which case they should stick with the sample size estimated from that. ","All analyses that researchers do are wrong, so it's fine to look among them. But you shouldn't cherry pick. You have to go with what in your opinion is the least wrong analysis, even if you don't like the output. This gets back to my previous point on checking about additional covariates that you think could influence your results. ","approaching fraud","Data imputation can be performed in some settings, but the imputed data should be disclosed as such.","Do you really think people will own up to serious fraud, even anonymously? Self-deception is a big thing in research misconduct. "
"21","Again it  conceals information, and may fail to prevent someone else wasting their time. It can also bias the results of later reviews and meta-analyses, but it is hard to publish negative results.","Again, results need to be accurately presented, whether they prove or contradict initial hypotheses. In my opinion it is OK to simply report within a paper that data is not shown as it was not statistically not significant, as long as raw data has been made somehow publicly available","As a general rule, I am not too bothered about stating a hypothesis (i.e predicting the result), rather I just prefer to report and understand the results. ","Again, the literature can support only so many negative results. Perhaps they could be reported in supplementary online material?","Although the actual p value should not be used evidence of support for a hypothesis (this is not the aims of a frequentist approach), it does demonstrate the level of confidence that the reader can have in the results, particularly when there is unavoidable issues of design such as unequal sample sizes, deviations from test assumptions","Can be used if tests identify bona fida outliers. Should be disclosed.","All studies should be repeated and validated. ","all statistical analyses are not adapted to all types of data and finding the best method is sometimes tricky, and often using simple methods first helps to better see the structure of your data and refine the method ","As a reviewer, I would reject any paper that does not acknowledge the limitations of the methods. It is unfair to readers to not admit them. Of the papers that I have read that do not admit them, it is usually because the authors do not know or understand the methods well enough to realize that the methods have those limitations. It is usually the author's ignorance, not their intent, when they fail to disclose those limitations.","Deception","everywhere in this questions, it seems that misconduct is implicit in one of the proposed action. But in all cases, it is very very unclear what misconduct is really and the questions were not sufficiently clear to identify which ethical problem was considered / This is a serious issue and I hope there will be no ""misconduct"" in the interpretation of this set of particularly unclear questions"
"22","Again unclear question. If a whole study did not yield significant results it might often end up not beeing published. If within a study a tested variables does not show results it is like reported.","Again, the question is so broadly worded as to mean this must happen, but especially in the age of big data where it is easy to collect and test covariates that may have no bearing on the question. ","As long as the original hypothesis is still part of the study (e.g. by becoming a side hypothesis), I don't see the harm of increasing emphasis on unexpected results (thus becoming a main hypothesis).","Again, this is unethical and reflects a profound misunderstanding of the scientific method, and of type I error.  Multiple tests expand the chance of a type I error.  The test should be decided before hand and one needs to rigorously behave ethically when practicing science.  ","An incorrect decision would be taken based on the usual levels of significance.","Caveat--I actually have done it, and think it is OK, if the existence of the discarded data point(s) is acknowledged and the reason for exclusion justified. ","All these questions depend on the particular circumstances if there in anything unethical going on. This question sounds to me like pilot data and power analyses.Why wouldn't you run stats on pilot data and see how many more samples you need to effectively and efficiently answer your question?","Alternative analyses can be equally valid. If the relationship you expected was not there it makes sense to see if there is an alternate relationship for instance an exponential relationship instead of a linear one. ","borders on misconduct","Dishonest","First question was very ambiguous. Other questions often where phrased as double negative so difficult to discern. My estimates of researcher practice are complete guesses."
"23","Again, fishing for results, you will get high type 1 errors.","Again, things get dropped or added through the peer-review process. Sometimes there is no more room for relationships that are not statistically significant. However, this is unfortunate and results in publication bias","As stated, it seems dishonest and misleading (the statement seems to imply that the researcher him/herself found it unexpected yet claimed s/he had predicted it).  /  / However, the impersonal formulation of this statement is really unhelpful!  If it had been, ""Featuring an unexpected finding or a result from exploratory analysis as an a-priori hypothesis"" - i.e. clearly not implying any deceptive first-person claims, then I would say (a) 50% of ecologists have probably done this, (b) including myself about once, and (c) it should be used rarely.","Again, this seems rather dishonest.","Aside from the inherent dishonesty in this rounding, this practice is reflective of a muddled statistical philosophy. The reliance on arbitrary thresholds - whether in p-values, Bayes factors, or dAIC values - is poor practice. This ought to be common knowledge by now.","Check for outliers first!","also not to recommend from an animal welfare point of view (in case of animal experimentation)","Although all the questions so far are (arguably) instance of research fraud, this one and the one before seem particularly egregious ","but it think there is restrictions on space & methods can take up a lot of this. journals want less detail","Dishonest.","For all of the questions, I found myself thinking that it totally depends on the specific situation if a particular action was ethical. It depends on if the authors are deliberately hiding something important. That's unethical. Not mentioning something unimportant is not unethical. Too many negatives in that sentence? Another way to frame the survey could be asking more specific questions about particular scenarios to explore grey areas and how researchers feel about the ethics of them. "
"24","Again, I have no idea how often this happens, but I suspect that every researcher sometimes asks the question: is there any evidence that supports hypothesis X?   THe climate change literature seems to be based on this approach: can evidence of biotic effects of climate change be identified?  It does skirt the issue of affirmation of the consequent.","Again, this comes down to clarity of presentation. If there is any a priori reason to care about the covariate, I think it can go in. But, if you're trying something that might be important (e.g., Julian date in a lab experiment), I can see dropping it. ","As Stephen Jay Gould stated long time ago, the publication is not a close resemblance of how research have occurred. Even though the data make sense at the ligth of current discussions, there are numerous examples in science history that unexpected results lead to new areas of research and to new theories. I believe that the frame of postulating hypothesis, testing them, and analyzing results should not be considered as the only way to do science, despite that publishing structure reduces in many cases the papers to that iron frame","Again, typically the models used for data analysis should have been formulated before any data analysis is done (or perhaps even before data are collected), in which case all models will be justifiable and are therefore informative enough to include (at least as a mention that they were tested). As always, there are rare occasions when there are genuine reasons for not doing this, for instance if some models were both uninformative and very much tangential to the subject of the paper (in which case a reasonable argument can be made for removing them), but this should also be an unusual scenario.","Attempts to conform to strict cut-off significance thresholds demonstrate an adherence to conventional practice over understanding of probability (e.g. the difference between p = 0.013 and 0.010 is and should be viewed as trivial).","Cherry-picking data for no independent reason points is close to data forgery. ","Although this could be useful to prevent spending money and time in some cases, this should be the exception, not the rule. I've never heard about this practice.","Although there is a risk of overestimating significance by doing this, there is a risk of missing important results by not doing this because the best statistical approach was not well identified initially. ","Completely against this! If there are problems they have to be stated! Especially if they affect the outcomes of the study","Empirical and simulated data must be reported as they are. ","Found it hard to distinguish between questions 1 (""covariates"") and 2 (""variables""). / "
"25","Again, I would differentiate between model validation (wherein I've constructed a statistical model and am checking to make sure there are no further covariates that I need to include), and p-hacking or fishing to derive inference from only the variables that are statistically important and not the variables that are statistically unimportant. The former occasionally leads me to explore covariates that may not always make their way into the methods of the manuscript. The latter is a misuse of statistics to guide hypotheses and inferences rather than test hypotheses or verify/formalize inference. ","Again, with enough thought given to data analysis at the outset of a project this should be rare. However, occasionally it may become apparent that the covariates are both uninformative and were not well-suited to the main question at hand anyway, in which case they could be ignored in the final write-up.","As with data dredging (see above), the following situation is - to my opinion - pretty conceivable: the researcher has got such a result during an explorative phase of work, then he has read a lot of literature in the field and found a reasonable hypothesis which predicts such a result (or its converse), and he/she presented the results retrospectively as a confirmative study. WHAT is bad here?! Again, if the subsequently found hypothesis is scientifically poor, qualified reviewers shoud reveal that!","Again, you will get high type one errors.","Basically I am neutral to this practice, because I think p value is just one piece of evidence to judge the result of the work. Most empirical work violate some assumptions of statistical models, and I wonder if very small difference in p values has some significance.","Cherry-picking. It is not OK for climate skeptics - it shouldn't be OK across the board.","Another tricky one... I think the emphasis on P values is the problem here. Studies should be designed to yield required precision or power but many field-based studies expecially don't have this luxury. Collecting more data to increase precision of estimates is sensible, doing it to shrink your P value is not - need to dinstinguish between these a bit more clearly! ","Always use the most suitable statistical test, independently of the result. ","d) Optional Question: Why do you think this practice should or shouldn't be used?&nbs","Ethically a total no-no!","Gender isn't binary. "
"26","again, negative results are informative","Again, you may have no a-priori reason to include a covariate or you may be testing for parallel lines. There are times this is acceptable, but they are defined. ","Bad science, weakens the resulting inference...","All statistical analyses that were conducted must be reported.","Be as honest and open as possible.  Such a small amount isn't going to make much difference in anyone's opinion of your work, unless they think you've intentionally hidden it to make a point.","Clear biases/distortion of analysis implied.","As before, depends on the design & the inferences one is trying to draw.","Analysis choice should be on the basis of the most appropriate statistic given the question being addressed, not whether the initial analyses were significant or not.","Data in ecology has a lot of noise and sources of uncertainty. But failing to report issues that could results in different conclusions is mal practice.","Even if sampling from numerous simulations, this method inflates the sample size. I could only see a case for this method if the analytical method was fragile, perhaps requiring evenly sampled data, and the fraction of missing data were very small. And in that case the method should at least be reported.","Good and timely survey. Yet, I would have asked the participants to estimate is level of productivity in the past few years (e.g., number of grad students, number of published papers). I have a feeling that the most productive people are also the ones who are more likely (tempted) to cut corners... or who may not have the time to verify / make sure that everything is being performed adequately in their lab or their collaborators' lab. Adopting a ""small is beautiful"" approach, and not succumbing to the publish or perish syndrome, is what allows me and my students to do things slowly and conscientiously. "
"27","again, this is a tricky question; if the non-significant results are presented in a table and referred to, than that's OK. Not acknowledging studies that did not find an effect on a given variable is academic dishonesty (have seem plenty of those)","All covariates examined should always be disclosed so that the probability of achieving statistically significant results by random chance can be assessed by readers.","Because an unexpected finding by nature is less likely to be found and therefore the probability of false positive is typically higher for them than for an expected one. It is then important to acknowledge them as such and as ""preliminary results"" to be confirmed by other studies.","All statistical teste have strengths and weaknesses. Running multiple testes help improve your understanding of the level of certainty of your results and should be done. This is part of the normal practice of 'getting to know your data'. However, selectively searching for the odd test that support you data to verify you believes should not be done.","Because .05 is arbitrary. It's unfortunate anyone feels this is necessary, but this is a problem of our own creation.","Clear violation of academic integrity","As it is less likely to find an effect in a small dataset, it seems one should collect more data if the effect is not significant at first... / Of course, not valid in the strict statistical sense, but probably used often as we all work under pressure (time and money and competition)","Analysis methods should be selected based on how appropriate the assumptions are and goodness-of-fit tests, rather than a desired statistical outcome.  / If the selected method does have appropriate assumptions and good model-fit, this is not as bad as some of the other practices. I can understand the temptation when there is pressure to publish.  / It is my policy not to do this, but if I am considering different analysis methods, it might color my decision-making process at some level.","Depends on if it is too technical and too much of ""statistical machismo"" and potentially greatly violates word limits to include too many such qualifications. It should at least be disclosed in a supplement though, but since not very many people read the supplement I took this question to mean the main paper.","Fabricating data is definitely an ethical violation.","Good luck with the analysis!"
"28","Again, this is similar to hide data and the interpretation of the results by readers are compromissed by this practice.","All covariates for the initial analysis should be presented ","Because it falsely suggests that an hypothesis is being tested, whereas this should only lead to formulate a new hypothesis","Alternative candidate models are representations of alternative hypotheses. The relevance of each model should be considered before implementing the test or including it in a set of candidate models under a model selection approach. In this case, nested models may indeed represent biologically relevant hypotheses, where multiple co-variables affect the response variable in an additive or interactive way. Interactions between co-variables represent a more complex biological explanation, but also a more interesting one. Therefore, I think it is common researchers investigate such interactions, but may drop-off these models in the report in the case of more simple candidate models are more likely, especially because some metrics from model selection (such as AIC weight) are affected by the number of candidate models tested. An honest alternative attitude is to report the full set of models in the supplementary material and/or mentioning in the text that more complex models have been tested, but excluded from the report due to its low adjustment.","Because alpha error is continuos, not a discrete value; and a P = 0.013 deserve biological interpretation. It is not the same a P = 0.98 that a  P = 0.054","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","As long as one is upfront about this practice, it is very useful. Exploratory analysis is an important tool in our scientific toolkit.","Analysis pathways should be chosen based on their merits, not in a way that results in inflated type I error rates.","Disclosure of everything that can help qualify the data is a good thing - it helps improve replicability/reproducibility of study results.","Fabricating data is fraud even if one uses a sophisticated model to fabricate it. ","Good work! Someone should do this analysis. Not exactly sure what is your angle. But there is certainly an issue of cumulative error in some areas requiring meta-analysis"
"29","Again, you can't test without reporting. (And, you should specify your analysis a priori.)","All covariates tested should be reported, irrespective of whether they are statistically significant.","Because it inflates type I error, leads to false discoveries, and generally reduces reproducibility.","An issue if there's a strong focus on p values","because if you set a value that is the value","Data are data. If they're true outliers, they should be removed, but it shows a lack of integrity to remove data points to reach a desired threshold.","As long as the experimental design is not altered as a result of preliminary trends observed during data collection, I see little harm in this practice. The process of collecting data is often a long one, and developing analysis scripts on incomplete datasets is a way to efficiently use time.","appropriate if initial approach did not really fit the question; in basic research the process of analysis is a process of learning","Dishonest, and you run the risk of others not being able to replicate your results.","Fabrication of data should not occur.","Great idea for a study. Please let us know what you find. You may also be interested in my study (forthcoming). An overview is available at http://guianaplants.stir.ac.uk/manuscript_fluxes/ / "
"30","Again: I am a statistician; all p-values are false if what is being reported is only the ""signal,""not the noise against which it was ""detected.""  Again, this practice is a form of lying to the reader.","All covariates tested should be reported. In mixed models for instance, even if the covariate is not significant, the part of the variance it explains is taken into account. Therfore, failing to report this covariate is not a truthful representation of the data.","Because it is used to pretend that the work was completely succesfull and that you found what you were expecting. I think in science there is a clear bias to accept only ""successfull"" studies, so frequently you need to do this kind of things to justify your findings.","Any set of models used in an analysis will only ever be s subset of all possible models, so your set is never complete.","because in those cases the true result should be &gt; or &lt; not  =. Results always must be presented as they are. The significance of a study must always be based on the actual results and not on how they are presented.","Data cannot be just gotten rid of.  But, I think it OK to do an analysis to identify outliers.  Once an outlier has been identified statistically, then I consider whether there is a biological reason to think the sample was not representative (wrong developmental stage, for example).  If I conclude it is not representative then I could consider excluding it.","As long as the post-hoc sampling is stated openly, it might be OK in some circumstances.","As I said before, the choice of test should never be based on the results of the test. ","Dishonest.","Fabrication of data.","Great questions, I'd be interested to hear the outcomes."
"31","All tested variables should be reported - whether we report or not the value of those that were not significant is another thing.  / More problematic are studies. Often in ecology we do exploratory analysis, and use it as prior knowledge onto what works and what doesn't. We then focus on what works. I have done it, most people I know have done it, and it is very bad practice.","All data should be reported in the interests of transparency. Several different analyses can reveal differing levels of statistical significance.","because it results in false positives","As for previous questions, I think this practice is obfuscating the value of the models tested, for reasons pertaining to correcting p-values or correctly assessing model weight.","Because it requires an obviously false statement such as 0.054 &lt; 0.05 to be made, and like all such practices helps to lower the reliability of significant results (if only slightly) and undermine public trust in science.","Data point exclusion must be based on other criteria. ","Barely have enough time to collect the first lot of data.","As long as the analysis is still appropriate to the data then I think it is useful to explore other possibilities. Of course, whether or not there is consensus between the two models, they should both be reported on.","Dishonest. No point in doing this job then.","Failure to accurately report methods means approach and results cannot be replicated ","I'd be interested in seeing the results when they come out. "
"32","All variables used in your hypotheses must be reported. 'Fishing' expeditions may be different as a researcher has no idea which variables are important and why.","All of the variables in the model should be reported. ","Because published results should be statistically confirmed","As phrased in the question, this practice should never be used as it is effectively lying.  /  / However, there is a difference between stating ""here are the results from the complete set of models tested"" and ""here are the results from some models we tested"". The latter case happens more frequently, and there is ambiguity in the extent to which authors are trying to mislead readers into thinking it's a complete set versus more innocently just trying to tell a simpler story/pare down lots of results.","because of the way it will be perceived by the readership. p values are silly anyway.","Data point should only be excluded if they are suspected of being inaccurate due to a mistake during collection or analysis or they are true outlier","because ecology is super variable and field seasons are expensive and data points are sometimes hard to get.  if half your experiment blows away you might need to redo.  in practise however I have never had time to analyse my data before decided whether to collect more, and always end up analysing at the end","As long as the context for the model and statistics is explained this is fine. Ideally an appendix would provide the other models and explain why they are less appropriate.","Doing this would constitute over stating one's results. ","Falsification. ","I've written a lot of social surveys. I think you might be biasing your results by using the word ""should"" (though I understand why you have). The problem is that it is not really an issue of 'should', but rather ""is the practice justifiable under particular circumstances"".  /  / Nice survey... I bet the results are like sexual behaviour surveys (everyone claims that everyone else is doing it, but not them!). "
"33","also non-significant results can be relevant","All test results should be presented as supplementary material.","Because sciences you make knowledge advance","As with all statistical practices, the truth is preferable, but here simplifications for the sake of readability are perhaps forgivable. ","Because the mathematical probability is the mathematical probability and should not be fudged (or conveniently rounded) to fall on one side or another of an arbitrary threshold. The text should contextualize the P-value and explain the near-significance and biological interest of a P=0.054 result.","Data points can be excluded ONLY if the exclusion is noted and justified in the text - it cannot be done without informing the audience.","Because, like the earlier scenarios, it results in overconfident results, e.g. where true alpha &gt; reported alpha. Clearly over the course of a study, under the null hypothesis a p-value could dip under alpha one or more times before the scheduled completion of the study, and if the study is declared significant at the first instance of p &lt; alpha, then P(significant | H0 is true) &gt; alpha.  This is a serious problem because many researchers do it, and most have either no idea it's wrong, or at best a vague unease about it.","As one doesn't know the data distribution in advance, it is not really possible to determine the analyses beforehand. In addition, very often reviewers suggest a different approach and paper would not get published if one would write back saying sorry - cant do this as it was not my initial idea. Also, it often happens that by speaking to others, presenting results at conferences people suggest approaches one did not think about beforehand.","Ecologists do themselves a disservice when they fail to disclose challenges or limitations. The one time I did this was on the advice of a supervisor during a junior research position.","Filling in missing data points without identifying those data as simulated misrepresents studies.","I am dubious about the validity of collecting opinions of what others do, although it will be interesting to compare the two sets of distributions."
"34","Although publishing this kind of ""negative"" studies is difficult, it should be done to avoid other researcher doing the same ","All tested covariates should be reported as tested and rejected, but should be removed from models to avoid mispecification. Hence, reporting of specific p-values for non-significant covariates is not necessary.","Because we can never predict what is going on before we have any data. The science is too young","Because it gives a wrong idea of how many statistical tests were performed / Because it does not make available valuable information","Because these thresholds are completely arbitrary. The difference between p = 0.05 and p = 0.06 is minuscule and shouldn't lead to completely different decisions about how the data is interpreted.","Data points should never be excluded based on their influence on significance. Only when an unusual biological feature can be associated with a data point, exclusion can be discussed (but should still be reported).","Best to conduct an entirely new study on independent data.  However, in cases where data are expensive (in monetary terms or otherwise) to obtain, this might be the best way to estimate the true effect size.  In these cases, p-values against null hypothesis tests should not be reported (or at least should not be emphasized), but estimates of the true effect size with appropriate credible intervals are useful.","At least, you should present both analyses.","Ethics people. Conclusions based on faulty assumptions or methods are not conclusions at all.  ","Fine to simulate - not fine to not identify","I am not a graduate student or a post-doc. I'm out of academia now but consider myself an early career researcher."
"35","Although similar to the previous question, the problem here is not the researcher but the publication system, only looking for ""powerful"" results.","Always report the all the results from your model, not just some of them.  It may be okay to focus on the significant results in the main article and then report the non-significant covariate in supplementary materials.","being forced to do by referees and editors two or three times!!!! When young, tend to agree with that for the sake of acceptance - I know it is a bad practice and I keep resisting now, despite being asked to do that sometimes.","Because it is more informative.","Because your estimate of p is greater than the cutoff, you're lying if you're saying a test resulting in a p-value of 0.054 met a threshold of p&lt;0.05. If you want to say that it is marginal (e.g. p= 0.05), another significant figure should be given (p = 0.054).","Data points should only be excluded as outliers if there is a justifiable reason why they were different than the others (e.g. Some extenuating circumstance), and this choice should always be explained. ","Can be used to really falsify/verify hypothesis","At times the results of analysis being counter intuitive might signify that you are analyzing the wrong thing - so perhaps your initial model was incorrect.  What is more of a concern is moving from say a frequentist approach to a baysian one for example, that should rarely be used.  / What I have done is used more ""modern"" models (including site etc. in models) when comparing to analysis run in the 1980s,1990s to analysis now in our long-term studies.  We do not want to get stuck in the past. / ","Every dataset has problems.","for obvious reasons","I am not as bad as I sound."
"36","Although this practice is not ideal, exploratory analyses that do not yield an interesting result are often discarded. Space in manuscripts is limited, and analyses that do not contribute to the overall conclusions/narrative may need to be skipped.","Any variables included in preconceived hypotheses or included in statistical tests should be reported regardless of whether they remain in final models (stepwise fitting) or have P &lt; 0.05. This information is important for researchers who wish to understand the larger results or repeat the experiment.","Better to be honest about what your expectations were. Also usually makes a better story, if you can highlight the unexpected results. Don't think it's practical to say it should never be used, though. Sometimes that might just make the paper very complicated to follow.","Because of inflation of Type I error. By reporting only a subset of all models tested you are typically biasing your results towards those models where statistical significance of one or more explanatory variables has been found.","Better to report a near-significant p-value exactly - it's not like there's anything magic about 0.05.","Data points should only be excluded if they are major outliers in the distribution of the dataset. They should never be removed to adjust significance. ","Collecting more data can be done to increase significance of the results, but in the end the important point is the magnitude or effect size of the intended comparison. Even for very small effect sizes, a significant difference can be achieved if collecting enough data, but then that probably does not have any importance in relation to the research question. Hence, in general I find this bad practice. / Another point would be if the initial data show a large effect size (according to the particular variable we're interested) with a very large variability because the sampling size is very small. In that case, I see perfectly justified to repeat the experiment with a larger n (ideally, with a previous calculation of the power of the analysis). In most cases, I do not see adding new data to the existing dataset as proper practice.","Bad form - stats should be framed in advance.  The sole exception might be if you are explicitly running an exploratory analysis, and don't have a knowledge base for your system of where to start.  In any case though, that should be clearly reported (and I don't think I've ever seen a paper like this). ","Everything should be disclosed and discussed. All studies have known biases/problems. Getting out ahead of these problems is the best way to deal with this.","For transparency, should explain how missing data were treated.","I am really interested by this topic. As a former biologist which then turned to do an MSc and a PhD in statistics I often worry about the use of statistics in biology, and try to promote a better use of statistical methods in ecological research. Would love to hear results from your survey. /  / I found the last question hard to answer and potentially misleading. The header column was about a temporal aspect (often vs rarely, say) but the universes being compared were very different in scale. So of course if one considers all institutions, we hear all the time questions being raised about serious issues like plagiarism or made up results. But if your talking about close collaborators, the universe is much small. So while there might be the temptation to say that researchers believe that their collaborators are better than the rest, that could just reflect the size of the spaces being compared. "
"37","Anything where you are testing a hypothesis should be used. There are times, legitimately, where you can ""snoop"" data and this should always be reported as such. There are good statistical practices for this. Sometimes this results in no new information and so can just be dropped. Overall, it is (or should be) a rarely used practice that occurs under defined circumstances with it clearly reported as checking data for potential hypotheses rather than a test of a hypotheses. ","As before, it depends on the nature of the variables in relation to the specific question.","Beyond being intellectually dishonest, there's no point to it. If the unexpected result is interesting and can be explained, then it can be published without pretending to have anticipated it. Simple rhetorical devices such as "" . . . in retrospect . . . could have predicted this result"" can also be used. ","Because of p-hacking issues.","better to report the whole p-value","Data points should only be excluded if they are thought to be genuine outliers sampled from a different process than the main experiment (due to a quirk/error in the execution of the methods). Unfortunately, sometimes their outlier status is only discovered after a test is done. In such a case, I believe it is justified to drop a datapoint if a methodological quirk can be identified that resulted in its outlier status. An outlier should UNDER NO CONDITION be dropped unless there is a clear reason methodologically that resulted in its outlier status.","Collecting more data for a study after first inspecting whether the results are statistically significant is justifiable where statistical power is limited by low n.","biases results, but can also be due to model not being formulated correctly (e.g. variance structure). Going through multiple models and ending on the one with the best fit is a common practice as you cannot predict the data structure accurately.","Evidence should be honestly reported so that readers can judge the reported conclusions.","fraud","I am self-tought in data analyses. Might have not understood all questions properly."
"38","As above.","As long as it is acknowledged that the non-significant covariates exist, this could be OK. ","By the way, I suspect that this happens mostly in cases where the unexpected finding somehow can be used to bolster original argument, but less likely when the unexpected finding challenges the finding.  And we may not think we're doing this -- we may explain it away to ourselves as a data artifact.  Either way, we should be more honest about the 'messy' process of moving knowledge forward.  ","Because some models are tested that then are discovered to make no sense, or do not add anything to what is already presented","both show marginal significance. /  / But why 0.049 is significant and 0.051 not?","Data should be cleaned beforehand with sanity checks and not afterwards with the reported model. Alternatively, an outlier process can be included in model selection. If these steps are somehow intractable, then, at minimum, discarded outliers should be reported.","collecting more data is always good - we should base our science on as much data as we can. Whether we collect more data after getting a result because it did or did not reach some threshold is immaterial, it will make our inference better. ","Changes should be guided by a better understanding of the data and features of the statistical methods not p-values","Failing to disclose problems with methods or data is misrepresentation, and I doubt many people do this. If they do, they are setting themselves up for retraction when their data are checked by another group.","Fraud","I am very concerned about evolutionay biology. Most researchers, including essentially all senior researchers, have a grave neglect for good theory. What they pass off as theory are degenerate just-so stories, which they 'confirm' with questionable research. Originality is largely absent. If I were World President, I would defund all evolutionary biology research, so that one day on the ashes perhaps some productive research could be done. But I admit that I am not equally familiar with all subdisciplines of evolutionary biology, so perhaps my take is too dim."
"39","as at my response on covariates - depends critically if the variable is just potentially confounding or of interest!","As previously mentioned not all analyses can be reported. This includes both significant and non significant results ","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","Because statistical models were established by math methods. Different models have their advanges and disadvantages. More models tested can get more realy results.","but really who cares?  borderline significance is still borderline significance","Data should be excluded only as a last resort (after all attempts of transformation are made), and only if the assumptions of the test are not met. However, in my opinion, choosing a different statistical approach is preferred over deleting data.  ","Collecting more data looks as a good thing, however if the motivation is to reach significance...","Changing analysis is no issue, unless the user is trying to ""force"" a result. Otherwise, sometime one can realise an analysis is not appropriate after they inspect the results and they are completely unexpected","Find me the space in a journal, and relieve me of the repercussions, and I'll give you the whole list of provisos every time.  Otherwise, being overly honest is unlikely.  Clearly, honesty is the best policy, but we don't have appropriate norms established.","Fraud","I am very intrigued to see the results of this survey at some point! Good luck with the study..."
"40","As previous","As stated, our practice is to report significant and non-significant results of tests, as well as co-variates. Much of these problems can be eliminated with the use of non-linear, machine learning methods, which we have largely adopted in my lab. In these cases, lots of variables can be included in model and statistical tests without needing to remove them, or failing to report them.","Data exploration analysis ofter offer I way of look at a dataset from a different point of view that they original aim. For instance, in ecology are common observational studies of species composition in multiple habitats. Maybe the objective of the study was just to assess some diversity level among habitats, but another researcher may use the same dataset to assess community assembly processes. ","Because this is not the most appropriate approach. We should be shifting to model averaging and dumping the p value!","By definition, porbabilities are real numebrs and so have an never-ending series of decimal places. Whether you report 0.05, 0.051, 0.0511, you are presenting a rounded off number. This is fine so long as the reason is solely to present results efficiently. But where the rounding changes the interpretation of the results (significant or not significant) you should report an extra digit. ","Data should only be excluded when there is a biological rationale for doing so.","collecting more data will always help in strenghening or weakning some results, so should be a good practice, but lack of time and money prevents this","Changing analysis is only justified if the analysis is inappropriate, which may take some time to discover - not if you don't get the answer you want.","For me, this would be dishonesty","Fraud.","I am very keen to see results for this survey.  I'll email about it too."
"41","As said before, it's important to report negative results as well, so it should be acknowledged. Also, not reaching a given threshold doesn't always mean there is nothing there, you shouldn't based everything on thresholds","Assumedly there was a reason for testing these covariates in the first place: if you tested them because they might add to the 'story', and thus did not include them because they did not add to your case, then it is tantamount to reader manipulation and deception.","Data mining has a place in science, and at least with my own research, I often find interesting results that were not part of a hypothesis testing regime.  However, I think it is not an ethical practice to essentially lie and say after the fact that these data mining exercises were specific hypothesis testing.  Rather, one should explain the data exploratory process and not misrepresent the results.  /  As with my answer to the last question, I am more forgiving of these arguably unethical practices provided that the author explain their reasoning and pre-emptively address any obvious criticisms of the practice.","Biases the p values","Cases in which rounding are irrelevant as key information (e.g. 0.000089 -&gt; 0.00009) are fine.","decide on outlier exclusion before running stats,","Collecting more data will certainly make the results more robust, so I see no big problem here. However, for practical and/or logistical reasons, this is often not possible, at least not in my study system.","Changing statistical tests simply to reach a statistical threshold is disingenous, but changing statistical tests because, after conducting one, the researchers realize that it is not actually a good test for the effect they are trying to measure (e.g. trying to measure a linear effect and then realizing that the effect is likely to be non-linear, thus requiring different statistical tests), is unfortunate, but not completely illegitimate.","for obvious reasons","Fraud. ","I do not see how you can expect researchers to accurate report their own questionable research practices.  When looking at one's own research, entirely honestly, one may be blind to practices that one would condemn in others.  I do not know how individual researchers can have more than anecdotal information about others' questionable practices.  In cases where I responded that a practice should NEVER be done, is it possible that some circumstance might show that to be unjustified?  Perhaps.  In sum, I am very skeptical that a survey of this sort can yield useful data. "
"42","At times a variable does not contribute much to the 'story' and reporting becomes mostly distracting, so not reporting is an ethical practices. /  / Some reviewers will tell you not to report those results,so you become careful about when you actually report n.s results. That is not a good reason, but it is a reason anyway","Assuming this practice hasn't been done to alter the results, there probably isn't anything wrong with not including uninteresting covariates from your model. Again, this is only all right if it does not affect the outcome of the model and helps make a more clear point.","Data used to generate a hypothesis should not be used to test it.","Biologists frequently explore their data using a range of approaches, before deciding on which are the most suitable for the dataset (e.g. by further reading on the applicability of each model, ease of interpretation of results, etc) - these preliminary data explorations do not need to be reported. /  / However, it is NOT acceptable to select the final analytical approach on the basis of whether or not it gives a desired outcome.","Clarification: it is always OK to round p-values, but never OK to do so for the express purpose of meeting a threshold! Usually, &lt; signs clarify: 0.054 is not p &lt; 0.05, for example. I always interpret p-values on a continuous scale, except when there are large numbers, in which case I may discuss numbers or percentages in different categories.","Deciding to exclude data points after first checking the impact on statistical significance or some other desired statistical threshold should be avoided because decisions must be made a priori.","Collecting preliminary data to see if there is some evidence for your hypothesis is often required to get funding or to decide whether it is worth the time and money to collect a larger sample size. I have had a funding agency provide 'proof of concept' funds to collect preliminary data that on its own is not sufficient, but provided sufficient support for the hypothesis and methods to gain full funding later to increase sample size.","Changing the analysis needs to be justified. Often we use a suite of analyses and then choose from those but try to avoid choosing a result based on a per-desired outcome.","Full disclosure is always necessary for the purpose of producing reproducible experiments. ","Full disclosure is necessary.","I dont like the use of 'should' for all questions.  I use the stats that I think are appropriate, and I make my students conform to my standards.  However, opinions on statistical practices vary a lot, and I am hesitant to be perscriptive if work is done in a slightly different field or using different methods from my own.   "
"43","Because it biases the whole of the ecological literature and contributes to the pollution of good evidence with bad.","Be open, and this should concomitantly be accepted by peers","Define unexpected. The fact that you were exploring the data in such a way to begin with might be construed as expecting some sort of similar result. Often, as ecologists we know a ""little bit about a lot"" with our systems. Locking in per-defined hypotheses a priori can/could stifle research.","can't pick and choose tests, but should use most appropriate test, regardless of results","Clearly this is unethical behaviour in science.","Decisions about the data should not be made based on the outcome of statistical tests unless those tests reveal some underlying issue with the data (e.g., typo, errant data point, etc.)","constitutes multiple testing","Changing to another type of statistical analysis after the analysis initially chosen failed to reach statistical significance or some other desired statistical threshold should be avoided because the appropriate statistical analysis must be selected a priori.","Full disclosure is always the best option, but often receives negative feedback from editors and reviewers. The definition of ""known problems"" is important in answering this question. While ecologists should be aware of deficiencies in many statistical methods and data sets, some might argue that they are unaware of these issues.","Hard to believe that more than a handful of people do this","I felt I could not really estimate how common these problems are in Evolutionary Biology or in my own field. However, I would guess that trying alternative but appropriate statistical tests to nudge a p-value from not quite significant to significant is widespread. "
"44","Because it diminishes the quality of the study - you have to account for the number of tests performed to know how many false positive and negative you expect among all your findings.","Because it gives a false idea of how many statistical tests were carried out","Depends on the quality of the study / paper. If it meant that the outcomes were still 'correct' and the paper was easier to read, then the act might be ok.","Candidate models are often tested that are not appropriate; those results are not meaningful and  is the researchers responsibility to  make the appropriate choice for the final presentation of results","Consistency should be used when reporting statistical results and results should be reported to a reasonable level of accuracy (e.g. to 2 or 3 decimal places). If a rule is chosen for a given paper (e.g. report everything to 3 decimal places) then this may result in rounding errors that fall on a favourable side of the threshold, however it would be equally likely to occur on the other side. Note that if results are reported to varying levels of precision in a paper it should be readily noticeable as unusual and would hopefully be picked up at the review stage. Note also that a more realistically quantitative interpretation of statistics such as P-values rather than strict adherence to an arbitrary threshold would lessen the advantage to this behaviour in any case.","Decisions about whether to exclude data points should be made without knowledge of what the effect will be on significance, and a statement should be made in the paper reporting whether excluding those data points changed the significance level.","Could be problematic but also useful. Not too different from doing a pilot study before something larger.","Changing to another type of statistical analysis upon failure to reach statistical significance can sometimes be for a good reason. It may not be assumed tweaking or cheating, as often researchers could be prompted to investigate their data given finding marginal statistical (in)signficance, in terms of normality, homogeneity of variance, etc. For instance, changing analysis is not inherently unethical if someone only forget to log-transform their data, which allows them to achieve statistical significance thereafter.","Full disclosure is the only proper way to report science. Let the reviewers decide","Here you start with clearly unacceptable ussues. Of course it is a step to data fabrication!","I found it difficult to assess the amount others in my field may be making these mistakes (or conducting questionable practices). The only time I ever detect it is when I am reviewing and usually the issues arise from younger researchers unaware of how to present the information. Only once early in my career did I doubt the integrity of a senior colleague, which was because their earlier reported data seemed impossibly good. "
"45","Because it provides a distorted view of the real distribution of results, biasing it towards those that show significant differences. ","Begs the question of why you are trawling through covariates in the first place","Dishonest & likely to contaminate the literature with spurious results","Come on, no one wants to read a laundry list of various models you have tried in your research. Please be aware that the purpose of model is to help you understand the system better. So the focus should be on how good your model explain the observations, not how many models you have tried. May be I misunderstand the question again. ","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","Depends if the there is noise in the data (measurement error can be excluded) or if the datapoints are robust (must not be excluded)","Could be used for 'pilot study' or power analyses...","Choice of model should typically be based on the nature of data, not the significance values.","Geez, I really hope this is rare. ","Highly unethical","I found myself thinking that in many cases the answer would depend a lot on the scientific question being asked."
"46","biased representation of the data. But then again, journals want positive results.","Better to report that the covariate was included but didn't increase the predictive value of the model and was then removed. That way it's clear what was done and why.","dishonest.","Complete reporting of all models is fundamental to assessing the model selection process. ","Depending on the context (I.e., a study testing a hypothesis), it could be fair to do it. However, it may lead to reporting p-values such as .049 as significant","depends on reasons for outlier and leverage. Reporting on both sets of outcomes shows influence of outliers which is helpful. ","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","Choice of statistical method should be based on data characteristics and study design.  Again, if one knows the truth statistical methods are not needed.","gives false perception of precision","I'd consider this fraud.","I guess you need to be a little careful with interpreting the results. I felt that there was some ambiguity about some of the questions (the threshold in what constitutes a positive answer here was occasionally unclear)."
"47","But it really depends on the nature of the variables in relation to the specific question.","Biases reported results. ","Dishonest.","Could be used when different suites of models yield the same result and you then pick one approach to report in your article.  Shouldn't be used if you get different results with each test, because then you don't know which one is revealing the true patterns in your data.","Depends on sample size ","Deviating data points can be excluded but only based on (i) biological reasoning or conservative statistical criteria (e.g. Sokal & Rohlf 1995). Otherwise two analyses, with and without the deviating point(s) should be presented. Using a nonparametric alternative (e.g. robust regression) is another good option","Data sets are not representative if the sampling size is based on significance.","Choosing models based on optimality criteria (such as the likelihood) in a reasonable fashion (crossvalidation etc) is well established. Optimising significance must not be used, of course.","Hard to judge with a specific case.","I'm not aware of any cases where one even needs to do this - e.g. missing data can be treated as latent variables and estimated in a Bayesian modeling framework.  Clearly just filling in missing points is wrong & can lead to bias and overstated precision.","I have about 20 y of practice in Ecology and I see the problems involved in the issues presented here. However, I am quite sure this is not obvious for young researchers. I hope you are able to make this clearer. "
"48","By definition most variable pairs have no causal connection, so not finding statistically significant associations is not by itself a surprising result or one that needs reporting. The more important questions have to do with the arbitrariness of the threshold, whether significance testing is the most appropriate metric, etc. ","Can be used if its explicitly mentioned in the paper","Disingenuous - just admit that you've discovered something along the way rather than lying.","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","depends on the journal instructions. neither of the two examples make any difference  / 0.05 is still not less than 0.05 ...","Examining outliers can be really useful, often there's a lot of ecological insight to be gained from them. But I would always present results both with and without outliers, even if I decide there is a good ecological reason to exclude one or more points.","Dense sampling is very important in statistics.","Clear ""fishing"" for significance, although in some cases this can be valid if ALL models are properly disclosed as a sensitivity analysis","Here again, I truthfully don't know if people are doing that. This probably happens but in my area of research prople tend to build very complex models that account for uncertainty in the data: this is part of the statistical practice.Too perfect data are usually considered ""suspicious"" in quantitative wildlife ecology, or at least extremely rare. Trying to hide this would not make any sense... ","I'm not aware of any valid reasons to do this. This sound like data fabrication to me.","I have heard a number of stories / rumours of serious misconduct at other institutions, but as I have never encountered this personally, I replied to these questions in the negative. However I do believe that it is out there. Most commonly these stories relate to plagiarism or theft of ideas, rather than manipulation of data. "
"49","Can't report negative results very often because of journal preferences for significant results.","Completely meaningless and irrelevant variables could be interpreted as ecologically meaningful if always reported on","Dredging data for hypotheses might be a reasonable thing to do if ones plan is to go out and test these on new data, but is a terrible thing to do if limited to a single data set.  The potential for biased inference is large, and seems like a clear case of academic dishonesty.","Depending on the scientific questions it is good to test all options possible to analyse the obtained data set.","Does anyone think that p=0.06 has materially different consequences for a hypothesis than p=0.04?","Excluding data is a serious data, and should be used with serious justification.","Depend of field. Sometimes is ecology the exp can failed because of extra like weather etc but maybe in cancer research is easy to add mew samples. In ecology you should have a big n because is easy to have fails. ","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","Honesty in the presentation of the methods is a crucial key of scientific publications, and because no ecological is perfect, methodological issues properly considered in the analyses and discussion of the results shouldn't be a limit to publication","I'm not sure of about a percentage estimate, but simulated data should always be disclosed as a matter of standard scientific practice.","I have produced quite detailed texts when addressing your questions "
"50","Context is critical to this question but it is unclear. If the question is about bias towards positive results then yes it's a huge issue in the literature still, despite being recognised as such. ","Covariance are necesarily important ","Encourages, just-so stories, we can always come up with a suitable explanation and prediction. The key point here is to avoid doing so without noticing.","Depends on the situation. A designed experiment should stick to an a priori model, but in an observational or descriptive study, there are fewer potential problems because the scope of inference is constrained from the beginning.","Does not make any sense - e.g. 0.05 is already an arbitrary threshold! ","Excluding data points because they reach some criterion for being outliers is fine, as long as you report that you've done this and show that your conclusions are robust to this decision. You should include these data in your publicly available raw data anyway, so others can check whether you've done a sensible thing.","Depends on the case and question. If I do a pilot study and it looks promising, its worth spending more money on a bigger survey. But I would not strategically sample so long until I have 'what I want', if that is your point. Commonly a stats analysis is only available a while after data collection, so returning is often not an option anyways. ","Data never fits a model perfectly. Changing ones mind about a model in the face of data seems to make sense to me. Nevertheless, this should be reported.","honesty is the best policy, to the best of your knowledge","I am in favor of rigorous data imputation, but the methods should be clearly explained. I really hope no one does this without reporting it, but I wouldn't be surprised if some people did.","I hope my answers are clear and helpful. I appreciate someone doing research that examines the strength and integrity of scientific research. I really do believe that is important to be 'idealistic' about the scientific method and doing things to the best quality and methods that are currently known. I do believe if you can not do something properly (unflawed, unbiased AND ethical) than you need to wait until you can. But, I feel that is hard and some people become bitter and compromising because of the pressures and perverse incentives in science to get the metrics to obtain the job/ funding/ promotions. While some competition is good to stimulate innovation, too much competitive pressure for jobs/ funding is also not good for science."
"51","Could be seen as data dredging and result sifting (bad), but I think this would only be used in prelim analyses when trying to gauge trends. I think authors wouldn't always report this because journals and reviewers would not want to know. ","Covariates of what? My answer to a lot of these questions is ""it depends"", because they're not very precise. For example, If you measured a bunch of variables that are hypothesized to be related to productivity, and some of them were and some of them weren't, the ones that were not related are as interesting as the ones that were, because they challenge some hypotheses, so of course you would report them. If you're just trying to control for a covariate in a model and it's not significant, I don't see why you still wouldn't just report the full model, because it shows thoughtful, statistical control. Only when removing a covariate increases power and all of a sudden a pet variable became significant would I have a real problem with the practice of not including and reporting all measured covariates. I think we have some good incentives to report them in general, though. ","Ex post facto changing of your a priori predictions is ridiculous and dishonest","Depends on type of research (exploratory vs pre-registered hypothesis).","Does not meet threshold","excluding data points can be justified in some cases, but it has nothing to doo with statistical significance. It must be done before statistical evaluation based on clear arguments","Doing pilot studies is a common and legitimate practice.","depends on how much you trust the analyses. I don't think this is necessarily a bad practice, or cheating, because actually few people really knows about statistical power, assumption violation and so on. In many cases, you apply a different method and then significant results appear... /  / THe cheat seems to be, in this case, no reporting all approaches that have been tested (and saying explicitly which ones ""worked"" and those that did not).","I'm a bit of a zealot when it comes to reporting contrary data and discussing caveats.  I think it's very important that almost any author discuss the possible weaknesses of the study.","I am never fan of filling in missing data. These data should be excluded. Only when missing data cause massive exclusion of additional data points this could be considered.","I hope your research help us to understand and overcome bad practices in science. Thanks."
"52","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","Covariates should be all reported, whether they work or not. Reporting ""models tested"" is a good way to do it. WIthout this piece of information, it's impossible to tell how good are the ones that work.","Existing constraints on publication make it very difficult to ""tell the story"" of how the research progressed in a paper - with a 5-paragraph introduction, one might not have space to explain that the original research question was something quite different, but led to surprising and interesting results that are the focus of the paper.","Depends on whether the set of tested models is informative or not; see comment about not taking the reader through the history of the thought of the writer.","Doesn't matter so much if it is 0.54 or 0.5. It should just be honest","Excluding outliers can be a good practise if the detection of outliers has been performed before other stats and in case of experiments or field data, we are sure that the outliers correspond to abnormal individuals. But the exclusion of data searching for a significant results is just cheating the results removing the individuals which don't fit with our hypothesis. That is completely wrong.","Ecological data varies widely due to natural variation. If the first test was non significant and you have resources to test a larger population then it may be worth doing.","depends on the context","I've never knowingly done this - but I'm no expert on stats. ","I am not in favor of interpolating missing data","I mostly guessed at the percent who do these practices. People don't announce when they manipulate their data. "
"53","degrees of freedom are only correct when the number of statistical tests is known","Covariates usually aren't that important for the ideas being explored in a given manuscript, but they might be of interest to readers interested in other, related ideas so are worth reporting but not essential. ","Experiments should always be constructed with a hypothesis from the start, and designed to test that hypothesis. Although it is not ideal practice, sometimes other hypotheses may arise along the way based on a more advanced understanding of the system. It would be a shame to prevent these hypotheses from being tested if they are more appropriate - however, these should only be considered if the experimental setup tests these hypotheses in the best possible manner. ","Depends on your hypothesis, data and aims, but it should be better communicate all models.","Even though too much attention is paid to whether a P value is above or below a threshold, this is deceptive","Excluding outliers is sometimes unavoidable, but it must not be based on the result, because this inflates Type I error rates and thererfore leads to spurious results.","Ecological studies usually have restrictions related to sample size. The main assumption is that strong effects could be detected under such small sample sizes. However, it can also incur in apparently negative results simply due to a limitation of the statistical tool. Increasing the sample size at this low range of values is not likely to increase the chances of committing type I error.","Depends, as noted earlier. ","I always try to explain in my papers why I chose a test and what alternatives might have been.","I believe it's not necessary to identify exactly which individual data have been simulated. And sometimes (e.g. with plant trait data) even ""observed"" data are obtained by proxy (e.g. average trait values for a species because we can't measure all the individual plants). But the methods description needs to summarise how many missing data were imputed.","I recently discovered a nice discussion on p values in peerJ from Amrein er al."
"54","Depends again whether you want to find the best model or have a priori factors to test","Covariates, and predictions about their effects, should be stated up front as a priori hypotheses. It is bad enough to simply list covariates, but not even reporting some is terrible. Also, failure to report does away with information about likelihood of prediction bias (resulting from high covariate - observation ratio) ","Explicitly stating this would be disingenuous, but admitting the exploratory nature of a study and then looking for patterns is ok with me.  ","Doing this makes no sense","Every value should be rounded off the same way. But I don't really ""believe in"" statistical thresholds anyway.","Excluding outliers should only be considered statistically (following specific test) and exclusively to meet statistical assumptions, and not statistical significance","Ecological vs. statistical significance is a problem. However, this is something the reader should take into account and why statistics such as effect sizes or predictions should be reported, not just significance. The fact is, something must be significant for publication in a major journal. Researchers will take steps, such as collecting extra data, to make this happen.","Different statistical tests have different underlying assumptions, and how well these assumptions are met is not clear-cut (e.g. visual inspection of residuals). In most cases different tests (or models) will indicate similar trends (otherwise I would take it as an indication of something else going on!)","I am 100% for data transparency. All code and analyses should be made available such that every analysis and figure can be EXACTLY reproduced as it appears in the final publication or report","I can't think of a situation where it would be necessary to mislead readers about this.","I said mild questions about my own research once or twice but that mostly refers to older research that uses some analytic methods that are less in vogue today than they once were and I would probably take a different approach analyzing them now."
"55","Depends on the scope of the study","d)   Optional Question:  Why do you think this practice should or shouldn't be used? ","Exploratory analyses are great, but should not be reported in this way. I blame journals mostly for this rather than authors as exploratory analyses are rarely accepted for publication.","Ecological data often overdispersed or meet only to limited extent statistical assumptions. Meaning that several sets of model need to be preliminary tested to select the more appropriate. It is a part of the data analysis and doesn't need to be reported in details","Exact p-values are a better representation, even though p-values close (either above or below) a threshold (ie 0.05) are not necessarily any different. It does still have a high effect on whether a result is accepted as 'true' or not by reviewers, in my experience, so I see the temptation there.","Excluding points after data inspection (but before statistical testing) can be appropriate if there is a good reason.  Shaping the data to confirm a hypothesis is not a good reason.  And this step should always be reported.  The statistical result that would have been achieved with those points included, can also be reported.","Ecology is a field in which statistical power is difficult to calculate. As a result, sample sizes can be smaller than needed.  I don't have a problem with testing the generalisability of a finding by extending the study.","Different tests, different ways to look at the data, different types of conclusions to be drawn. ","I am guilty of not calling attention to aspects of experimental design that were not ideal when really I should have fessed up to these in the Discussion.","I cannot imagine people do that. There is one exception though. If you need a continuous time series (e.g. meteorological data) and you lost one point, it is better (more appropriate) to fill the point by the mean of the two neighbors (and mention this under 'handling data gaps') rather than have a gap creating an asymmetric data representation for technical reasons. In may actually be incorrect not to do that in special cases. But reporting is key. So there is no general good answer to this, although basically 'data creation' is a no go. ","I think ""mild"" problems of data analysis in Ecology often arise from poor statistical knowledge, not from intentional data distorsion.  / "
"56","Ecological significance is not the same as statistical significance","Data could be reported not specifically though, e.g. if all variables are stated, and then only those which show significance discussed, by default the remaining variables might not be reported specifically.","Exploratory results often prove to be false on validation. This should never be done as the results from exploratory versus confirmatory analyses have completely different meanings. ","Ecology is so complex that there are often endless models that could be constructed or tested to explore particular questions. It is therefore not always possible to present ALL candidate models, but the researcher should present the most logical models that make the most sense","Exact p value must be reported always, but more important is to report effect sizes.","Exclusion of influential points should not be used unless it is thoroughly discussed (repeating analyses with and without the point) or in cases where they originate from errors (e.g. data entry)","Evaluating the power of the data set is a necessary step in research design. As long as the data collection procedure is unbiased (i.e. not designed to produce a statistically significant result) this is not a practice that would negatively affect the study or scientific process.","Doing different tests to determine which works best with your data is different than picking a test based on significance. I have tried different tests to see outcomes.","I believe in full disclosure of methods, assuptions and potential problems, and discussing their implications","I cannot think why anyone would do this for honest reasons.","I think if I had made more experiments/field works I would have been involved more often in dubious scientific practices."
"57","Evolutionary biology is a parameter-rich field. There are many questions where one might ask if one factor is associated with another. To perform science, one must not just investigate hypotheses, but craft a message that will convey what you learned without being off-putting to read. Not listing offhand comparisons that did not reach significance required in a publication is a judgement call; if that comparison was part of a clear, discrete module of comparisons it should be reported. But if it was a kind of exploratory query as data was being investigated, it becomes cumbersome to track and leads to unreadable papers that will impede scientific communication to list all of them.","Depend on the research question, all variables are not necessarily relevant. ","Exploratory stats very often report findings with some ""verbal predictions"" to explain them. The thing is, exploratory stats are what their name suggests - exploratory. Journal practice makes it almost mandatory to report exploratory stats as checking verbal predictions, but this is rarely the case in reality.","Exploratory analyses may examine many models, including some considered spur-of-the-moment without much of a theoretical rationale.  We can have some bad ideas and there is no reason to include them in a candidate set.","First, 0.05 is a convention, not god given, second it depends on the power I have. Adding further decimals could be seen as preudoaccuracy. What I do not like, is the abundant use of &gt; and &lt; . I rather prefer to see the actual p values. ","Exclusion of outliers should be dictated by the distribution of data and variance and not by the p value","Experiments are expensive!!! You can not analyse all at the beginning obtaining only negative results which no journal wants to publish","exploring statistics is good - perhaps the next analysis that you wouldn't have thought of is better for the data or hypothesis. And if it gives the same result it makes you more confident you reached the right conclusion. But you should always be honest enough to acknowledge when there is a discrepancy if there is one","I believe that such honesty in this respect is often ""punished"", by reviewers and editors - which is a problem.","I do not think anyone do that. May be I am too naive. ","I think some of the practices you asked about were explained a bit too broadly and could be interpreted quite differently by other participants."
"58","Exploratory analyses are often not worth mentioning","Depending on the study approach, this is almost inevitable. But it should be noted that such things were tested but found to be non-significant. You can't fill a results table with the actual stats of all the negative results you find, for example, when doing a genome scan test with many environmental variables. In such a case, I report the effort clearly, but not the specific negative results.","Exploratory work can yield important results. But they should lead to the development of sharper hypotheses, that are then tested.","Exploratory analysis is a key component of ecological analysis given the number of correlated variables that are equally plausible a priori","for a start a p value is an arbitrarily derived threshold, not a magic value set in place by god (see the whole basis of AIC based approaches or Bayesian).  Aside from that if you are interested in 2 dp you round to the nearest 2 d.  Rounding 0.056 to 0.05 would not be good, but 0.054 to 0.05 seems fine to me.  Effect sizes are far more relevant than significance from applied perspectives and these should be considered more in paper.s ","Generally, this is unacceptable. Sometimes, however, there may be an a-priori reason to remove certain points that the researcher did not consider before the initial statistical analysis.","For all your questions, there is nuance. The nuance here is that it must be clear to readers that intermediate analyses were used to decide to gather more data. It is never OK to keep collecting more data until one has the desired results, but it is OK to try to clarify ambiguous findings as long as one is open about the process. For example, I don't prespecify my analyses before conducting them because most are exploratory anyway. When I find a strange result, I always go back and double-check the code, re-run the analyses with other R functions or packages, verify with other data sets, etc. In an exploratory analysis context, this is equivalent to gathering more data. And it is fine as long as I don't just find one result I like and publish that, but publish an honest representation of the full range of results.","Finding the best model to fit data can involve fitting several disparate types and comparing them. This would include using, for example the simplest lm and a complex GLMM and demonstrating through residual fits or AIC criteria etc that one better fits the data than the other.  /  / Sequentially using different model 'types' until one gives the significant result you desire is wrong!!","I guess that the only case in which this might be OK is if it is reasonable to assume that all informed readers would know the problem.","I don't see the point of doing this.","I think that currently there is a shift happening in the way people carry out and report their research. I certainly try to be much more careful now."
"59","Exploratory data analysis is okay in a limited setting (e.g. pilot data); however, when conducting an analysis that's to be published, I think it makes sense to identify a list of variables worth testing and to report effect sizes for each.  Otherwise, one runs into a form of non-response bias (which can lead to publication bias, etc.)","depends if covariates were important part of study design (in which case should always be reported), or part of exploratory work (e.g. after field season I felt like snow pack might affect germination but didnt collect great data on it as it was a post-ho hunch, so cobbled together the best data i could and it turned out there was no relation so i didnt explore it further).  i think there are lots of exploratory analyses that no paper has room to contain, so as long as its not covering up a fishing expedition expect that not all explorations will neccessarily be reported.","Exploring data is much weaker evidence than testing pre-defined hypotheses. Although both methods are important, the reader needs to be able to assess this information.","Exploratory modelling has a very soft edge, some decisions are clear cut but others are more subjective. Different people put that boundary in different places, so we should give readers sufficent information to judge our choices.","For reporting simplicity it is easy to use symbols to report rounded numbers but this omits a level of precision in reporting","Highly unethical. ","From certain statistical viewpoints this is perfectly OK (e.g. Bayesian updating).  However, it is generally not. /  / My work generally involves deep sea data, where the thought of 'going and collecting a bit more data' is completely out of the question.","First type of model may have been inappropriate for data. Scientists may not have been aware of more suitable techniques until part through analysis.  / If methodological reasons, OK. If trying to get a particular result, not OK. ","I have no idea how often this is used - but it is not appropriate.","I guessed at the percent who do. / You should report your methods, so not reporting is bad.  / In ecology, it should be fine to impute some data values, so I don't see much temptation to hide that you imputed some data.. / ","I think that most people strive to be as honest as possible, but the pressures of publishing in shiny journals and the desire to make the results elegant and easy to understand can compromise some results and reporting. I think honesty in the methods is most important; selective reporting of results with honest intent and thoughtful consideration of artifacts driven by multiple testing is acceptable."
"60","Failure to achieve significance adds equal information about how to interpret scientific findings as finding significance does, although studies that lack significance can be difficult to publish.","Depends on the scope of the study. If the covariate is not in the focus, it can be omitted. But needs reformultion of hypothesis","Failed results also are results.","Failure of  full disclosure; absence of  information on number of models fit (an important determinant of likelihood that one fits by chance  alone). ","For representation is is necessary to round to a certain digit and if an equal sign this is formally correct (p = 0.05). For p-value it is standard to report if a value is below a certain threshold (e.g. p&lt;0.05) and in this case, rounding is not correct, but this was not the question here as I understood it. ""p=0.05"" is different from ""p&lt;0.05"". / But may be I misunderstood this question...","I'm against excluding points on almost every occasion. If done, the reasons for doing it should be clearly presented.","Gathering data is expensive. And journals only buy significant results. We need to sell stuff and produce the stuff at the minimum possible cost. Exploring if a given dataset has a pattern before conducting a full study seems reasonable to me.","From my point of view, in this case, you are not manipulating the data but looking for the best way to analyze them. The only point to be considered here is to keep a clear coherence with previous hypotheses in order to avoid bias for your expected results","I hope I have never done this. Certainly I frequently recommend and practise being upfront about problems.","I have never heard if people doing that BUT it would of course be very bad practice","I think that studies with low sample sizes (esp. in nonexperimental disciplines such as paleontology and vetebrate conservation ecology) have a reputation for dubious statistical analyses, and yet the perceived impact of these papers gets them published anyway. There isn't much a dinosaur researcher can do if they have so few specimens available, but I would be fine with strict statistical rules that would prevent them from publishing research prematurely. I doubt major journals would agree."
"61","First, to elaborate on my ""never""... I have certainly failed to publish analysed results in the distant past (my PhD), although I wouldn't say this was due to non-significance, more lack of time and commitment. However, had these studies produced simple interesting significant results, would I have been more likely to publish? Yes, I think so. /  / To answer the question... Failing to report n.s. variables seems the same as the first scenario. Failing to report non-significant studies has a similar effect and is also wrong, but I find slightly more forgivable due to the pressures on academics.","Depends on the type of study: in experimental studies with clear setup and predictions, all measured covariates should be included in the final analysis, and reported regardless of whether or not they reach statistical significance. By contrast, in correlative and exploratory (field) studies (which often measure many different traits/variables without clear predictions for all of them), I would not mind seeing only a subset of all variables reported in detail in the final analysis (I would, however, like to know that they were measured and excluded based on some widely accepted criteria such as AIC).","For any Bayesian statistician, like me, surprise is a fundamental part of evaluating _any_ empirical finding. And lying about surprise is, well, lying.  ","First I'm not sure what you mean by ""complete tested set"". I take it as reporting the results from the models that have eventually been used, while not reporting all the models that have been tried. /  / I think this is common practice and it makes sense, because often we have to explore various options before figuring out which one fits our data best. Reporting all the exploratory statistics that have been done would complicate the manuscripts tremendously, and wouldn't be good for science in general.  /  / Having said that, we SHOULD NOT cherry pick the models that give the results we expect, but select the models that fit the data best.","For the sake of precisions and objectivity, rounding practices (including the direction of rounding and the number of significant figures to include) should be decided by the research a priori, rather than after seeing the p-value. Rounding with the intention of reaching a certain threshold is unacceptable (though the actual meaning of a p value of 0.05 is nearly equivalent to that of 0.054 to any reader who understands what a p value measures).","I'm guessing on the percent who do.  / Eliminating data that way is a form of faking data, so it is dishonest and undermines science.","Generally not something that is justifiable. However, if there is good justification the data must be collected with maximal (demonstrable) attempts to avoid any sort of bias (i.e. double blind, by other researchers etc).","Generally I find using &gt;1 method for data inspection and testing useful to examine whether biological conclusions are robust to variation in statistical approaches","I like to at least put those statements in the SI. However, I might not emphasize them. But I would never lie about them, and actually often would bring them up in a research talk or something like that. If I thought there were a real problem, I would not publish that data.","I have never heard of this happening, although potentially I presume it could. It would be the same as making up the data entirely. ","I think the questions are all pertinent, however I also believe that a simple display of the survey outcome can appear more negative than it is, as many exceptions exist. Especially because many of these bad practices are related to the obsession for p-values, but many modelling studies do not make much use of p-values anymore. I believe some of the questions are far less critical when applied to other modelling approaches. Very interesting study btw."
"62","For percent who do this, I made a wild guess. / It seems silly to not report a variable bc it was not significant, because that is still a result.  / However, journal word limits require brevity, so it is often necessary to omit fruitless or discarded inquiries.","Depends on whether covariates in question were of interest in the study","For me, the main reason for doing this, would be to improve the flow of the story. I don't see much harm in that.","Fitting too many models leads to overfitting and the readers should therefore be told how many models were fitted (even if not all can be shown). There is a bit a grey zone with exploratory data analysis, which is important but does not necessarily need to be reported on in detail.","From my point of view 0.054 or 0.05 are basically the same and should be interpreted as marginally significant and it should be explained so in a paper. But I am ok with rounding in the presentations at conferences to avoid the smartasses that try someone just to put down.  ","I'm totally uncomfortable with data-editing practices and believe this is not part of good scientific practices","Generally people should design a thorough experiment in advance, but sometimes it is necessary to check for an effect before you invest lots of time and effort in additional experimental work with more samples","Here again, it depends what causes the lack of significance. Sometimes, it is when trying to interpret non-significant results that one may think about other issues that need to be handled, and these cannot be handled with the same kind of analysis. So, I would not consider it is systematically wrong to do so. But it is definitely not a good practice to try another type of analysis JUST to see if it would yield significant results...","I mean, this practice is obviously against the spirit of scientific inquiry and transparency in the communication of findings...","I have never seen or heard of such behaviour (outside economics). / In fact, even after reading the chapter on data imputation in Fox et al.'s Ecological Statistics, where it is made clear that not filling in data missing not at random can cause bias, I am still very reserved about inventing data at all.  / I routinely estimate missing data (e.g. in Bayesian analyses, where it happens automatically, but maintains full uncertainty), but I never use or report these data points.","I think the wording ""should"" in the questionnaire is problematic as it implies a strong opinion on each point."
"63","Generally non-significant results should be published, but if you have plenty of data that should be published soon, it is better to write a paper about those results that are more probably accepted to some decent journal than those that no journal wants to publish. However, if you have a study where some variables show significant and some non-significant results you should ALWAYS report them all if they are linked to the same ""story"".  ","Depends on whether the negative result might be of interest to anyone.","Generally I'd say it's poor practice, but I have had reviewers and editors push towards this in the interest of decreasing the length of manuscripts. ","From my point of view, in this case, you are not manipulating the data but looking for the best way to analyze them. The only point to be considered here is to keep a clear coherence with previous hypotheses in order to avoid bias for your expected results","Honestly effect size approaches are better anyway, so this type of p value should be paired with an effect size and argued to be biologically significant or not. It is frustrating to have arbitrary cutoffs and little understanding by reviewers of effect size approaches, however","I am interpreting this question as not including a presentation of alternative results with and without the inclusion of outliers. ","Generally you should always design you studies such that you have enough replicates originally.","I'd advise using the best statistical method first, and if such a practice is carried out it is essential to quote and publish both analyses, indicating the marginal nature of any such result","I never did that when I was leading a study. However, I have co-authored papers in which I disagree about some these issues. This was a matter of opinion. I believed data/procedure was not good enough, but other co-authors disagreed.","I have no idea how much it is done by colleagues so I left it 0%","I thought some of the questions were phrased ambiguously, and didn't think the answer choices were that appropriate. It may be that something isn't necessarily a bad thing to do, but that doesn't mean you feel it ""should"" be done at some frequency. It's more along the lines of what is acceptable practice. "
"64","Generally not good practice, however given career pressures I can understand the lack of enthusiasm for writing up non-significant results, which will never form the basis of an important publication.","Depends on whether you want to test the effect of some factors on your test variable or whether you want to find the model best describing your data.","Great question! A philosophical one for sure! While I strongly believe in the tenets of the Scientific Method, and believe that great science is rooted in great hypotheses, the actual practice of science and experimentation sometimes doesn't follow suit. How many great findings of science were uncovered via some fabulous accident?! I think this is more a problem with the way journals accept articles rather than the science itself. Traditional journals have a legacy of format, such that oftentimes these surprise findings need to be presented as if that was the planned research question the whole time. The alternative would be that only science that was previously conceived should be carried out to completion, which I think would make for science that is prohibitively limited in scope and wonder! In addition, these exploratory findings can often lead to great future planned experiments.","Full set of models should be summarized in supplemental data, but subset could be listed in main manuscript due to space limitations. Most readers will gloss over the entire table anyways, but it should be available ","Hugely skews the collective output of the field! Absolutely unacceptable.","I am taking this question (because of the use of the word ""checking"") to imply that the not-significant P value was the motivating reason for exclusion and that the data point would not have been excluded except for the desire to ensure the P value was significant (even if there were good reasons for exclusion). /  / Inevitably, it is not uncommon to examine P values with and without exclusion of data, either just as part of procedural work before exclusion has been performed or because of an explicit desire to see both. I do not think there is anything wrong with this practice in data exploration; it is inevitable and in my opinion it is not philosophically problematic. But the exclusion must have a clear reasoning completely independent from achieving a particular P value.","Given the costs of experiment or field work sometimes it is cleaver to focus on the most promising variable. Make sense for me ","I'm sure this is legitimate so long as there are other reasons for favouring the new analysis, besides its delivery of ""significant"" P-values.","I see that regularly in manuscripts I review, and it is some times evident in some published studies. It should not be done consciously as it is a clear bias.","I have no idea if this is done or not by others","I was rather disappointed by the questions - the percentages can only be wild guesses, and for the other questions there was obviously a right answer that I usually resisted because I dislike dogmatism."
"65","Getting ns findings accepted for publication: easier said that done","Depends on which conceptual statistical framework is being used. Of course, you must always report that some covariate was not important, but sometimes there may be practical issues about journal practices, publication space and so on. Moreover, you can never by definition know what are the actual covariates, so it is more important, in a more experimental or classical inference framework, to explicitly define the hypotheses and null hypothesis/model being used.","HARKing - a no no.","Fundamental violation of the premises of multi model inference ","I'm not against rounding, but it should never be used ""to meet a threshold"".","I assume that this question is asking about the intent of the researcher to generate a statistically desirable test result by omitting data points.","Goes against the entire ethos of stats. ","I already used this strategy, but I found out after some time that it is not ethical and agree that indeed it is not. Inflating the number of ""positive results"" by choosing methods that corroborate hypotheses is not a good way to advance in science. Unfortunately, I admite that this is a common practice in ecology. ","I suspect a much bigger problem is ignorance of problems, rather than intentional deceit.","i have used data imputation, but indicate this. it should be always be mentioned in results. the process of data imputation given the right circumstances if justified and articulated might be appropriate","I welcome this initiative; it would be useful to raise awareness about practices associated with Type 1 error and publication bias in this field."
"66","Given space limitations in journals, you can't include all potential analyses. ","Didn't we already go over this one? ""Not reporting studies or variables that failed to reach statistical significance (e.g. p ? 0.05) or some other desired statistical threshold.""","HARKing dishonestly exaggerates the prior support for a hypothesis, and this overconfidence flows into the interpretation of the result.","Generally, this is poor practice. The only reason to do this is that it can be too complex to describe multiple statistical efforts, when some methodologies later proved to be incorrect applications. This should never be done just because the tests do not yield stat sig results","I'm not aware of any negative consequences to this practice.","I assume this isn't just removing an outlier for a suspected reason. That is okay but removing without reason for significance isn't treu to the data.","Good experimental design should have incorporated variation across individuals into the sampling design. However, some traits are quite labile and if there is evidence for this it would provide support for further sampling. But, this should be made explicit in any manuscript with appropriate justification. ","I always use it when a p-value is close to 0.05 (below or above), to check and to avoid trying to publish results with low statistical support, which would be ethically questionable. There are almost always a variety of statistical options to test the robustness of a result, and when several methods converge, I conclude that my results are robust, otherwise it suggests that I am wrong or that I might be wrong by giving the only method that returned ""significant"" results. Fortunately, the review process routinely requires such cross-checking when p-values are close to 0.05, which ultimately educates many researchers to do it systematically before they submit a paper.","I think it is often the case, but not necessarily because the researcher intends to cheat. Sometimes the paper maximum length, together with many other important things to discuss, lead to the choice of not discussing everything if not considered necessary (no doubt that what is important is always arguable!). It should also be considered that high impact journals generally want a simple, fluent and ""not boring"" story to tell, so it might be a strategic choice.","I HOPE this is rarely done!","I worry that concern over fraudulent researchers (who should be sacked) will lead to stultifying institutionalization for all other researchers (who should not be punished for attitudes they never had)"
"67","Hard p cutoffs make limited sense, but may occasionally serve as a filter when faced w/ multiple tests.","differ if the covariate is of interest for research question - then DO report. If potentially confounding variable you prefer to see how covariate with ur variables of interest (to make sure you do not miss something for interpretation), than omission of its reporting is fine, I think. I concur with that in my research and other likely do the same.","Here I assume you mean the start of the entire project. Often through data collection or initial site selection a likely mechanism becomes clear which you'd then examine in the analysis stage. But if you mean the start of analysis, I'd change my answer to no.","Giving the reader as much info as possible (i.e., reporting all competing models) is preferable, but the reality of space constraints does not always allow for it. It also makes the writing more complicated, because it sends the reader on tangents that may not be necessary","I'm not bothered by it for P &lt; 0.05, and I might have done it for P close to 0.01, but not to make my results look more significant. For P &gt; 0.05, then it's shady and shouldn't be done.","I can not guage how many people do this.  While this practice is deplorable, the revise practice is often necessary.  That is, sometimes data points need to be excluded to determine if they have an undue influence on the statistical results. /  / ","Had to avoid if you run the stats when you get back from the field each time. ","I am ambivalent on this. As long as I have a good justification for the new test being used, why not? Alternatively, I put all tests right at the beginning as the tests I am going to use (and justifying how each of them is slightly different from each other). Overall, I do not think this is a major bad practise even if Nakagawa chooses to differ!","I think it is probably a bit underhanded, but the publishing game dictates that sometimes we don't reveal all the details unless challenged by a reviewer. ","I imagine it's often used to make figures look clearer, which is not appropriate but may not influence the overall findings.  I'm sure it's also used to bulk up sample sizes and even drive a statistically significant difference, which takes it away from ""questionable"" and moves it into outright fabrication.","I would also be interested in questions about positive things like replication and the repetition of experiments, and the generalisability of results."
"68","How is this different from Question 1?","Even non-significant covariates should be reported as they are part of the results","Hypotheses should drive science, not data mining.","Here my assumption is that you mean exploratory or early stage analyses? This is often required for students who need to report preliminary results at candidature reviews, or diagnostics may show the model was inappropriate so it informs selection of an appropriate approach","I'm not sure this is a major issue. Reporting ""P=0.05"" is taken to indicate borderline significance. The more serious error would be to round from P=0.054 to 0.05 and then report ""P&lt;0.05"".","I can see this being part of someone's approach to outliers, although that's not really appropriate. Other tests and justification for why a given sample might be problematic should be used, and then this whole process and its impacts should be reported, at least in the SI of a paper.","Having more data makes for clearer results so it is probably always a good idea. The only problem might arise if observers are biased by their knowledge of the significance of variables from preliminary analysis, but there are methods for avoiding this.","I am not sure this can be applied often... I mean if results are not significant with one type of tests, I don't see why they should be with a different test - at least in my discipline, it can't be possible. A test just help your data to talk, if your data don't have anything to say, you can use whatever test you want, it won't be relevant at the end.","I think most often researchers fail to disclose potential problems because they are unaware of the problem. However reviewers sometimes see problems that are not relevant. Reviewers also make this mistake both by obsession about sources of error that are negligibly small and by being unaware of sources of error that may be important. ","I just don't think many people create data like this.","I would have preferred more nuanced questions about each situation, instead of having to describe my thinking in the comments. Still, important questions to ask. Glad you're looking at them and best of luck!  "
"69","How is this different from the first question?","Even when the effect of a covariate is not significant, the inclusion of it can improve the detection power of other effects. The inclusion of the covariate can be justified, for example, by the previous findings.","Hypotheses testing is important (and influences the statistical test used). So many papers I review have no hypothesis and when I point this out a post-hoc hypothesis is formulated. The very worst type of science here.","I'm assuming in this instance the unreported model was valid. If there are legitimate reasons for not reporting results from a test, I see no problem there.  /  / If not, there's still nothing wrong with publishing your results, as long as you provide data on why you favor that test.","I've put a high value (90%) because many journals used to REQUIRE you to report threshold significance levels, so all but the youngest researchers have probably had to do this. These days I would never do it, and I'd fight an editor who asked me to.","I cannot understand why this would be done. A data point is a data point. I don't see many justifiable reasons for excluding data if the experiment had been designed properly in the first place. ","I'd only see a point in increasing sample size if low sample size was the reason for failing to reject the null hypothesis. However, data collection should be done following the same design originally set out.","I avoid this practice now, but have done so in the past. I don't recall a specific instance. I think it sometimes appropriate to change methods during analysis (e.g. you may discover a more appropriate method you were not previously aware of), but these decisions should never be contingent on statistical thresholds.","I think that often people feel defensive about their methods and analysis, and worry about getting them past reviewers.  This is an area where I believe a cultural shift in the field could be helpful.","I must admit I've never heard of this one, which is just plain illegal (""fabrication"").  Remember what happened to Stapel.","I would like to do research ethically and openly, though I can see how people get tempted to take short cuts. The intense pressure on researchers both from their peers and their own-selves is difficult to balance with the ideal way to do research."
"70","I'm not sure one size fits all here. If you undertake an analysis that you realise is flawed in some way I don't see any reason to have to report it with the final, well-justified analysis. However, not reporting multiple equivalently valid analysis should be discouraged.","Failure to mention what has been looked at opens up problems around multiple testing.","Hypothesis can be unverified ","I'm not against people exploring their data. The fact of the matter is, that we can fully explore so many different parameters these days, so many different sensititivity tests, but is it really worth it to report every single one? I find that many of my supplemental information's are now running to 30+ pages. Often I'll submit a paper with many sensitivity tests and statistical explorations of the data, and find that reviewers demand even more. It takes an incredible amount of time to keep up with these requests, let alone properly draft figures and explain them for every other little test I've run. So, often times I'll leave out sensitivity tests or different statistical models.","I always report P to 3DP (not sure why...just habit) and try not to take a binary view of statistics  (ie &lt; or &gt; 0.05).  P=0.051 is no different to P=0.049 so I don't see any ""value"" in rounding down. However, if you are reporting to 2DP I don't really see a problem with it either. The problem is in people wanting a yes/no answer when a P value is continuous!  In other words - if folk are fudging this one I think its because reviewers don't underestand P values","I do not find the procedure of selecting data to produce nicer statistics meaningful. But occasionally the data checking as figures is done at the same time as statistical analyses (e.g. in the R environment), this may show outliers that are reasonable to remove. ","I'm guilty of this, although my experience is that the extra data almost never change the result.  I think this practice is ethically debatable and I admit I have mixed feelings about it.","I believe this picking of statistical methods to get desired results is one of the most widely used abuses of statistics in ecology. However, it is ok if one can justify the choice with reasons besides that they give a significant result. For example, if the method avoids an assumption about the data that the data do not strictly meet. ","I think this has become slightly more common. I review a lot of papers and there are some authors with a pattern of doing this.","I really hope this isn't happening.","I would like to known the result of this survey. Congrats for being addressing these issues!"
"71","I agree that non-significant findings should be published (after all, we did learn something from those non-significant results), but the reality is that those studies are much harder to get published. And in a climate where you have to be productive, it is then ""better"" to work an another manuscript that has data that is easier to publish instead of working on a paper that will take more rounds to get out and will likely only make it into a lower-tier journal.","Flom 2007 ""Stopping stepwise : Why stepwise and similar selection methods are bad , and what you should use"" is a great argument against this practice. /  / I am assuming that your question refers only to selecting model covariates based on p&lt;0.05 and thus I am not considering those who ran a model and decided not to present the entire thing. /  / I don't have a problem with this practice provided it is not the sole criterion for removing an effect. An effect can be more safely removed if it has both low significance AND a low effect size. However, not many researchers consider the second criterion. I would personally not do model selection any more, and I would never recommend that it is done on significance alone.","Hypothesis should not be generated post hoc by nature. However, in some cases researchers can be driven to do this because it can increase the appeal of a manuscript to journals.","I'm not proficient enough at statistics to know whether this is warranted or not. ","I always round p values (if I present them), but never because of a threshold (I don't take thresholds seriously).","I do think it's OK to report ""what ifs"" in scientific papers (""If Sample A were withheld, the R2 would rise from 0.54 to 0.88""), and to discuss why such points **might** be outliers. But the data should be presented as they are.","I'm not sure about this one. Does this apply to pilot studies? Trying to figure out what should be studied and what could be left off? Seems like an okay way to save research dollars.","I did this before realizing quite how bad it was. Again, this is fishing for results - test procedures should be set up in advance, and should not depend on the result of the test in terms of significance.","I think this is a problem that many run into.  For instance, all assumptions of a particular model might be met but it is the best thing available so people use it anyway.  In an ideal world, minor defects in one's modeling armor would be made visible for all to see.  However, when reviewers are looking for reasons to reject, drawing attention to the possible inadequacies of an analysis approach can help cement a manuscripts' path to the rejection pile.  I guess I'm a little more forgiving with this one: clearly if there are major flaws with the analysis approach or method, these need to be corrected or described properly.  However, when there are minor issues that don't impact inferences as much, I'm not as critical.","I sincerely hope that my estimate of 1% is too high. This smacks of data fabrication. It is my sense that exceedingly few scientists stoop so low.","I would like to note that I rarely work with statistics in my work with simple lab systems (therfore answered mostly with ""never""). I get my opinions from how I hear other people talk about their research (mostly medical) and how they (and colleagues) go about statistical analysis. I might not be the right audience, but I'd like to let you know that the practices mentioned in the questionnaire are at least thought to be applied, undermining the regard for statistical analysis. I know they are applied -to some extent- (people told me they did) ant this makes me skeptical towards statistical foundations of a statement/result. I work with ""obvious"" differences and because of this skepsis often I don't see added value of putting a statistical evaluation to the results in my work."
"72","I am commenting here not about the use of such or such threshold, but about the fact of publishing positive-only results: we should be allowed to publish also negative results, but it is often rejected by editors as 'non novel' or as 'should do more experiences to confirm/infirm patterns'. / I have not published a particular data set because I am still not sure what to tell about it, since there is no particular effect nor expected variation, but instead weird values that contradict each other","For completeness and for the benefit of future researchers, I think it is important to state all results, both significant and non-significant.","I'm almost certain I would have done this at least once when I was young - I'd undertake a study without clear predictions or an idea of how to analyze my data, and then figure out how to make it work. It shouldn't be done because it muddies the process - the statistical model is meant to test your a priori predictions.","I'm not quite sure what this question is asking. I do a ton of exploratory analyses before I conduct my ""real analyses""--they serve to give me a feel for my data, and they also serve to alert me to potential convergence or other issues in my ""real analysis"" (like, when a result doesn't make sense based on my feeling for the data, I know to re-check my code and model, etc.). Publishing these initial analyses, the ones that give me a feel for my data, would not be helpful to anyone. I do not agree with the practice of not publishing other, feasible, candidate models that were tested in the ""real analysis"", at least in an appendix. However, if your understanding of your data/system changed based on subsequent analyses, and you felt that some of your early models didn't reflect this new understanding, then I see no problem with not reporting earlier models. ","I am not familiar that this is an issue. p=0.05 is an arbitrary value anyway, so it does not really matter. We should be focusing more on presentation of effect sizes!","I hate the very idea of 'outliers' and eliminations. I would only ever exclude data which I knew to be erroneous / or from a specific subset of data, but then I would do this BEFORE my stats, not after.","I'm not sure what the question is asking. If it's about a pilot study, I (and others) often do that. But if it's about a stopping rule, most of us don't do that. The question, however, is poorly phrased.","I do not do data dredging. ","I think this is not a recommended practice at all. But I must reckon I have done so in the past. This practice arises, in my opinion, basically as a result of the researcher's knowledge limitations about which methods he/she could use when there are violations of the model assumptions. For example, not many researchers are aware of the modelling options they have when their models fails to fulfil the assumption of non-normality (e.g. using GAM or non-linear models), homocedasticity (e.g. moving to a different error family with GLM), or overdispersion in a Poisson model (e.g. moving to a negative binomial error family or including observation-level random factors in your model). /  / Data limitations are also barely reported, and I think this might importantly impact the conclusions of a study, as much as model limitations.","I think it's fine to fill in missing data this way. Many statistical programmes do this by default.  Just say what you did.  ","I would subdivide the question about not reporting variables and studies into two questions - these are different issues."
"73","I answered a similar question before. If you measured it, why not report it? ","For me this was a modeling scenario when there were many hypothesis-driven options that we tried, and we reported what wasn't significant but didn't provide all the values ","I'm not sure what the problem with this is??","I'm not sure whether your question includes early exploratory analysis with preliminary data. As someone who is relative unskilled at modelling approaches, applying different approaches to one's data is a way of learning. Journals are not interested in publishing this sort of detail in one's Methods section","I answer ""never"" because it is silly. Ecologists, among many others, tend to be hung up on p-values, even if many cannot say what they are, let alone what they mean in a hypothetico-deductive sense.","I have certainly trimmed data, but only if there is clear justification for removing outliers. For sure, the tests are also run on the non-trimmed data (normally before any data cleaning).","I've decided to run an experiment again with a larger sample size if I feel the original run wasn't conclusive. But you shouldn't contiue to gather data until you get the result you want.","I do not see the point of this practice but to publish more paper; it is not honest.","I try to always report weaknesses.","I think modern data analysis and simulations have come a long way, such that researchers are not afraid to report if data were simulated. I don't know of any researchers or papers where simulations were not disclosed, but I am sure they exist. I think this is a blatantly crappy thing to do.","In general, I believe we should report ALL the data, as honestly as possible. Any type of cherry picking (data, stats, etc.) should be avoided at all costs."
"74","I consider the reason for statistical testing is in finding the differences between groups. The text do get loooong if all statistics are explained in text, especially when testing interactions of several factors (e.g. ANOVA) and not using predictive statistics. It is however, reasonable to present these in the tables but it simplifies the writing to focus on the significant effects. /  / There may be justified reasons to present also nearly-significant results if the factor is of high importance in the article.","for such things as body size, a normal covariate for body morphology should be used even if it isn't significant","I absolutely agree; this is a quite dangerous practice, increasing the chance to find and report spurious effects. However, I must admit that sometimes we have done this, as some screening of observational data without clear a priori assumptions often opens new views and helps exploring new emerging field.","I'm not totally clear either why someone would do this, or why this would be an issue. I'm not sure I understand the question.","I basically present 3SF most of the time. Sometimes that makes rounding errors in favour of significance; sometimes not. ","I have no idea on teh % of collegues using that so I left it to 0%","I am a macroecologist working with very big datasets. Thus, most correlations are usually vet significant (very small P values). In addition, field ecologists collecting data in the field usually do not have time for more collection of data, due to deadlines to be reached.","I do say, bad form.","Ideally it should never be done.  However, there are known problems with every methodology I have ever seen -- no methodology is perfect.  Usually one can take care of it effectively by citing references to the methodology, where there will be literature discussing the strengths and weaknesses of the methodology.  In the interests of space and readability, one should discuss such problems in proportion to the likelihood that they can alter conclusions.","I think this behavior is rarer, because it is downright fraud. But then who can check what exactly you have done in your lab? Most researchers would probably use other methods to make the data say what they want it to say.","In the earlier questions in particular, the more appropriate response would be ""it depends"" - the particulars of the situation seem to dictate what's appropriate. Is the goal of an analysis hypothesis testing or finding a strong descriptive model? /  / I suggest you go one further and ask whether researchers should do exploratory data visualization - this probably defines the analyses that people perform and what relationships they focus on."
"75","I did not report the result when I thought the power of my test was too low to draw any reasonable conclusion.","Generally, the covariates included in the analysis should all be presented in the methods section. Somewhere in the paper the author should therefore present p-values for all the covariates, although they might only highlight significant ones in the text.","I am a bit unclear as to what exactly this means--sometimes the framing concept of a paper changes based on an unexpected result (and I believe this to be very common, and often useful), but that seems different to me than presenting specific predictions.","I'm sure that most ecologists do not report all tested models (probably 98%), and I guess that most will not lie and state that they reported all tested models.  /  / Reporting all tested models is not really of value, given that many times one first runs some rough preliminary and exploratory stats and if the results look promissing, one then applies the more sophisticated and more time consuming correct models...","I believe 0.05 is a 'magical' cut-off and should not be strictly considered because for me, there is no big difference between 0.054 or 0.05 since these are really low p values. I feel that even 0.07 is such a low value that could be viewed as 'significant'; however in these cases (i.e. 0.05 &lt; p &lt; 0.10), I prefer to clearly state a 'partial significant effect', precise the exact p value, and interpret it as a potentially significant effect.","I have removed an ""outlier"", but I have also reported that in the paper (and presented results with and without the ourlier). Removal of outliers is highly unwanted, but if truly necessary (and you have good reasons for that) it can be done if you openly tell it. / ","I am actually not sure of the impact of such practice on statistical tests. ","I don't agree with fishing for significant results. But I certainly have advised others to use a more appropriate test  if they have made a poor choice.","Ideally now one should do this, but it's hard to expect people to fire bullets at their own research.","I think this is rare because most people in the field don't know about data imputation methods anyway.","Interested in hearing the results!"
"76","I do'nt understand - in a review you're asking should researchers mention studies with null results? Yes of course! that is what an objective review is all about.","Generally, you should not collect data unless you plan to test it and present the results. Collecting 20 variables and picking the best one to present is clearly asking for trouble. Sometimes, plans change and you might realise that the data you collected are not informative, and it's not useful to present them. Omitting them expressly to get 'better' results will clearly add bias though.","I am ambiguous here. An unexpected finding can stimulate more reading and thinking, which may change the way one would e.g. write up a paper introduction. For instance, one may notice to have overlooked an existing hypothesis that may predict the unexpected. As long as one does not pretend a posteriori to have tailored the study design specifically to detect the unexpected result, I do not see a serious problem.","I am not certain about whether it should be used, but all analysis pipelines do give you different models that can be simultaneously tested, and you identify the one that best suits the sample characteristics.","I do not do it, but statistically 0.054 and 0.049 are not really different. There is no magic in any particular threshold (and usually 0.05 is too liberal anyway. too bad AIC, these days method of choice, is even worse - far more liberal than even p=0.05)","I imagine there is a lot of posthoc justification for omission of troublesome data. There should be stronger pipelines for analysis that include outlier and leverage analysis to prevent this.","I am basically neutral to this practice. The detection power depends on the variability of the data and the effect size in interest. There parameters are usually not known before performing the study. And if the effect in interest does not have a significant effect, additional data collection should basically help us know its effect is not important, though I understand this practice can bias the conclusion.","I don't know. I think it is the reverse: you change of statistical test because you realize that the first one was not adapted (eg pseudoreplication needing to be taken into account), and then it happens that a previously non-significant effect becomes significant.","If a problem affects the outcome of an experiment and you know it, not reporting it is obviously misleading.","I was asked once, not really to fill in data points, but to pool some, so as to have less levels in a factor. This is slightly different, but likely not quite acceptable approach either.","Interesting questions - will be interested to see results. "
"77","I don't agree with this practice, but practically, we all have limited time and journals tend not to view negative results favorably, so I understand publishing these studies not being a priority.","Gives misleading impression of the number of tests performed.","I am an empiricist. We should report the story that the data tell us in the end rather than a story about our pre-conceived notions whether they turn out to be right or wrong base don the data. In other words hypotheses are important at the stage when an experiment is designed and proposed, but there is no reason you can't change your views as you learn new information from your work or other researchers work. ","I am not clear.","I do not feel strongly about this. 0.05 is such an arbitrary number - we may as well just move the goalposts rather than round to meet them.","I keep all my data points in. But I remember my statistic courses where we were told to look for ""outliers"". I look for them, but I keep them in my analyses because unless I have strong evidence to believe there is an error, the ""outlier"" may hold biologically important information.","I am linking this practice to pilot studies, which can be many times ideal to conduct. ","I don't think you can say this practice is good or bad: more to the point is which of the analyses that were tried are most correct in terms of underlying assumptions, and which is the most powerful. Most frequently, the analyses that do reach statistical significance are just the statistically more powerful approaches.","If I did do this, then I would jolly-well check to see if the issues did impact the results or not.  If they did, then it would be reported.  If not, then I might not mention it (or mention it only in passing).","I would consider this to be data falsification. ","interesting questions (where the real truth can be difficult to get)"
"78","i don't not report variables, but for studies with null results the pain/time commitment of trying to publish trumps the moral position. We all know we want null results published, but we all know there isn't enough time in the day to do this everytime - hence publication bias","High p-values can be due to low statistical power or lack of influence of the explanatory variable, thus one should not decide that an explanatory variable has no influence based on a p-value. Also, no effect is a contribution to science and are necessary for meta-analytical purposes.","I am perhaps misunderstanding the question, but while for some studies it is possible to design all experiments before hand and predict their results, for many studies there is considerable trial and error. Making a high quality publication requires us to organize the results according to a testable framework (fitting the results within the framework of existing hypotheses and proposed results): that may not always match initial predictions that set the course of the study. The saying ""We wouldn't call it research if we knew what we were doing"" comes to mind. Studies that merely report results without attempting to fit their results within the existing literature are often painful to read and show a lack of effort on the part of the authors.","I am not entirely sure I understand the question -- if a set of candidate models is tested this should be reported","I don't believe in the magical properties of statistical thresholds. ","I personally don't like throwing out data points that are often difficult to obtain in the first place (i.e. months of field work!), so try to keep everything in an analysis if I can.  However, if one outlying data point is massively skewing a result I'll remove it.","I am not sure about the question. / If the results are alredy significant (and the N is enough), why to take more samples? / Or the aim of collecting more data is to increase the N? / Or is to check that the significance can be repeated with other data? / If the aim is to increase the significance of some thata over other data (e.g. when making comparison of treatments), then it should never be used","I don't think you should change analytical approaches just because you failed to reach significance, but there may be other reasons to change your approach.","If I understand this correctly, this practice is rather close to falsifying the conclusions, isn't it?","I would hope that this is extremely rare!","Interesting subject area!  I look forward to seeing the results of your survey."
"79","I don't think it is a deliberate strategy to not report nonsignificant results - it's more of a matter that researchers have limited time, and that they prefer to spend most time and effort on getting their most exciting results out","I","I answered 'it should be used rarely', because while in principle this is not justified, sometimes unexpected results do lead to great scientific progress - and may lead a researcher to some up with interesting novel ideas. How the researcher, ultimately, presents the study, may sometimes hide that originally the finding was unexpected.","I am not understanding the question! What do you mean?: / A) doing a study about the performance of several competing methods and omitted the results from the several due to my personal convenience? That is so wrong! / B) Doing a study on a subject of ecology, choosing the method of your preference, and not presenting the results from other methods that you legitimately disregarded because they did not perform so well? That is legitimate!","I don't feel as strongly about this because p-values are rather arbitrary cut-offs. reporting large effect sizes and a p=0.054 with a power analysis would be more convincing to me that small effect size and p=0.04 with high power.","I really think excluding data of any sort, unless a methodological mistake was made, is bead practice. I did it once on the request of a reviewer but wish that I had not.","I am not sure about this issue. This seems to be unfair in frequentist statistics, but I am not sure whether it is acceptable on the Bayesian context.","I don't think you should change your analysis based on whether one achieved statistical significance. If you determine part way through that there's a better way to analyze the data, then go for it. But don't do it for the significance...","If it affects the conclusions, of course it is bad.","If data is interpolated, that should be reported. The actually number of physical observations is essential to report. ","interesting survey !"
"80","I don't think it is a good idea, because it contributes to the publication bias against negative results. However, I have received reviewer comments that it's a waste of space to talk about variables that were not significant.","I'm afraid the question is a bit ambiguous.  I do not report the details regarding the test statistics in the Results but do report in the Methods having conducted such tests.  I conduct field experiments involving a control and one or more treatments.  I collect data on variables that I think could confound the experiment, such as temperature or rainfall, and then test whether including any of these variables affects the main effects.  For example, I can't control the weather and it might arise by chance alone that all control trials fell on rainy days and all treatment trials fell on sunny days and the animals I'm studying like to stay dry.  I undertake tests of such covariates as a precautionary step and do not expect any of these variables to affect the outcome, and then report in the Methods that they did not affect the outcome.","I believe it should not be used but editors and reviewers often demand that exploratory results are framed as a priori hypotheses","i am taking this question to assume the situation where there are multiple ecological hypotheses to support multiple candidate models. generally, i think that this practice should not be used. it is too easy to 'select' a model that confirms your a priori ideas about the existence of a relationship between response and covariates. i could imagine that due to the sometimes iterative nature of science and the building on of previous knowledge, one might end of including additional covariates and not reporting simpler models, but the simple models were parameterised first. but something abou thte outocmes made one believe that the addtional covariate (based on ecological knowledge) was valid.","I don't have a major problem with this. The cut-off used is arbitrary and the models that I use are usually permutation-based (so re-running the models will give you a slightly different p value anyway). I think it is important to give the full result though (i.e. report p = 0.054) for clarity.","I teach my students that removing outliers can be done if justified. Any removal of outliers should be acknolwedged and justification should be presented.","I am primarily a field researcher, so my data reflects a point in time. It is generally not possible to collect more data in a way that is consistent with the original data. In rare cases, that may be possible.","I don´t see a problem here.","If it impacts the conclusion it is misleading","If simulated data are derived from existing data, then degrees of freedom in the model will be incorrect.","Interesting to be challenged this way. Thank you, this is an important issue to strengthen the already threatened reputation of science / "
"81","I don't think it is necessarily to report all results from exploratory analyses in a peer-reviewed paper. However, the peer-review process forces scientists to not report these results as there is little interest in publishing insignificant results. I do not think that this is at all beneficial to the field as non-significant results are also meaningful. I think the pressure from peer-review process is the main reason scientists choose not to report insignificant results.","I'm indifferent to this, but when used it should be clearly stated, e.g.:  /  / ""When a factor (main effect or interactions) in the model was not significant, the p value was higher than .25 and the proportion of variability explained by the factor lower than 5% we removed the factor from the analysis, and the model re-run without the excluded factors following [33]"" /  / 33. Engqvist L (2005) The mistreatment of covariate interaction terms in linear model analyses of behavioural and evolutionary ecology studies. Animal Behaviour 70: 967–971.","I believe most people do it because most journals (or reviewers) pefer texts which gives the impression that the main result of the study was already being pursuited from the start.","I believe it is impossible to strongly distinguish between data dradging and pilot (preliminary) analyses. Furthermore, we never can check this. Therefore, we have to carefully evaluate (as reviewers, editors, etc.) how convincingly the study hypotheses are introduced, statistically examined, and discussed. If something scientifically unappropriate was involved, it should appear. ","I don't have a strong opinion on this. It seems better to present the actual values but on the other hand the 0.05 and 0.01 levels are quite arbitrary so as a community we have set up a strong incentive to round down. I would prefer that we avoid using p-values altogether and focus on effect size instead.","I think in certain situations it could make sense to do this as long as you report it. For example, if you have one site or plot that is an extreme outlier and when you take it out you do not get a pattern/significant response, you should report this and say without this plot the pattern is different and here are some potential reasons why. You don't want to report on a model that gets a significant p-value if that response is completely dependent on 1 point. ","I am sorry to stop my comments, but like for the previous questions, I find the question ill-posed and not reflecting or pointing to a clear problem","i dont know.  but i have been asked to change analyses by reviewers / editors for a couple of articles and have done so.....results were very similar with the differnet types of analyses i did.....","If it is really likely that the error would change conclusions, then it should be disclosed. Only if it is an unnecessary complication to the paper and the results are very robust, might it be reasonably left out. ","If this happens, the amount of data replacement, and the details of those values (simulated, means, etc) have to be reported. However, better if one stays away from such a practice","It's a very interesting survey. / I think you should also have asked how many quantitative papers each participant has published, because most questions had an option to respond ""once"". If someone has only published one or two papers, ""once"" is a much bigger proportion than if they've published, say, 20. (In my case, the answer is 14.)"
"82","I don't think there is any scientific justification for not reporting such results. It is more a matter of prioritising projects and available research time - so an example of a conflict between individual research career requirements and the common scientific good. Also, ""not reporting"" can be interpreted widely... in my case I did report this work as a thesis chapter (so it is in the public domain) but never ended up publishing it as a paper because it was not as ""interesting"" as other results. ","I'm not sure. I can't think of a specific instance of this. What we certainly do is go in the field and measure a million things that we know won't all end up in a final model, but generally use a formal model selection/simplification procedure to get rid of the ""extra"" covariates. Dumping them without mentioning them seems unforthcoming. ","I can see why people do it but it seems a bit disingenuous.","I could see cases where you had variables that were measured incorrectly that you dumped, and then it would be reasonable to not describe them.","I don't hugely care about this--I don't think it's a big deal.  The alpha level is an arbitrary threshold anyway.","I think it's fine to test for overall effects, then to re-run analyses after removing outliers if there's a justifiable reason for doing so","I am under pressure to use as few animals as possible in my research. It is not possible to know effect sizes in advance, and it is sometimes important  to adjust sample sizes after looking at the data when effect sizes differ from what was forecast in one's power analysis.","I have chosen ""it should only be used rarely"", but what I actually think is ""it should be used when necessary"". I mean, sometimes we realise that the first model we have chosen was not the best approach only after we encounter unexpected statistical results (i.e. results different from what you intuition indicates while inspecting raw data).","If one knows there is a potential issue with the data, methods or analysis, the researcher should be transparent. / These problems often don't invalidate the outcomes of the analysis, but instead inform better the interpretation. Sometimes the data available is all there is on a particular problem - some data is better than guessing, so once these data are presented with the appropriate 'health warnings' then it can be legitimate to publish and try to develop better datasets.  / Other times, the model fit might not be as perfect as one wants - however, again, be transparent about this. There may in fact  be no good way of solving this problem at present.  / I think there is a fear factor for some researchers to be completely honest with the limitations of their analysis, for fear that the editor will not accept their paper. / I try to put in a 'limitations' section in the discussion in many of my papers - this happens more in vet epidemiology and disease ecology, I notice.  / As a general comment, I do worry about the sample sizes in some ecology papers published, with almost no comment on the potential limitation of this problem. / ","If you have large quantitative datasets, a few missing variables may be substituted. But I would always state this was done, and the method, even if the specific data points were not identified. (and this is in part because we do not submit raw data for publication).  /  / If data were submitted, then these points I would identify.","It's fantastic to see someone running such a survey.  Unfortunately, I can't see a way of making scientists be honest about themselves in an email like this, as they will be worried it will come back to bite them later (especially when more personal details are requested on the last page, and the respondents don't know how many people this survey has been forwarded to [i.e., it might be easy to track them if there are only a small number of recipients of Fiona's email]).  The only way to curb this might be to let the survey ""go viral"" so that hundreds/thousands of scientists respond.  Aside from the issues with self-assessment of research integrity, I think the responses given about *other researchers* will be very informative.  All the best with it!"
"83","I don't think we should be retiring statistical significance at all in 2017.","I've used it to save place in some papers, but it's a way of not reporting a negative result, which may flaw our general opinion (see my comment on the question about negative results)","I consider it should be used whenever such a finding contributes with the general question and hypotheses of one's study. It usually opens a new perspective of the subject and could be positive for the understanding of any pattern/process/mechanism.","I do not believe in data dredging and running many many models where you do not have strong predictions/ ideas of why each factor should be included, because the chances of getting SOMETHING that is statistically significant (but maybe not biologically significant) increases with the number of models that you run. However,  I think learning how to use models is really hard because there are so many approaches even within Bayesian vs. frequentist approaches. In fact, despite having published four studies with models (yes, I know that is not a lot and shows my early career status) and haunting stack overflow, I still feel like I may be missing something, that is why I try to be super clear to my readers why I have selected the models that I have. ","I don't like misrepresentation of numbers, but the threshold of statistical significance is a somewhat arbitrary barrier anywy, so the degree of harm that this practice could cause is rather limited.","I think it is OK do do that when both steps are reported and there are reasons for excluding a data point.","I believe that the only situation where this practice can be used is on ethical grounds, but only when a pre-agreed cutoff has already been made. For instance, if a study involves the use of animals with welfare implications, then checking your results before the pre-agreed sample size has been reached may mean you can stop the experiment with a smaller sample size than might be necessary/have already been planned, and thus cause less suffering to the animals you are studying.","I have done this in cases where I wasn't sure that my approach was correct. I admit that if the results had been significant I may have not tried other tests. However, more often than not I was convinced that the statistical approach was correct and therefore didn't change my analyses, even though the results were non significant.","If people know the analysis is appropriate, they should change it. ","If you knew the truth ....","It's irrelevant to the statistics, but for many of the questions I think there are multiple motivations for corner cutting or bad practices. In particular, I often find the hard and fast rules of statistics to be inappropriate for field data (model fit may be poor but there's no one that's good). It's also the case that it's just not possible to go through everything done in the Methods sections - a clear read is really important to getting something published and having an impact. Unfortunately, admitting potential issues (not even problems) can really sink you.  "
"84","I don't understand the difference between this and the last question","I actually enjoy reporting on all covariates regardless of significance (but then again I also engage in lots of qualitative research so trends and non significance are as interesting to me as statistically significant relationships)","I did in the past, to provide a more effective narrative. With the years, I have been trying to avoid it","I do not have an opinion on how often it should be used because I think this question is weird: let's say you're facing a novel data set, and you're doing exploratory analysis. Then of course you might test some stupid models, or even be wrong in the models you're testing and the fact that the assumptions underlying those models are respected or not. In those cases, obviously you won't report those tests. For any data set, there is an infinity of models to be tested. What matters and should be reported is the set of tests that we think is of interest regarding the hypotheses and questions studied, and regarding the trends in the data set.","I don't round p-values. I would honestly call those values you reported for this question ""marginal""","I think it is ok to exclude outliers but the best practice in my view is to report the outcome of the test with and without the outlier in question.","I believe this would be acceptable if absolutely no prior data existed to run a power analysis. If the effect size is thought to be extremely small, BUT ecologically or biologically significant, then this practice might be OK. However, just seeking to increase sample size to reach a threshold so one can simply report significance is not an OK practice","I have done this in the past, when I was young and foolish (and poorly trained).","If the finla analysis has issues then this shouldn't be used! But, if problems have been dealt with, or are the reason that a particular test was chosen, then it is difficult to include every issue encountered in space-limited articles.","Imputed data should be identified as such.","It's possible that I am not the intended recipient of this survey, as I have more statistical training than many of my colleagues.  Some of the statements are clearly bad practice.  However, others depend on the setting.  The focus on P values is also topical, but does not describe my own research very well (as I have largely moved away from these, except in specific circumstances).  Hopefully my comments are still useful though (and don't bias your distributions!)"
"85","I don't understand the question. Before there was a question about reporting covariates, and now there is a question about variables. How are these different? And why is this lumped together with citing other studies?","I addressed this practice in the previous question about dropping variables that fail to reach statistical significance.","I did it as a student, frankly I don't care that much about this but tend not to do it now.  Although you tend to start off with hypotheses for a study, sometimes the analysis suggests something else is going on.  For practical reasons you may write your paper suggesting this was the original hypothesis.  Assuming the experimental design is capable of testing that hypothesis I believe it make no practical difference other than one to the individual s own attitudes.  Being inflexible is so 1800's, to much pressure and too few resources are available not to capitalise on opportunities to publish assuming the design can test what you are talking about to be picky. ","I do think this practice should be used. Statistics are already complicated and hard to follow, thus adding ALL of the methods you used to eventually arrive at your final method would be confusing and superfluous.","I don't see any problem with this (but I am not a fan of arbitrary alpha levels anyway...)","I think it is only OK to do this if it is transparent in the publication. For example, some people will report the full analysis, and then report the results if outliers are removed. In this case, the reader can then evaluate for themselves which analysis to accept.","I can see a number of different reasons why one might want to collect more data, e.g to check orginal analyses, expand datasets etc.","I have done this only when I felt that the initial statistical method failed to account for an important characteristic of the data (e.g., switching from a frequentist to Bayesian approach allows incorporation of measurement uncertainty)","If the method or the data is flawed, it is a pity, but the study must be corrected or if it is not possible it must be thrown away. Again, cheaters certainly exist, but I want to believe that they are only a minority.","In the reporting of experiments, all details must be disclosed so that other researches can ideally reproduce the experimental procedures. Failing to identify simulated data points would prevent such replication.","It's very hard to estimate the percentage of ecologists who've done something without knowing who you're surveying - if you're asking about senior folks, every answer is higher (more likely to do it) than if you're asking about graduate students, simply because grad students/postdocs have published relatively little and have had relatively little opportunity to do the things described."
"86","I guess it includes all results never published because we focus our time on articles that are more likely to get published....","I am assuming you mean within a model, other aspects of which were reported?","I disagree to do this in principle. However, editors and reviewers are most likely will ask you to do this.","I don't believe there is any way of knowing whether this is done or not","I don't subscribe to the idea that 0.05 or 0.01 are magic numbers.  It doesn't make sense to me that 0.051 is not meaningful whereas 0.05 is considered significant.  I much prefer an author report the p values, but also briefly discuss them.  For example, if my p value was 0.056, I'd report it as that value or 0.06, but also point out that the data are highly suggestive and should not be discounted outright because they don't meet an arbitrary threshold.   /  / To clarify my answer to c), provided that the p values are properly rounded, e.g., I don't see a problem with reporting 0.05 if it's actually 0.05444444444... ","I think it may be OK to remove data points *provided that you explain your reasoning in the publication*.  Of course, this should be done rarely and should have a good excuse.","I can understand for measures which are time consuming. But usually we construct the experimental design based on our possibilities and expectations and look at the results only at the end. ","I have done this when I have changed from an ANOVA to an ANCOVA or regression type design when I realized the ANOVA was not using all the information available such as a covariate masking the result, which was then included in the analysis. ","In a perfect world we'd all list every possible weakness of the study - authors are usually best placed to notice these. However this practice is not sanctioned by editors and reviewers, so I'm often tempted to keep quiet about stuff I consider to be minor (though this obviously leaves room for my own confirmation bias etc).","Indicating where data points have been simulated is simple and transparent and improves interpretation and is therefore sound practice.","It has not become clear to me, what you are after. / Personally, i think scientific ""misconduct"" is very rare, but I still don't trust the vast majority of ecological findings (here and elsewhere). But that is not because someone is cheating, but because there is so much leeway in the analyses, and we were not taught to look out for its effects (see Ioannidis 2008 PLoS Medicine).  / So if your aim is to investigate, how much of research we can trust, I would go for a very low number. But if you want to find out, how often researchers do wrong at least half-knowingly, I think very rarely. I don't know if the questions allow you to tell between these two points."
"87","I guess it would depend on the model, how similar it was to the one ultimately being tested, whether it needed to be reported.  It's not a best practice, but I guess it could be understandable under a limited number of circumstances.","I am guessing at the percent who do. / I don't see the point in concealing insignificant covariates. You can have a table that lists all the covariates and then report that ""none were significant except X."" And if you report what was not significant, that can help others design their studies, or contrast their study to yours.","I do not feel that I can answer question a for any of these questions. / As for b and c - I once inluded some kind of finding (can't remember specifics) as a hypothesis. I think this is a bit different from this description of the practise.  / I think researchers might be pressured into including results as predictions or hypotheses because we are taught not to do exploratory analysis and to always use deductive approaches. I tell my students that scientists usually use inductive approaches and case studies but do not admit it. maybe we should be more honest with ourselves.","I don't have experience with extensive model testing regimes so don't have a strong opinion about this.","I don't think it's that important. I don't expect many people to to it. I expect most researcher to know that the difference between 0.01 and 0.013 is not important.","I think this could only be used if the manipulation was reported in the publication. Otherwise it is a stretch of the truth that would led to the scientist deluding themselves.","I did that as a student before I understood the problem with it. / Again, I am guessing at the percent who do that. / If you collect more data in order to get a significant result, you are too motivated to ensure the data comes out the way you want.  / If you stop collecting as soon as you get a significant result, it is not random sampling, so the results are not valid. / Also, this is another problem with arbitrary significance levels, because it encourages this type of behavior.","I have in fact been encouraged by previous advisors to do this practice, but have refused. The only time I can see this being a legitimate practice is if the original analysis has issues such as low power or is not appropriate to the data distribution and another analysis is discovered that is more appropriate.","In an ideal world, all decisions with impact on the results would be completely transparent and reproducible. Often these decisions are subtle and of course subjective but we simply lack standards and tools for corresponding reporting. Journals are way too restrictive (wrt space and detail).","Interpolated/simulated data should always be identified as such, because it potentially strongly affects the validity of the results","It is fairly rare in evolutionary biology to report only P-values; most studies report parameter values.  I would reject papers that focus too heavily on P-values because those are not particularly informative and few contemporary studies are that naive."
"88","I had trouble getting negative results published and eventually gave up. This is not ideal, but unless journals change their MOs publication bias is inevitable. Given that a study was published the core hypotheses tested should have p-values reported.","I am not entirely sure about this issue. In my research articles, I used to present only the final model including only significant covariates but explain which covariates were non-significant and dropped from the model. However, once or twice this practice was criticized by some reviewers. Since then I tend to report the full models. However, main conclusions derived from the two methods are usually the same, so I am not sure really which practice is better. ","I do not see the point. It seems arrogant. Scientist may be arrogant but this is wrong in my opinion. ","I don't know of anyone who asserts that the reported set of models is the complete set. People often do exploratory analyses. I have never encountered a manuscript reporting that the set of models reported in a paper was the complete set ever ran. ","I don't think there's ANYTHING wrong with rounding off a P value if it reported as P = 0.0X. However, in the question it states ""to meet a pre-specified threshold"", which implies you are asking about staiting P &lt; 0.0X, in which case it is dishonest and should never be done.","I think this happens more than it should. It is ok to remove data points from studies if it can be justified. Typically, my group handles this by leaving the removed data in the meta-data collection but omitting them from downstream analyses. Then, at least it is transparent.","I do not have anything against collecting more data if the original sample size was too small. I often work with remote sensing data, where it is relatively easy to change sample size. This is much harder for field ecologists. Often researchers do not know the amount of variability in their data until they conduct preliminary analyses.","I have switched from non parametric to parametric and tried to persevere with that in order to increase power - does this count?","In ecology and conservation, my research field. We lack lots of information - see Hortal et al. Annu. Rev. Ecol. Evol. Syst. 2015. 46:523–49","Inventing data? crazy....","It is good to think about these issues."
"89","I have sometimes omitted citing studies that I think are so flawed that they shouldn't be cited (given citations are almost always seen as a positive thing for a paper, even if you're trashing it)","I am not sure how this question differs from the last question. I will say this one is tricky though because a covariates might not be significant  if they are highly correlated with another variable and thus should be left out of the statistical analysis. However, then that logic should be detailed in the methods. So in a way they are reported but not as statistically insignificant.","I don't generally write papers that predict outcomes. I often present questions, or even hypotheses, but not predicted outcomes. I do not think that is a desirable way to present research.  /  / On the other hand, I certainly do have expectations about what I might see in my results, and have often seen unexpected outcomes. I find it effective to present these in articles as surprising, unexpected outcomes.","I don't know the occurrence of this","I don't think this is a great issue, just because what ecologists do is usually a pilot study anyway given we usually have relatively small samples sizes. Our statistical results are an indicator of a difference, and effect sizes and F-ratios give a better idea of the magnitude of the ecologically important result","I think this is problematic unless there was something odd problematic in the design that potentially caused the outlier that you can point to. Say if an experimental plant was accidentally damaged while being measured or an technical instrument was acting up that day.  /  / However, if that is decided and reported then  every measurement on that plant from the damage onward should be removed from the dataset- not just the measurement where it was an outlier. ","I don't have a major issue with this. I think that pilot studies can be helpful and then collecting more data will further test the hypothesis. I don't normally do this because of the usually lower power associated with the original test - so even if it isn't significant, there is no guarantee that the main test won't be.","I interpret this question to include, e.g., realizing that one should have log-transformed one of the independent variables after first running a model with an untransformed variable. This decision wouldn't be based on the p-value so much as the overall model fit and residuals, however. /  / ","Incidentally, I disagree with the phrasing of question (c) throughout. I'm not sure to what extent anyone would argue that any of these practices specifically SHOULD BE USED. I think what you are trying to get at is HOW PROBLEMATIC IS IT to use this practice, and some people might argue that it's not problematic which is different from saying it's a practice which should be used often or always. /  / For this particular question, I see that realistically given space constraints and overall flow that there are inevitably details of the dataset or nuances in interpretation that are glossed over or left out. How big a deal this is clearly depends on how much this impacts the primary conclusions of the study. ","Inventing data?? How can anyone take science serious if data are just made up? ","It is nice to quantify bad practices in science and congrats for tackling this sensible subject.  / I would like to receive a feed-back on the final results of this project. / all the best,"
"90","I have two opinions on this question. For variables that are not significant within a study, they should absolutely be reported. However, for studies with null results, it is hard to publish them and so I see why researchers would not bother to write the results up and report them.","I can't believe that many people do this for covariates that are important to the argument of the main paper. If they do, I can't believe that reviewers let them get away with it.","I don't like how question C is phrased.  Its not that I would advocate that other people do this (as is suggested by the word 'should').  Its that I don't think it makes a difference whether people do this practice or not.  They can engage in this practice or not, I don't think it matters.","I don't see this as a problem in principle, so long as it is done carefully.","I don't think this is a problem as the value of the p-value has some meaning on itself not just compared to some threshold so that whether it's 0.05 or 0.053 is very similar and doesn't change the perspective of what you are finding. So if you round-off at 2 digits, then it sounds logical that 0.053 is 0.05.","I think this question is talking about cherry-picking data and excluding data points that do not fit the desired pattern. This is like making up the dataset!","I don't have a problem with adding more data (I guess in practice in ecology this may be in terms of adding another years worth of data).  If its collected under the same methodology and you have the resources why not.  I have certainly had it stated in reviews that it would be good to have another years data, or if another years data could be obtained the editor would reconsider the paper.  More data will only make the result more robust, not less.","I often do just an opposite (conservative) approach. I use another analysis (if available) to check for robustness of my finding.","Intentionally hiding problems is bad. But we don't always know if we are ignoring issues. That is what peer-review is for. ","is misconduct","It seems to me that a lot of the common questionable practices (some of the early questions, in my opinion) probably are not done intentionally.  People dismiss a set of non-signficant results because ""we'll I guess there was no expectation there in the first place"", or similar.  Or some committee member suggests to a student that they are doing themselves a disservice by not fully analysing there data that are non-signficant so far, until they have tested all the interactions.  I should think few people look back on these things and self-identify them as having been questionable practice. /  / The other thing is reviewer pressure to do bad things.  In my experience, this is more common than has been acknowledged in discussions to date.  A lot of these things arise from misunderstandings about what is rigorous, and this applies to newbies as much as experts (maybe more to senior people in many cases).  I have had reviewers demand additional exploratory (not exploratory just because I didn't make some prediction, but where the reviewer gives no predictions), and then in a subsequent round of review complain that they are not presented in a null hypothesis testing framework (this is in the paper that I presume led to me receiving this email - we largely resisted and attached a very cautious interpretation to the most exploratory components).  I have repeatedly had (senior) colleagues and reviewers insist that it would be more rigorous to drop non-significant covariates from models.  I'm sure the factors nudging people toward questionable statistical practices come in many forms, these two types of case of mis-guided ideas about rigour from senior colleagues are just ones that have come up recently for me, and I think are possibly not as much part of the discussion as they should be."
"91","I may have misunderstood this question. It seems rather broad. There are many aspects of any study that do not get reported precisely because they are moot or trivial or mere sidebars. Who among us has not been frustrated by a paper that spews a litany of methods only to report on some fraction of the results but nonetheless zig and zag among irrelavancies?","I do not understand how this is different from your previous question on not reporting studies or variables... maybe a problem with my incorrect use of statistical terminology. ","I don't really have an opinion about this because I think it's a failure of frequentist statistics to map onto the actual practice of science. In very few fields do you conduct research linearly from observation -&gt; hypothesis -&gt; test -&gt; reject/fail to reject hypothesis. I think instead that inference should come from study replication but science on the whole doesn't support researchers who replicate studies.","I don't think it is necessary to report all exploratory analyses in all instances. Sometimes this is just carried out to best understand your data and to choose a more appropriate model. That said, I think it is good practice to describe these analyses in supplementary materials. ","I don't use p-values, but information criteria, or I focus on estimated parameter values. The same problem holds with confidence or credible intervals though (boundaries; or do they include 0, or not...). /  / The ""rounding"" problem is a never-ending one: all this depends on the number of decimal places you choose. It is hopeless to try to solve the problem. I prefer referring to Fisher: /  / ""No scientific worker has a fixed level of significance at which from year to year, in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas. Further, the calculation is based solely on a hypothesis, which, in the light of the evidence, is often not believed to be true at all, so that the actual probability of erroneous decision, supposing such a phrase to have any meaning, may be much less than the frequency specifying the level of significance.  (Fisher 1956). / ","I think this should be used to check the influence of particular points on statistical parameters","I don't know the considerations here. It seems that people are quite open about doing a pilot study followed by a power analysis to determine what sample size would be required in a continuation of the study for the observed pattern to reach significance.","I see no problem with exploring alternative statistical methods in a case where significance was not 'strong' enough - as long as the research question is well-designed and the data is adequate. But I think it then becomes important to briefly mention these 'changes' in the text (with necessary justification) or even present initial analyses somewhere in the Suppl material.","Is mandatory to communicate any problem or potential weakness with data and methods, in order to be able to open any clarificatory scientifc discussion.","Is necessary to collect data or at least to use simulations.","Just one comment: I wrote an entire book about this, but nobody is inviting me to teach from the book.  I bet it's not in your bibliography, either, but I can'task without breaking anonymity, can I?  Good luck with the dissertation; it's much needed."
"92","I often don't report non-significant results if I believe there was a problem with the experiment (low sample size, ineffective treatments, etc.)","I don't believe I have done this, but am not personally expert in all modelling performed in all papers I've coauthored. I think a full representation of model components should be reported to aid in reproducible research and in line with statistical best practice. ","I don't see why anyone would do this.  I don't see it as a detriment to one's work or reputation if variables pop that weren't necessarily predicted to do so.  There is no motivation to mischaracterize this.","I don't think it should be done, but I think it is done often.  I may have even done it, often when there are a very large number of models that either seem trivial or nonsensical in highsight, or perhaps with just some additional thought that occurred during exploratory model fitting.  ","I dont think I'd do this, and it seems trivial anyway, if the cutoff is p&lt;threshold not p=threshold. Whether you round or not you're in ns/trend territory.","I try to never remove datapoints, except if I know for certain there was a problem with sampling or experiments.","I don't see a problem with following up preliminary results with more data collection. But doing this based on P thresholds does not seem like good practice.","I see nothing wrong with exploring different analytical techniques - it should be encouraged. What is critical today is to fully deposit all data in on-line data repositories, allowing others to reanalyze your data with alternative methods if they disagree with you.","Issues with the method should be acknowledged in the methods section.","Is that the science of ""alernative facts""?","Looking forward to the results of this study. Also wanted to tell Nakagawa that I am a fan of your stats papers. "
"93","I personally remember less of non-significant results; moreover positive results are often emphasized in publications so they are less documented. Unfortunately I have a few manuscripts with non-significant results but am really concerned about how editors and reviewers will react when facing papers with non-significant results. For this reason, I prefer not further developing them","I don't have a problem with omitting some statistical results if they are not statistically significant and if they do not add to the story of the main paper. However, I always try to include full statistical tables in the supplementary information to ensure that interested readers have access to the full analysis if they want. Often this kind of practice is advocated by reviewers who want to streamline the messaging in the paper.","I don't think it is a problem to use exploratory analyses- the author should just be clear about it. ","I don't understand the question","I feel it is always best to present 2 significant digits for P values, and do not like arbitrary cutoffs such as 0.05. I personally only put much faith in results with P&lt;0.001","I would say that it is very possible that these data points might be true outliers, e.g. data mistakes, and then correcting the analysis is sensible. This should be always done with extreme care.","I don't see a problem with this if you are just checking whether you are seeing interesting trends in pilot data to decide whether to invest considerable time and effort in collecting additional data.","I see this happening a lot with authors using model selection based on information theory (AIC, BIC, etc...) to justify this decision. While there are valid reasons for adopting an information-theoretic approach to model selection, this is not one.","Issues with the methods are important context for the results, and should not be viewed as invalidating the study. They should only change the context within which we interpret the data.","Isn't that just inventing data?","love to see the results"
"94","I run lots of experiments that do not find anything, or are inconclusive. I can't explain them, and they are pretty boring. ","I don't really see the purpose of not reporting such covariates? If they are deemed to affect the response variable in the first place.","I don't think it is necessarily a black and white situation ...","I don't understand this","I had never considered this ""method"" before. Of course it should not be used as it is mathematically incorrect however you view it.","If few points strongly change the significiance of the test, the relationship is really weak ","I don't see the problem in seeking for more support to your initial ideas. It will be only abductive support anyway. To really test your explanatory hypothesis you need evidence coming for others sources.  ","I sure hope people don't do that unless they have clear evidence that their first statistical method was flawed.","It's a relevant information and can impact the way the results can be discussed.","It's bad practice, for sure. ","Many of the practices you report are well spread among the scientific community (at least as ecologists are concerned). However, my feeling is that in most occasions there is not an unethical conduct, but most often there is just lack of knowledge and ignorance about the proper use of statistical methods. This extends to scientific journals, that most of the time have not adequate reviewers to evaluate the robustness of the statistical methods used. My opinion is that scientific journal should have some kind of statistical reviewers to see whether the methods used are sound or they have serious limitations that might question the validity of the conclusions."
"95","I sometimes focus more on significant results and decide not to report some other results for the purpose of making a more clear conclusion. But, I don't feel it's the best practice. Other ambiguous results should also be reported if there's enough space and given attention.","I don't see any good reason to not report all statistical results if used in writing the paper. ","I don't think it matters as long as the data were analyzed in a robust and unbiased manner and the results are presented accurately. I think that the paper should present the facts in the way that makes them easiest for the reader to understand and place in the context of the broader literature. The practical aspects of the results were uncovered is, to me, irrelevant.","I don't understand whether you are asking about the practice of *reporting* the testing of a 'best' model when multiple models were tested, or the just the testing multiple models. My answer assumes the former.","I have no problem with p=0.051 or the like being interpreted with caution. I expect the same for p=0.049","If I am concerned about apparent outliers I present results for both analyses that include and exclude them.  Readers can then come to their own conclusions concerning the information provide by such observations.","I don't see why this practice couldn't be used. Not finding statistical significance is just indicating that you are not able to reject the null hypothesis given your data, and that might have to do with having insufficient number of observations. In that case, it might be justified to increase the sampling effort after running some preliminary analyses where you did not find any significant result.","I think a lot of people realize that their data and the initially chosen model don't work, once the data are collected - variance is too high or they forgot to think about some covariate at play. In an experimental study, where the researcher controls design of the data, I think it is less common but not uncommon.","It's common to run into problems with ecological data, which are generally quite messy. Not disclosing problems may result in a lack of consensus when other researchers test the same hypothesis, and also reinforces the notion that ecological studies should be completely free of issues. Problems that are explained and justified belong in the manuscript. ","It's deceiving to do so. You should always report it","Many questions were a bit too vague for me to have a clear opinion. Depending on how I interpreted the question, I could see cases where I thought the practice would be appropriate, and those where it would not. "
"96","I suspect most researchers would readily publish non-significant results if it wasn't so difficult to do so (particularly in terms of what journals are willing to accept) and counter to the culture of high-impact publications.","I don't think this should be used. If a covariate was included in a model, then it should be reported whether its statistically significant or not.","I don't think it shouldn't be used... I think this practice is more to do with the way the science tends to be funded. Those cool ideas or patterns that emerge often weren't predicted form the start... but they turn out to be more interesting than the original plan of research!","I find the question not very clearly phrased. Do you refer to testing for potential explanatory variables that later are not reported on. Do you refer to something like transformations where one looks at diagnostics of different models and then reports the one model with the best diganostic plots. Do you refer to something like the order of explantory variables?","I haven't used p-values for the last 7-8 years. I don't think p-values are of much help, and even more when they are close to the significance threshold (either alpha = 0.05 or 0.01). Anyway rounding-off a p-value is not an honest practice. Fortunately I have never seen a researcher doing so.","If it is mentioned in the methods and if there is a good biological reason to exclude those points (e.g. measurement error), I think that this practice is ok. It just needs to be transparent.","I don't typically do studies looking only for significant results, but this would not be a good way to conduct science.  On the other hand doing preliminary analyses to assess power or determine appropriate sample sizes is credible.","I think during the data exploration phase people probably try out multiple statistical analyses to see which best models their data. For example, maybe they start with generalized linear models (and maybe p-values are not ""significant"") but then realize that their data do not meet assumptions of these models so then move on to a different analysis, not necessarily because their p-values weren't significant but to find a more appropriate analysis for their data.","It's dishonest","It's dishonest.","Many statistical issues depend on context. Observational studies have different scopes of inference than experiments. Nonparametric or Bayesian approaches have different issues with significance."
"97","I think each individual study needs to decide if this is appropriate or not. If the study was designed specifically to test the significance of particular variables, then those should be reported (significant or not). However, if the study tested the significance of other variables as a precaution (i.e., just to make sure that there were no other confounding variables), and found them to be non-significant, then I don't think it is an issue not to report those. Or, perhaps, to put them in the methods section as ""Variables X, Y and Z were monitored and found not to significantly affect response variable T"".","I don't think you should just pull covariates out of your analysis because they do not significant effects because I think it is important to show readers that you have considered these variables (supposedly you had a reason to believe they may have had an effect) and they were indeed tested. When it comes to meta analyses and comparing between studies, I think it is valuable for scietists to be able to see which factors had an influence in which systems (e.g. amplitude of traffic noise may affects signals of some species of birds but not others).","i dont really have an opinion on it","I guess in principle this shouldn't be done, but in reality you often do some simple exploratory analyses or alternative types of models, just to get a feel for the data. It's not usually very interesting to report all these various approaches, especially if they tend to give the same qualitative result.","I personnaly used it to homogeneise the presentation of the results. But I think that being more precise is important. /  / It is also important that recherchers do not be obsessed by values of p, but more on teh biology behind","If ONE point affects an entire data set either way, then it has way more leverage on the results than it ought to.  /  / It should always be reported as occurirng and is easy to do so in a plot","I estimate exactly 83% for this one! No really, I think the above slider is a little bit strange because if you want to talk about artificial accuracy, it lies in the idea that anyone could differentiate between 82 and 83% of researchers. I might suggest a metric such as ""None, Some, Many, All"". That's about the level that I think most researchers could actually estimate at. /  / Anyway, not a good practice to perform, but sometime researchers on limited budgets may have access to some data but need to decide whether to invest additional funding/work into a project. Again, not an ideal way to do science but occasionally life intervenes. Ideally, research projects should be self-contained and experimental designs should be conceive of with a total sample size in mind, but of all not-so-great-science practices, this one doesn't bother me as much as some of the others. ","I think it can be used as long as both results are reported; the readers can decide for themselves how to use that information.","It's dishonest.","It's dishonest.","Most of my percentages were really plucked out of the air -- I have no real certainty about any of the values, so I doubt these data (from anyone, not just me) can tell you much. For example, if you end up reporting ""50% of behavioral ecologists think that researchers at other institutions are guilt of scientific misconduct"", then I'd reply ""so what, I don't care what they THINK, I want to see verified data on actual levels of misconduct""."
"98","I think it's appropriate when an entire experiment (or an entire set of analyses looking for some pattern in observational data) turns up nothing -- the whole enterprise is then not worth reporting. It's not appropriate when it is used to cover up ""data dredging"" -- doing a whole bunch of unreported analyses that didn't achieve the usual standards for significance, and only reporting the few that did. ","I find most people report insignificant covariates. I don't think this is a pervasive problem.","I feel strongly that we shouldn't have to pretend unexpected results were predicted. However, this is often problematic for presentation in a manuscript. I once tried to present a manuscript without a clear prediction (noting that the literature didn't support one) and an editor made me re-write it around predictions that I told him/her I didn't really have!","I guess it all depends on the approach you use whther frequentist or not. It's sure that if you use AIC or AIC weight... you need to report the complete set of models you used. ","I see no problem with this practice as long as the rules of rounding are followed (i.e. 0.054 is rounded to 0.05, but 0.056 is rounded to 0.6), and the actual value of statistical test is reported. ","If there are strong grounds for thinking that extraneous factors are involved, then yes.","I feel that trying to confirm a previous significant results by doing further sampling is a valid way to increase confidence in this specific result. On the other hand keeping on to continue collect data when p is not quite significant and stopping when it is not such a great idea.","I think it is another way to explore the data.","It's dishonest. I believe it is standard to report problems with the methology, so I don't think people make this mistake very often","It's pretty easy to explain interpolation and simulation - can't imagine why you wouldn't. Results and interpretation could be compromised, and given importance of n in statistical analysis, results are quantitatively compromised by the practice.","Most of my research involves large, long-term, observational data sets, rather than controlled experiments designed to test a specific hypothesis.  I think that might cause me to be more open to exploratory analyses and trying out various models that are never reported (because the data are complex and messy, with many potential covariates due to uncontrolled factors), compared to someone who works strictly with data from controlled experiments."
"99","i think it is difficult to publish non-significant results. but in my opinion, these studies or results are equally important, if not more important.","I find the options provided worded as ""It should..."" very restrictive. I think it is common for people to have many covariates that are not reported in the end because they were simply not useful to begin with, but exploring a covariate A not necessarily useful to explain C might be sensible any way as it could e.g. lead to a better undestanding about what might cause a given response in C from variable B","I find the opposite to be common practice in order to make some predictable results appear groundbreaking","I guess it depends on the dataset and the type of question. For experiments set up with clear a priori hypotheses, it should not be done. But when working with large datasets collected by multiple groups, data exploration is often necessary to understand data structure and identify the proper statistical test.  ","I see no reason not to show the exact p value. ","If there is an outlier affecting the relationship, then this could be occasionally removed, but this has to be mentioned clearly in the manuscript. Otherwise, we should not remove any data points. ","I guess I've head of this being an issue in the psychology community, but I can see that ecologists might use it as well. To be honest, I usually have had only limited funds. Even if I wanted to collect more data, I haven't had the $ to analyze them.","I think it is okay to try lots of different statistical approaches","it's hard though, because as a researcher in a complex study, there are so many areas where you can have doubt... eg did the equipment actually work as it was supposed to? did we make any mistakes in data entry? some of these may be just doubts, but sometimes you know that the equipment isn't always 100% accurate but you still have to work with it, and at some point you have to trust your data.","It's wrong to do because you will tend to not find potencial real effects (well, depending how you do it you could go the other way, but that would be much more critical and essentially fraud) I used it in my first ever published paper. I am embarassed to admit it. I had 2400 simulation sets, and after resubmitting a paper a reviewer asked a question about a summary statistic that required recomputing a mean over those 2400 simulations, or some such. I had deleted inadvertently 100 simulations. I just simulated the 100 observations I needed from the distribution I knew the data had (ie. I had a mean and an sd right, bot the actual values). This was a minor point in the entire paper, so I know the impact on the paper was negligible, and of no practical consequence, but it was still wrong.","Most of the issues you ask about are side-effects of significance testing. They don't really apply to researchers who focus on estimation and likelihood-based approaches. "
"100","I think it is important to report results that show ""no effect"". In some cases, however, it might not make sense to report these if the investigators believe that there is simply insufficient power to test for differences - and therefore ""no significant difference"" is not so biologically meaningful. Mostly though I think there is just not much space in manuscripts and so authors are encouraged to focus on ""interesting"" findings.","I have failed to report non-significant covariates when I didn't trust the original covariate data (i.e., I suspected it wasn't significant because there were too many gaps or it wasn't collected well)","I focus a lot of my writing on building a narrative, so if I get an unexpected finding, I still typically build the story around that result. I don't say that I predicted it from the start, but it often becomes the focus on the narrative","I have been taught that data exploration and checking residuals is really important. I would be very reluctant to have confidence in my results if I spent 5 minutes doing a model once and then wrote up the results.  /  / I understand the theoretical concern of multiple testing. ","I think it's close to data manipulation","If these data points are outliers and their removal from the analysis can be justified on these grounds, this practice is acceptable.","I have chosen ""it should only be used rarely"", but what I actually think is ""it can be used if necessary"", although I have never done it simply because I don't feel like repeating the same experiments. ","I think most of the time people change their mind on stats to use once the R² or p-value is obtained and does not suit intuition. The thing is, they also probably change their mind because the initial statistical approach was ill-suited (like using ANOVA with asymptotic F tests on non-Gaussian data). This is not as crucial a problem as the other questions so far - if an effect does not exist, it should resist all statistical procedures applied to it.","It's important to let other people evaluate the conclusions of your analysis","It disrupts research integrity.","NO"
"101","I think it is often hard to get null results published unfortunately.","I have had this discussion on numerous occasions as well as published on it. ","I have mixed feelings on this. I understand the need of predicting before running the analyses, running the experiment, but I think important finding come from exploratory analyses as well. I also have difficult to understand what ""from the start"" mean. It cannot be ""before obtaining dataset"" as it would not work for long term and large scale dataset (e.g. from stakeholders) that allowed to find important patterns. ""Before running teh analyses"" would not work as well because it would almost always be true.","I have done it in the past to save space and make the manuscript cleaner when all answers are qualitatively the same, but one model is simple to explain - in these cases I saved all results, ready to share if needed. More recently I use electronic supplements to provide these alternative models for reviewers and readers. I think it's common to test multiple models when people are working with large public data sets, and uncommon when people are working with experimental data in which they controlled the design of experiments.","I think it's fine.","If this specific point can be clearly argued as beeing an outlier, I think it's fair to do so. And/or if the final result is not affected.","I have done it but I checked ""out of curiosity"". Collecting more data was planned anyway. Should be avoided as the adequate sample size should be chose a priori. I think it is ok to do it if one has no prior of the expected size of the tested effect but then it should be mentioned in the publication.","I think that exploring multiple analysis types is a natural part of data analysis. The question is - how much faith do you have in the one kind of analysis you chose? Why did you choose that one? Is it really the best way to analyze your data? If you expect to see an effect, but don't - is that because of your analysis? I think that's a worthwhile question to explore.","It's very important that researchers are honest with each other about the reliability of their results, so that everyone can draw their own conclusions based on the facts.","it is a manipulation of data and can change the conclusions","Nope, great questions. A little long but useful"
"102","I think it is sometimes inevitable when exploring complex datasets ; it is frequent to investigate the influence of some variables that are not crucial to the main question at the exploratory stage, and to omit it from the final manuscript eventually. I believe banishing such a practice would be very difficult. Given the level of energy required to publish, it would be difficult to publish all the student projects that have not been fully completed, for ex. ","I have never knowingly done this, and I think it is wrong. /  / However, I think it is possible to do this by accident, because of the need to be brief in writing up results, which may be complex and cover lots of variables. /  / It would not always be detected at review either.","I have never presented a finding as if it alone were predicted, but I have presented an unexpected finding as if it were one of several possible predictions, which I think is OK if done carefully as it can help set the context of the study better","I have done this in the past due to 'jumping' into my analysis, and then realising later on (not-related to whether results are significant) that I can use a better proxy for some of my predictors/response variables, and so redid the analysis. I have not done this simply because I didn't get the result I wanted though! This is the hall-mark of data-fishing, and I think a huge number of researchers do actually do it without realising it is a terrible practice.","I think it is not important","If you have a good reason to exclude the data point it's OK even if it does improve significance. But cherry picking and removing data for no reason other than that it removes your significance is unethical.","I have no problem with this.","I think the simplest statistical approach that does not grossly violate model assumptions should always be used. In my mind, using more complicated statistics to clarify a marginally significant trend will almost never result in better p-values unless your parameter estimates and model inferences are biased by broken model assumptions. ","It disrupts research integrity.","It is cheating.","Ok as it is is missing in a question"
"103","I think it should be used because in ecology (and field work) there are too many reasons why a result is not significant.  Flooding the literature with insignificant results would drown out the significant findings. /  / Our field is not like medicine where, e.g. a drug not having a significant effect is a significant observation.  /  / The only time it seems worthwhile to report an insignificant result is when it's important for hypothesis testing.  ","I have no idea on the frequency with which this practice occurs ","I have no answer for (a), but I do think it is more interesting for a hypothesis to be refuted than supported.","I have not been in this position before in my research. I am not entirely sure what would be the reasoning for not presenting certain models, but I feel that if tested they should be presented or at least report that they were inconclusive (if that is the case)","I think it is OK for secondary results; maybe it is also not a big problem for main results. One should keep in mind that these thresholds are not of special value. For me, the results with p=0.049 and p=0.051 are of virtually  the same confidence, while the results with p=0.048 and p=0.02 are of different confidence","If you have an obvious outlier, you collected data on a day when it shouldn't have been collected, I can understand leaving a data point out. BUT you report that in the methods section and explain why you did it.","I have on the other hand, done a preliminary analysis for a prepublication talk, and then finished collecting the data I had always intended on collecting. Several times, this has resulted in me publishing different results than I had initially presented at a conference.","I think this is dishonest. /  / The only exception is if this is admitted upfront and a justification of the method switch is provided. Then the process is transparent and the reviewers can decide if it is justified.","It doesn't have to be explained why this is bad. The motivations are clear as to why authors do not to want to hand a noose to their reviewers, or otherwise this wouldn't happen.","It is dishonest. ","Perhaps the main issue is not the use of statistical tests and significance levels, but rather that people don't feel free to use alternative methods. This is an education problem. / Hobbs, N. T., & Hilborn, R. (2006). Alternatives to statistical hypothesis testing in ecology: a guide to self teaching. Ecological Applications, 16(1), 5-19. / Also, this survey is not completely relevant to some areas of research within ecology where missing data are inherently present in datasets (e.g., CMR data), or areas of research where prediction is more important than explanation (e.g., data mining). /  / https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/ / "
"104","I think often when you're not sure how to analyse your data you end up doing many many different tests. Reporting them all is actually quite silly. Of course you shouldn't just report significant things, but reporting tests you did that turned out not to say anything at all about your questions, in the end, is not useful.","I stated ""never"" above. Sometimes I may not describe the full results of a model in the Results section. However, I believe I have always stated the independent variables used in the Methods section and/or published the model output as supplementary materials. /  / Why is this important: The non rejection of a null hypothesis can be as important as rejection. For meta-analyses or Bayesian approaches, the non-significant results of previous studies can be included. The problem of collinearity.","I have no idea how to gage the number of people that do this.  When I publish an ""unexpected finding from an exploratory analysis"" that was not in the original model set, I clearly identify this as a post-hoc model.","I have reported a set of models while indicating that other candidates were not reliable. I think it is important that ecologists and scientists in general should be honest with themeselves first, and if a model is a candidate but will not explain anything in its own opinion/experience, they are free to decide not to report it.","I think it should not be used for the 0.05 threshold.","If your question concerns 'massaging' the data, i.e. exclusion of some data points to acheive statistical significance, then this is just a falsification of the results. However, exclusion of statistical outliers is a routine practice; it should be preformed when justified, and it should be reported in publications.","I have used it when current results are marginally significant and reviewers have asked for larger sample size.","I think this is legitimate, so long as all of the approaches are documented in the write-up. For example, one might conduct an analysis which shows non significance. This analysis would be based on a set of assumptions that the test is itself based on. One might then test these assumptions. If the assumptions have not been met, it would be legitimate to try another kind of test. I have often shown the results of both parametric and non parametric tests of the same data, for example, to examine how robust the findings are to the kind of test that is used. I think this is a useful approach. But again there needs to be complete transparency. ","It is dishonest and it could hurt other scientists that rely on your results. And it could make you look bad if other people know that the analysis is not appropriate.","It is dishonest. Again you just have to indicate it in the paper and it isn't an issue","Please consulting an statistician on the questions included in this survey. Quite few of them are very ambiguous. "
"105","I think that all possilities should be tested and included in a publication.","I tend to do experiments, so wouldn't analyse the data with a covariate except as a post hoc bit of data dredging (and if something showed up, I'd do a new experiment to test it).","I personally dislike this approach (re-framing hypotheses down the track based on initial findings) but believe it is very common and have definitely been specifically recommended to do this by supervisors / senior coauthors. I think it can be done in ways that are less and more detrimental... it can be useful in wrapping up a 'neat story' but definitely biases against negative results and risks inappropriate study design for the final hypothesis. ","I most certainly think that this should not be used simply to try and find a model that has significant p-values (i.e., p-value fishing). However, I personally have used it on occasion when several different approaches are valid but one might be more sensitive to low sample sizes or more prone to low statistical power when using multiple predictor variables. I do, however, on occasion (if word limits of journals permit) also report those initially 'failed' tests plus the final test I then ended up using. Most of the time, though, this is only reported during the review process but simply cannot be added due to restrictive word limits.","I think its misleading. As scientists we should be able to interpret p values for the information they give. Adhering SO STRICTLY to a cutoff is not useful or meaningful.","In an ideal world this would never be done. /  / However, if the process is transparent, the points excluded are flagged up, and justification provided (e.g. an independent assessment that the data is less reliable), then the reviewers could decide if the decision was justified. /  / It would need to be stated that significance was conditional on the decision to exclude.","I often conduct meta-studies of published data, or collaborations for which data is collected externally. If the dataset is pre-defined, continuing to collect is not an option.","I think this is marginally acceptable IF YOU REPORT THAT YOU CHANGED THE ANALYSIS BECAUSE THE FIRST DIDN'T TURN UP ANYTHING. If not, it's fraud.","It is dishonest.","It is essentially falsifying data.  If there are cases where it is justifiable to fill in missing data, it should be report in the study so that reviewers and readers can assess whether it is acceptable in that specific case.","Please email the paper, once is published, to all the people you asked to answer this survey. Tks and good luck!"
"106","I think that the practice biases the results of published work by omission and perhaps encourages researchers to follow the same dead ends over and over because they don't have the benefit of the body of evidence that indicates that something is not related. The issue is finding places that you can publish non-significant results. I don't aim to exclude those studies, but getting them published is a challenge and they're unlikely to be highly cited, so they can become low priority or simply get rejected.","I think all of the results should be available even if it means that non significant variables or covariates are reported only in an appendix or supplement.","I really don't know how often this happens. Sometimes an author may need to cast a paper a certain way to make sense, so I don't really have a problem with it. ","I suppose my opinion on this depends on the type of data being tested... sometimes I test models with a great deal of parameters in the exploratory data phase, without thinking they may be biologically meaningful (just to check for any unexpected surprises prior to moving on).  I think it is not a good practice if you fit a model that you expect to be a good fit, then find it to be a poor fit, and continue on to test others.  The best bet is to be very clear in the description of methods.  ","I think p-values in general are a poor way to evaluate results in ecology. Things are so messy in the natural world. Although I think it's important to stick to a priori methods, being so rigid in this p-value business is probably not good for gaining a better understanding of the natural world. That's probably why so many of us have switched to using information theory.","In cases where the significance of a particular effect is doubtful given the global trend, or the data cloud, or the number of data points, and is due to the presence of a single point in the data set, then yes you should potentially remove that point and stand that the significance observed is not strong enough","I rarely use p-values. Anyway, p-values are highly dependent on the sample size, and sometimes hypotheses are rejected because sample size is insufficient, or supported because sample size is huge and even small differences become significant. So in my opinion it really depends on how honest are researchers (and to what extent they understand what they are doing), because the practice in itself might not wrong I believe.","I think this is ok for data exploration .","It is fraud. ","it is fraud","Please let me know the results of your study. Thanks!"
"107","I think there are two questions again: reporting studies and reporting all single tests in one study. Not reporting whole study that fails to find (any) statistical significance is problematic and affects, for example, meta-analysis later (the famous file drawer problem). But not reporting all the tests within a report or article is not that problematic if stated that ""only significant tests are reported"" and if also mentioned what variables were tested. In fact, often editors and reviewers ask for this, to reduce the space needed for an article. Nowadays, electronic supplements offer a better possibilities to report all tests.","I think it's standard practice to not display covariates that did not reach statistical significance.  I would normally only show them if my original hypothesis was that the covariates were significant (to show the negative results).","I see this as a common temptation among graduate students. However, there is nothing wrong with predicting one thing, but finding another. So, there is no reason to say something was predicted from the beginning if it was not.","I tend to favor reporting results from all models fitted to data.  However, as with the previous question, there is often some EDA that happens somewhere along the way.  With all the possible model permutations, the possible number of models to run on a given data set it often extremely large.  On occasion, I have started fitting models where one or more functional forms lead to obvious lack-of-fit (e.g., Poisson vs. Negative binomial).  I've used this process to restrict functional forms, often reporting this in the text (e.g. initially, we tried fitting models with Poisson error structure, but...).  But then when I report results of a model selection exercise, I will often include ALL specific predictive covariates being examined -  as these tend to be of the most interest scientifically.   So, I think this practice is okay if used to identify one or more useful model structures as long as inference for all tested covariates is reported.  In an ideal world the process used to identify model structure would also be made clear even if specific results are not reported.","I think p-values should not be used as absolute markers of significance as there really is very little actual difference between p =0.054 and p=0.05.  So I guess I am ambivalent as to whether or not this should be done, but am against using p-values as the sole arbitrator of 'significance' in general. ","In my field, collation of training sets is important. And if you are interested in investigating a desired gradient, increasing number of sites may change significance of specific data.","I really don't know how many people might do this. Scientific design books suggest doing a pilot study to see if things may work, so... I personally just do my study with the data/ sites I have and do the analysis after the data is collected. I have never been in the position to have enough funding to go get more data.","I think this is quite a field-specific question. In cases where the entire study design and analysis appraoch is planned form the start, it should be avoided. However in some subfields (e.g. ecological genomics) there may not be an obvious 'best practice' analytic approach and it may be appropriate to investigate different analysis types. Obviously this should be done whether or not the analysis reaches a significance threshold and (I believe) I tend to investigate several options whether the initial analysis looks 'significant' or not. ","It is important for researchers to know this information, as they will try to build on published results.","It is not ethical.","Please note that some of my answers are due to the proliferation of fake science/predatory publishers. I believe this issue to be of extreme importance and is undermining the quality of research. Too many researchers are becoming editors, reviewers, and authors for bad journals such as those published by OMICS. I do not know of a good way to control this problem with the publish-or-perish model of professional advancement."
"108","I think there is a degree of common sense to be applied here. I think a quick analysis of a speculative nature that finds nothing suggests dropping the subject and moving on to other more productive research areas - in such a case preparing the work for publication is likely to take much longer than the analysis. A more detailed investment in analyses, where the write up as a relative proportion of time invested in the analysis means writing up is far more important","I think it can be used if also explained carefully the reason for excluding the covariate. In field conditions when collecting empirical data from non-experimental design, it may be occasionally impossible to get valid measurements of a covariate even if this was originally aimed at. Excluding covariate is not staistically ideal in such cases but may still be ecologically justified.","I shouldn't be used, but editors and reviewers often want a consistent message throughout the paper. If the intro and discussion cover two completely different fields due to a complete pivot in direction of the results then this isn't possible.  However, I would always try to ensure that the story is consistent throughout the project.","I think all models should be reported. However, this could be interpreted in various ways. Sometimes people do exploratory analyses before settling on the final set of models. I think exploring the data is okay (we encourage students to make figures as a first step) and part of the scientific process, so I am not sure how I feel about this. ","I think prespecified p values are fairly unimportant in general.","In my work normaly one point is a individual so i get the variability. Because in nature is normal big variability. Once again you should have a proper n and think in the biological meaning of things. Because sometimes could be an error the outlier.. ","I think as long as you perform the appropriate multiple comparison significance adjustments then this is ok. There are guidelines around ethical 'peeking' at data that should basically be adhered to and it should also be reported in the study if you did it. ","I think this practice can sometimes be justifiable.  For example, a researcher may re-analyze a dataset using a method with greater statistical power that they had not considered initially.  Of course, in many cases this is a dangerous practice, particularly if the researcher is essentially just using an alternative analysis approach to obtain the desired significance.  ","It is important to disclose weaknesses. This example differs from the preceding ones, aporoaching falsification of data.","it is okay to fill in data you don't have (e.g., say all terrestrial vertebrates have one heart even if you haven't checked...) if it is almost certain that it is correct, but you MUST acknowledge that otherwise you lie, and make your science unrepeatable and un-falsifiable.","Questions about n maybe / "
"109","I think this is justified in occasions, because often a study does not show a significant result and hence it is deemed worthless to be reported. In my opinion, ideally we would report ALL experiments irrespective of the outcome, because 'negative' results are also useful, both to understand how a system works but also to prevent other researchers. But in practice, most of us know that it is not possible to report such negative studies as a standalone contribution, nor in scientific journals nor in other venues. / However, I believe negative results should be reported if they belong to a larger study that have also yielded other findings, as part of the research methodologies. ","I think it depends on the question being asked in the manuscript, sometimes it is impractical to report values for all non-significant results, however it could be included in a supplementary table. If the result was of direct relevance to the research question and the hypothesis then I think it should be reported in the main findings of the study (i.e. not supplementary).","I suppose that as long as scientists are human beings this cannot be fully avoided, although it should. / The difference of ad hoc, post hoc, a priori reasoning is not clear-cut. Especially experienced scientists can move easily around these, and it can be impossible to know from a published study how it went - because we cannot write that ""we studied this but actually found that""","I think everyone probably does this - try several different analyses - but I don't think researchers claim that the analyses reported are the only ones they have tried. / It is different if you are referring to a set of models within a specific test.","I think rounding-off p-values should only be used when rounding to thousandths or further, never to hundredths or tenths. That is because it would incur in rounding-off exactly towards the arbitrary threshold, as in the example sentence (reporting p = 0.054 as p = 0.05), which could be interpreted as a significant effect due to a technical issue. Under the p-value statistical approach, such threshold should be determined a priori and strictly respected when reporting results.","In some case outsiders can reduce statistical power while they are artefacts. ","I think collecting more data after checking the results is not a problem, as long as the data collection is then performed blind so that unintentional bias cannot influence the collection.  ","I think you should use the most appropriate statistical test and don't ""test shop""","It is misleading.","It is serious scientific misconduct to report results that were not observed","questions seem to come quite heavily from a P value centric view. I think good/bad practices in stats  - to a degree - depend on how heavily you lean on P values as a tool, e.g. what is wrong for a frequentist hypothesis test not always wrong for a Bayesian model trying to make best predictions"
"110","I think this is part of the publishing problem — there isn't enough space to talk about everything that you did, particularly what proved not to be an important effect","I think it depends on the situation. by and large I think if you collected data on something and it was part of your hypothesis then you should report the results, whether they are significant or not.","I tend to think it's a more interesting story to highlight the unexpected nature of a result than to pretend it was expected all along.  Explanations for unexpected results can also be fairly speculative, so it may be hard to pass it off as the logical outcome of a study.  Regardless, this misrepresents the scientific process that led to the result in question.","I think if a researcher realized that a model simply didn't make sense ecologically, it's probably more important to remove that model from the set. However, they could say they did so in the methods, thus allowing the reviewers and readers to decide for themselves whether that was ok.","I think rounding is ok because the interpretation of the results don't dramatically change.","In some cases, you can remove data points if you doubt that they are real, it might be misreporting... But this has nothing to do with these points changing the message you want. Or then you might present in your analysis the result with the full data and say that if you remove this given point it changes the message so that the robustness of the result might not be very high.","I think it is more a matter of sampling effort / time. Thus, it is useful to conduct a pilot study if resources (funding, time) are scarce.","I would need to know the context. But in principle, it seems prudent to explore your data using a variety of statistical tests.","It is scientific misconduct to mask problems potentially impacting the conclusions","It shouldn't be used, but unless it represents a large percentage of the data, it is unlikely to be very important either way.","Quite a few of the statements you proposed in this survey where at least somewhat ambiguous and/or included more than one aspect to evaluate, which made simple ordinal ranking very difficult."
"111","I think usually this practice is used to support verbal arguments, either assumptions or general conclusions. In the cases where the main goal is to say that ""since X can happen, then..."", negative results are not evidence that X does not happen, and I think it is ok to use this practice. However, this practice becomes worse when researchers claim about the generality or incidence of the argument. For example, this practice should never be used in reviews or meta-analyses, because the main goal of the study is to compile the overall evidence in support (or not) of a biological phenomenon.","I think we learn best as a scientific community if all results, whether statistically significant or not, are reported. Also, often reviewers or readers will want to know why certain ""obvious"" covariates were not tested.","I think honesty is the best policy. It is fine to report the unexpected result and there is no reason one has to pretend it was anticipated from the start. However, I also do not subscribe to the hardline experimental design thinking that the only valuable results are those you meant to collect in the first place. ","I think it can be used if explained in the Methods what and why this was done. In the exploratory type of data collection and analysis this type of approach is very hard to avoid completely. In a purely experimental, manipulative context strict model approach is easier to follow. It could be used if and so that procedures are openly and honestly explained.","I think rounding only becomes a problem when people round off selectively in a way that makes their results look 'more significant' - presenting to p to 2 decimal places is not bad in and of itself, since the precise value of p is not that interesting (because the p=0.05 threshold is arbitrary). I think there should be a flat rule, like always present p to 2 sig figs or 3 decimal places etc. ","In some particular cases, some outliers are clearly incorrect and altering the result. After a careful analysis and justification, I think that some outliers could be removed.","I think it is normal to investigate other samples or a broader set of samples to discover if a trend holds.","ideally should not be used.  but at same time there's often no absolute right answer re the correct stats test, so it's natural (and fine) to explore several and see how robust results are (do signif results wink in and out?  are models stable?).  in my experience no method (that's legit) gives all the right answers anyway, so some results become more signif and some less.","It is unethical / I have reviewed more than 100 papers and I try to dig in to see whether there are issues. They often result from ignorance, more rarely, if never, I have seen unambiquous misconduct.","it suggests a higher confidence in the data/study than it deserves.","Scientist are sick for the ""need"" quantity (not quality) of publications. And scientist are worried in publishing, not doing science. Therefore, bad practices are common. /   / Concretely in my sub-discipline, many people do ecology but, sincerelly, they don't have a deep knowledge in ecology, nor in statistics: normally people just collect a lot of data and afterwards they try to find a beautiful story. Moreover, they choose statistic test as they choose a t-shirt in a shop..... /  / "
"112","I will not provide substantial detail on something that is not significant - I'll say what was tested, and if it was not significant, then that will be reported. If this challenges previous findings, then it will be explored in more detail.","I think with modern data analysis techniques we are moving past statistical covariates. More sophisticated modelling techniques such as the Akike's information criterion mean we are able to deal with complex systems (and relationships) more easily. ","I think it's dishonest and conveys a false sense of how science works. I now avoid this practice, but I may have done it in the past (I don't recall a specific instance, but it probably occurred).","I think it can hide results, just as in the first question","I think rounding UP is not a problem. However, I would never advocate rounding down as in the examples given. It is misleading.","In very rare occasions there are data points that may be the result of different factors or different populations and could be removed but taking this problem as common practice, may lead to pass any statistical test according to our desires","I think more data are always a good thing.  If folks want to increase their statistical power I think it's better to let the ""Universe"" tell us the answer than it is to massage data with a variety of statistical tests. ","Ideally, one would know the best statistical methods to apply to one's data before starting analysis. However, I have found that is not always the case. Either I learn of new approaches, or my data look different than I expected, or I learned something new about the system in collecting the data. In any case, if preliminary analyses are not getting at the most important questions and returning non-significant results, I see no problem with exploring the data further with alternate approaches.","It is unethical and not scientifically justified","It would be so dishonest that I don't think anyone would think of it.","See previous comments about the sliders; good luck with the research! Maybe it takes you in some new and exciting direction?!"
"113","I would know the non significant difference ","I understand this as not reporting all covariates that were included in a given model? similar conceptually to the previous questions, but easier to control.","I think it's honest, and more interesting, to report unexpected findings","I think it is a byproduct of the work. Often you may not realise your model is quite right and have to go back and revisit it again. Often, it is helpful to use multiple analyses in order to examine if the final analysis is robust, and this would often be reported in the paper, without specific results from these models. Of course, this approach shouldn't be used in an unethical way to dredge data for significant results using different types of models. ","I think that the p-value only provides a small subset of information about interpreting a statistic. If the person is relying solely on the p-value to indicate the significance or lack thereof then they probably are poorly representing their data set. In instances in which the paper is being rounded to two decimal places to me it makes sense to keep rounding the same, even if the p-value is 3 or more digits. It's not as accurate, but basing your decisions about a phenomenon's accuracy based solely on the p-value is also poor statistical practice. Reporting effect size, for example, will often bring attention to the fact that your p-value of p = 0.054 might be questionable in its significance. ","Inflates type-I error rates","I think running analyses at different stages in a series of experiments is OK. I don't support editing experiments to meet arbitrary statistical thresholds, but if people are just looking at the numbers as they come in I don't have a problem with it.","Ideally, people should use a few different tests to make sure that the result is real, and report the results of all of them. Reporting the results of only the test that gives a significant results risks reporting a result that is not real.","It should never be used in an ideal world, but in a publish or perish era, with assays costing more each day, it is difficult to disclose everything that can potentially prevent your paper from publication. Along with negative data, this is another beast that can quickly drain a researcher's funds.","Just because it's lying. ","Seems like a very interesting study, I would be curious to see the results!"
"114","I would phrase the question differently - when I do this is because some potentially confounding variables are not significant, or when some idea I explore in very early stages of a work turns out not to be supported. Usually once the I am well into a project I report all non-significant variables, but before hand sometimes not (which on the one hand adds to the file drawer problem, but doing it will lessen the chances of a work being published - referees and editors will say it lacks focus etc.)","I usually prefer to report a 'full model', a 'final model' and all effects even if they are not significant. As such, we can guide new analysis while delineating new models and experiments and prevent people from loosing time doing the same set of procedures.","I think it's somewhat common to report interesting results that were not explicitly predicted a priori, but are clearly within the realm of what could be predicted under the conceptual framework of the study. But if the unexpected result is outside of what could be predicted under the conceptual framework of the study, then this practice should not be used.","I think it is normal to analyze data in a variety of ways to find the best analytical fit for a particular dataset.","I think that this practice should not be used in cases where the rounding directly affects the pre-determined alpha value (typically 0.05), but otherwise think that it is a reasonable simplification of calculated p-values.  ","It's a cheat, unless you have a legitimate reason to exclude a data point (e.g., you have evidence that it was mis-measured).","I think that's fine. A first pilot dataset may be collected to determine effect sizes and low sample size may lead to non-significance. However, this may warrant further and deeper studies with larger sample sizes.","if a test was chosen there is a reason for it ... then why change it","It should never be used, but I can't see this being stamped out anytime soon.  High impact journals demand concise, punchy narratives that are not blurred by inconvenient details.  Given these powerful pressures to remain tight-lipped about problems with the study, scientists can no longer be trusted to be open and honest, and thus they must be *forced* to provide all the important details using such approaches as pre-registration, public archiving of RAW data, and visual evidence of the experiments.","Just state that you do it and it's fine. The readers can decide.","Several questions are a bit hard to answer because they could make sense in different contexts in which the answers would be different"
"115","I would rather prefer the ""It could be used often"". I don't see any problem in having an analysis were I have collected covariate A and B to explain say C and D, respectively. Then, just because they are there, I use A to explain D or B to explain C. Not surprisingly, they are not useful. I don't feel the need to report that. ","I would report the ones tested in the methods but again if there are page limits, NS data is always the victim","I think it depends on the question etc; sometimes could lead to lack of objectivity ","I think it is often justified not to report all models tested, since there are methodologies such as model selection which can generate hundreds or thousands of different models to assess which one is the best fitting. It would not be necessary to report all models as such, but the algorithm used to design the different models, so that the procedure can be reproduced by others.","I think that we should report the exact P value, even if we unable to explain the results.","It's a post-analysis bias. Data exclusion needs to happen prior to hypothesis testing, and have robust  justification.","I think that if you are collecting more data to conduct an impartial test of your ideas can not be considered a problem as more data provide better support to your conclusions, no matter which are these.  ","If a thing is above the threshold with one and below with another method, you should be critical towards this value. A practice of 'finding the right methods to obtain a p-value below the threshold' lessens the value of a statistically ""significant"" finding","It should never be used, but when you disclose all issues, your paper may never get published... So much for transparency in science.","Just wrong.","Some of the practices are due to reviewers' comments, who ask to simplify model presentation / included statistical variables / predictions, for publications"
"116","I would say that this is not really a problem in the hands of researchers. It more relates to the expectations of what ""novel"" research should entail (""interesting"" results, interactions etc.). Thus, it is in the hands of research society in general.","Ideally of course it shouldn't. Again survivorship bias leading to big dangers for e.g. meta-analysis. But hard to be as strict about it. If you're going to use NHST then nonsignificant results may just not be publishable for one thing. Also things can just be a bit woolly. You can try a test as a exercise to see if it works on preliminary data. Maybe you'll follow up and do more work on it, maybe you won't get round to it. You're more likely to get round to it if initial results are interesting.","I think it is a misleading way of conducting hypothesis-driven research. Statistical methods should be adapted to the explorative nature of analyses, or to the fact that a specific hypothesis is being tested. ","I think so, specially if you are testing multiple alternative hypothesis.","I think the p-value should be reported with more than one significant figure. What I have done is called a p=0.054 significant, but also reported it as p=0.054 so the reader has all the information.","It's awful to think people do that, but I have personally witness more than a few cases in one particular context: graduate student annual committee meeting: a student is disappointed that his/her results does not yield statistically significant results. The supervisor convincingly suggests that some particular data are not strong enough. The paper ends up reporting significant results. It's sad to say, but this is usually with 'big names authors'.","I think that this can used at times when the first, pilot sampling shows a promising trend, but appears to have low statistical power.","If an analysis does not seem to show a significant difference, then it is not unreasonable to try another analysis type. So long as the analysis is validly applied, you can adjust to meet the needs of your data.","it should never been used - but may be enforced by referees! you may go on discussing potential pitfalls, but there needs to be a line. No data are likely perfect (I work in behavioral ecology).","Like my last answers, I think that by reporting that a data was included after back-regressions, for example, does not hurt the paper. However, hiding readers is un-ethical, because readers should be aware about this practice (how these data were estimated, how many times etc) to judge whether the conclusions are solid or not.","Some of the questions are ambiguous."
"117","Ideally one would report results for all studies and tests one carries out so that the information is available to the scientific community. In practice, this is impossible- space limitations and the need to prepare coherent, readable manuscripts....particularly one wants them to be accepted for publication...means that some results (including statistically significant ones, incidentally) will go unreported .","If a covariate is added to a model, then there is a 'biological' reason for doing so. Thus, it should be reported.","I think it is fine to note/comment on/highlight unexpected findings as long as you caveat them as not relating to a priori predictions. They may be very interesting but need corroborating.","I think testing alternative models is ok, in that case reporting should include all models, but again sometimes it is journals who point to reductions.","I think the question is probably moot. 0.5 or 0.1 is simply a statistical convention, the actual values don`t really mean anything other than the power that we have invested in them. The issue is rather that of norms of practice (which is why, personally, I have never rounded down) - I think the majority of ecologists understand this. Moreover, as new methods (such as bayesian statistics and model averaging) become more common the epistemological significance of the p value will diminish.","It's called an outlier analysis. If you have outliers, and you have a justified reason for excluding them, then exclude them. But it does have to be justified. It goes both ways too, if the outliers make your results significant, but they are outliers, you check to see what happens when you remove them.","I think that this practice is usually done in order to ensure that resources (usually time, but also consumables etc.) are used as best as possible - it is not an ideal way of working but piloting an experiment like this can be a useful method and I believe it is commonly done.","If another test proves more appropriate, I have no problem changing the statistical test - this is sometimes part of the review process, but the motivation should be on the appropriateness of the statistical analyses, not the results of the analyses.","It should only be used rarely because methodological approaches are like the lens of a camera, if it is dirty, you get dirty photos too. Pitifully this may be a common practice as Journals want to read ""perfect"" designs, and editors are not open to understand problems in the field. I think tropical biologists we suffer a lot of that every time we submit a new MS. ","Make up data: make up science.","Some of the questions are badly phrased ... e.g., several of the questions that ask whether something SHOULD not be done.  Whether the practice is allowable or not can depend on the context (e.g., recognizing after performing test A that test B is actually more appropriate to the circumstances). "
"118","Ideally, everything should be reported, including exploratory analyses. But this is practically impossible. If an exploratory analyses was not significant, it is reasonable to not to report it, and invest the time / energy / space in (perhaps) more central analyses.","If a covariate is of interest, statistically significant or, the fact that it was tested should be reported","I think it is more important to create a steady story line for an article (e.g. story line being interesting, novel, question provoking) and this may be something else than originally indicated. This is acceptable as long as the study design is flexible enough to address this question as well. /  / I think it is also rather compulsory to seek the data whether also other theories / factors define the results parallel to the factor presented in the analyses. This brings credibility to the results, but complicates the sampling procedure as especially in ecological studies, it can be hard to predict covariants in advance and therefore there may be need to record many variables.","I think that as space in journals is reduced (tighter word limits), people omit loads of stuff. I think should go in supplementary materials. /  / Otherwise - just 'fishing' for the result you want.","I think this practice is completely unacceptable.","it's cheating.","i think the question is too vague......some of my research is beogrographical and we keep building knowledge and dataset i.e. we work on permanent plots every year there is a little more data for these and our questions evolve a little in line with the possibilities","if assumptions for the first test are not met.","It will not be convincing.","making up data is a bad idea, backs you into a corner preventing finding a truth.","some of the questions are stupid. ecology is a super noisy field. there is so much variation across antural field sites, seasons, etc. it is unrealistic to expect that people can a prior decide which is the best model. it takes thought and analysis to gain insights into the results. sometime that can mean new data collections to flesh out a new hypothesis to explain results. sometimes in course of analysis, other mechanisms become more clear. To not treat data analysis and collections as an iterative process would be a travesty. "
"119","Idem as before: it is a bias towards positive results, unless there is severe design problems which explain the lack of results.  / But journals do require positive results...","If a covariate is stated in the reported model, it's significance needs to be stated.  ","I think it should be used. Finding new and interesting results that describe the world around us is what we all strive to do as scientists. Unfortunately the publication system that we have currently produces papers that follow a certain formula and if you do not follow this formula then you have less of a chance of being published.  / ","I think that this practice should be used very sparingly. However, sometimes a researcher will initially explore a wide pool of models, some of which may provide terrible fits to the data, may not converge properly, or may have been set up incorrectly by the researcher. I am fine with such models not being included in the set of models that are reported.","I think this should not be used, because really we do not care at all: what we should care of is the exact p-value and associated statistical parameters (F, Chi, df), and intensity and direction of effects if any, and not at all about pre-established threshold which are intrinsically meaningless.","It's dishonest unless there's a justifiably good reason to exclude the data point(s) (such as it being an obvious outlier). This should typically be done before any statistical tests.","I think there are two situations in which this is valid: 1) If there is an effect that is approaching statistical significance but one lacks the statistical power to properly determine if its significant or not and 2) One is analyzing data from a pilot study to decide if the hypothesis warrants a full study.","If different tests provide different probabilities of the null hypothesis, I think is not wrong to carry out more than one test and understand the reason of the observed differences. If a Mann-Whitney test is not statistically significant, but a one-way ANOVA is, if the assumption of the parametric test are satisfied, I think it is correct to present results of ANOVA.","It would not be justified to leave out potential problems that might impact key conclusions. This would be poor science. Yes there could be problems - and they might influence your results - this should not necessarily stop the researcher from publishing, but the problems need to be properly acknowledged and ideally, their potential influence on the conclusion properly stated.","Making up data is fraud.","Some of your questions could be better focused, in that when interpreted literally I could make caveats that do allow certain practices, while I also knew the bad statistical practices that you were aiming for. /  / Also, asking questions that parse as do you not, not do something (double negative==positive) might be confusing for ESL readers."
"120","If ""not reporting"" means ""not publishing"", then, yes, I think it is quite common, especially because it is more difficult to report negative findings.","If a covariate was analyzed this implies that the researcher believed there to be some biological reason why the covariate would or would not be related to the focal variable. Thus the results of the test should usually be reported. However sometimes the test is performed as a preliminary analysis or the focus of the study shifts, making the  test less relevant and distracting to report.  ","I think it should not be used, but the current format of publications, where one need to ""sell a story"" many times makes it necessary to reformulate the original aims of the study.","I think the question is weirdly asked. It largely depends on whether the ""unshown"" models confirm or not the results of the shown models. I frequently run far more models than can be shown in a paper, to test for results' robustness using different datasets or approaches, or to consider alternative hypotheses. In that case, it is more for the sake of clarity and concision that all models are not shown in the paper. If unshown models are voluntarily hidden because they yield different results, it is totally different!","I usually present my results as an asterisk or bolding to indicate significant results, and state in the text that p &lt; xxx. One does not usually see results presented as p = xxx. Of course, if I have a table where I am presenting full p values, they are usually rounded, but to more than two decimal places.","It's just dishonest. The last defence against doing something stupid or dishonest with statistics is transparency. All of these practices depend on hiding some part of the methods.","I think there is only a justification when serious animal welfare issues are apparent - in this case I think a small pilot study should be designed to estimate the effect size to enable a proper power analysis to be calculated, potentially using such information to decide whether to continue the study or not. But this should be part of the plan from the start.","If failure to reach statistical significance is the only reason to do this then this is completely unjustified. If, however, you think of justifiable (on a priori bioligical grounds) factors that you failed to incorporate the first time then this can be justifiable.","just dodgy!","making up data, and people need to know so that they can properly interpret the results/analysis","Some prompts did not allow me to 'vote my conscience'. ""Should"" is  a term that I am less likely to use than ""does it"""
"121","If a researcher thinks a study was not good, e.g. did not have appropriate power or replication, I can see why they would decide not to include its citation in a discussion - especially if they are young and not ready to criticize a colleague's work. And again, in another scenario, if you are doing a large complex analysis with existing data, you may explore some patterns that are not significant and decide not to take a paper in that direction - but I'm not sure if that scenario is the sort of thing you mean.","If a variable is included in a model, its result should be reported or the variable needs to be left out (and justified why left out). The covariate affects the model's power and so always has an *effect*.","I think it should only be used when we felt an effect should be tested but did not have a very clear prediction on why it is effective. / We are pressured to present our research in a hypothesis testing framework, which is good because it forces us to think of mechanisms, but could also stifle creativity because sometimes you find interesting patterns by data exploration. If the latter happens we should try and think of mechanisms after finding the pattern, and it is easier to present, and get something published if we ""linearize"" the process and write it as a prior hypothesis even if it wasn't (writing ""I then decided to do some pattern seeking"" will not get you published, and such very useful acts are often termed in a derogatory way ""fishing expeditions""). It will even be easier for the audience to read, and the price in ""honesty"" is not really that high - the person reporting it did the thinking, whether he did it before or after getting the results is not that crucial","I think there's a gray area between basic data exploration (I'd think this is generally considered acceptable) and testing full-fledged models that go un-reported in a manuscript.  Possibly some basic models may be run to formulate a final candidate set.","I would hope no one ever does this.","It's not common to have to exclude outliers, but if they exist, it is of interest whether excluding them affects the conclusion.","I think this by default this happens, since in many fields people present preliminary data as posters and talks at conferences. I almost always end up adding more data between a conference submission and a published paper.","If multiple approaches are all valid, then why not explore multiple approaches? Reporting that this was done is important though.","Just report everything and let the readers decide.","making up data... is not science","Sometimes the question was not fully clear to me, which I mentioned in the comment box and sometimes the possible answers did not suit my answers. I therefore used the answers more as a rating system rather than fully agreeing with the answer."
"122","If a tudy is done appropriately, insignificant results are also informative.","If covariates are used, they need to be significant. ","I think its probably more of a detriment to the author who uses this practice. The most interesting and high impact papers come from interpreting and understanding an unexpected result.","I think there are legitimate reasons to do this, for example if honest mistakes are made (e.g., accidentally leaving out an important term from the model), or if a model has trouble converging, or if new data is acquired that enables a model different from a preliminary test. But it creates a temptation to try many different models until a desirable or ""significant"" effect is obtained- which would be inappropriate usage","I would say it is customary to use a 'less than' sign to indicate meeting a p-value threshold.  I don't have an issue with rounding p-values to some degree since such minor differences are not that meaningful.","It's OK to excluse data, only when doing so is explicitly reported in the publication.","I think this comes down to replication issues Ecology is often logisitically and temporally challenging, and lower levels of replication are often used for logistical and cost reasons. This is not ideal, but often is the only way of doing particular experiments at all. If a pattern emerges with a small sample size, then there may be an argument to collect more data to see whether this is a true pattern or not, in the same way as collecting preliminary data can be useful in deciding the direction of a new experiment. However, this same practice should then also be applied to experiments where no pattern emerges with a small sample size. ","If one is sure about the right type of test to use in the very beginning, then changing test does not make sense, but if there is uncertainty, then, changing the type of test is okay - its all part of the data analysis process.","known problems should always be discussed, however with the increasing presure for space, I can understand that only the most important ones are discussed and other minor problems are only mentionned or not discussed","Making up data?  Really?","Sorry about my lack of responses. I did not feel I could quantify my answers."
"123","If a variable is part of your a priori hypothesis, then results should be reported regardless of whether they were significant.","If I understand correctly you are asking if a covariate is added to a model but fails to explain any of the variance in the dependent variable should it be reported or not. The answer would be if it is retained in the model or removed through objective criteria then report that.","I think people do this more for writing a concise story. ","I think this is a very important issue in ecology - probably floating in the background of all/most good publications (and were possibly filtered by the peer review process). I am not so sure if one ALWAYS has to report ""a set of statistical models"". I think reporting one model may also be sufficient - after other candidate models/statistical approaches were considered during the analyses; choosing the final product to report depends on the former (and of course, nature of the data and the question(s) asked).","I would say it should be used at the discretion of the scientist. Generally, a p-value this marginal would likely become significant with a slight increase in sample size, and I think we place too much value in p-values to begin with. ","It's unethical. ","I think this is a fine practice because if your sample size is too small maybe you don't have the power to detect differences, so before you say there is no difference you want to be sure that the lack of difference was not just due to small sample sizes.","If statistical tests are used to look for support to your initial explanatory hypothesis, they should ideally use other kinds of data different from those used to generate your initial hypothesis.  Your explanatory ideas might be correct even if your tests are marginally significant, which is probably the case when they critically depend on the kind of statistical test at work.  ","Let it all hang out. ","Making up data??","Statistical practice has changed over the years. Education is the best way forward to consistent practice. In reality funding is to limited to repeat studies. Effects which are consistent across studies come to be accepted because studies are repeated "
"124","If by ""reporting"" it is meant pear-reviewed publication, then I would say that usually P &lt; 0.05 is often a requirement of publication, except in the case of a very compelling hypothesis with high statistical power of the test or grifted writer as author.  ","If I understand the practice correctly, an example would be to fit a number of covariates (test a number of null hypotheses), then selectively report the ""successful"" ones. The significance of the successful results would be exaggerated because the probability of a given covariate reaching P &lt; alpha under the global null hypothesis would not be alpha, as it should be, but would be &gt; alpha. In simpler terms, dishonestly hiding some tested hypotheses tricks the reader into overconfidence in the others.","I think that to publish, particularly in higher impact journals, the ""story"" of the research question is sometimes changed. I also think that exploratory statistics can help shape the ""story"" that is finally presented for publication. Although there is currently a shift away from formal hypothesis testing, research questions are still (usually) presented in the final paragraph of the introduction of papers. ","I think this practice should be used when alternative models are being examined in pilot trial analysis only, until the best set of candidate models are identified.","If a threshold of &lt;=0.05 was agreed upon, than everything above 0.05 is not significant. The threshold is of course arbitrary and could be adapted (which happens if rounding-off is applied). In most cases, I believe, no great harm is done if rounding-off is applied. However, it is just wrong, if the threshold is not adjusted.","It a point fails a formal outlier test, and/or there's reason to think measurement was compromised, than I might exclude if it's also influential I guess. Sequence of checks important here.","I think this is acceptable in some cases (e.g. long-term experiments where an effect may only become apparent after a number of years - it's okay to perform analysis while still collecting further data as planned). Also when samples are very rare, expensive to process or hard to find it may be acceptable if transparently reported. In other cases it may be difficult to perform a power analysis at the start. However in such cases it is much more appropriate to treat the initial work as a pilot and then perform a new experiment of the appropriate size. ","If the collection of data and the experimental design was right, then the data analysis should be enough to obtain adequate results.","Listing of the problems of the data / method may obscure important messages. Therefore too long lists of what could have affected the results are not needed, especially if they are listed in other articles and the method is already widely used. (Like all in vitro experiments...)","Making up results is very close to scientific fraud and cannot be tolerated. ","Thank you for doing this interesting and valuable study! I'd be keen to see the final results!"
"125","If it is an important variable rather than just a variable that was thrown into the analysis at the last minute then it would be disingenuous not to report it. ","If it's in the statistical model you present in the paper, you need to report on it. ","I think there's a place for it only if your predictions weren't well informed - if you revisit the theory and figure out that in fact you were wrong, you might consider that your result is more or less surprising than you previously thought.","I think this practice should be used with caution. I find it easy to experiment with analyzing a data set with different models in an exploratory fashion before solidifying exactly how I want to approach the research. Unfortunately, this can be pretty messy stuff to publish. I think that most journals reward very clean and organized approaches to research. Although I recognize that this is not a transparent process, it seems necessary at some level.","If a value is known to exceed a pre-specified threshold, then it should be treated as such. ","it can be a fine line as to how to decide what it a real outlier and not; using some exploratory analysis may help, although I think it should only be used here for when a point has already been identified as an outlier, the test might show that it doesn't matter if it is still included in the original data, and in that case it may be best to use the full data set, this simplifies the analysis and strengthens it (in that it shows that the trend or lack thereof was not affected by the presence of the outlier).","I think this is justified if it is an open question as to whether enough data has been collected to draw any kind of inference.","If the driving reason to try another analysis is the lack of statistical significance when you expected significance....this practices is a bad idea that could easily result in inflated type-I error rates.","Many datasets/methods have basically an infinite number of problems with them. It's most important to highlight the main issues and be specific about your methods - then most other issues can be correctly inferred from this.","Manipulation of data is inexcusable. ","Thank you for participating. Before you leave, do you have any comments, questions, or suggestion..."
"126","If something gets left out of an analysis it needs to be described.  I think the full analysis prior to removing anything should also be reported in an appendix or something.  Studies that don't find anything are critical data.  I know it's hard to publish these, but that is because reviewers and editors are crazy.  ","If it's relevant to your study, it should be reported.","I think these questions are getting silly.  This should be obvious.  It misrepresents the truth, and fails to conform to the requirements of hypothesis testing.  Such discoveries are valid and valuable - but ONLY in an appropriate way.","I typically use burnham-Anderson method or reducing final Model from full model using forward backward steps so always starts with full model and report final as just one of tested. / Therefore by nature of my methods used I report in all models tested. I think this is relatively common","If anything, 0.054 is more significant than 0.050 the 95% level but more legible in the abbreviated form. ","It can be done if there is a biological or ecological explanation for the outlier that has a large impact on the coefficient","I think this is justified when an interesting potential finding is observed but sample sizes are small, e.g. because it is very expensive to collect data or the study species is rare or hard to observe. Also, sometimes animals need to be handled to collect data, and there needs to be proper justification to do so - especially in case of endangered species. If a potentially interesting pattern emerges but requires a larger sample size to be confirmed (or rejected!), this is justified.","If the originally chosen method was correct and its assumptions was satisfied, choosing other model inflates the type I error rate","Many ecological data have known problems. For example, counting things is always almost subject to a degree of errors. Such errors are implicit and understood by others in the field and there is no need to provide an exhaustive list. When the problems are not implicit, then they should be and almost always are reported.","Many ecologists might impute missing data while saying that they do so, but not identifying exactly which data points are imputed.  I don't have a big problem with this.","Thanks for working on this line of research."
"127","If something was tested, it must be reported.","if non-significant covariates are retained in the final model for any reason, they should be reported","I think this is a common practice of new students, especially when the direction of the relationships was opposite of what was initially expected. It should be clear that it's OK and even interesting to get results that contradict your initial thoughts - it given you the opportunity to propose new hypotheses, that can be tested by future studies, that may explain the contradictory findings. ","I understood the question as whether people sometimes fit models which reults are not reported in the end. I would say that this is an extremely common scenario, because quite often if doing a rigourous job you will want to fit models which you know are not necessarily sensible but you want to check they are not. So I see model fitting as a kind of extended exploratory analysis, and from that perspective, don't feel like we need to report everything we do. The question might be seen differently if you are not reporting the fullset of models because some models present results which would be hard to explain, etc.","If decision criteria are to be objective, they should be applied strictly.  If one knew the truth they wouldn't bother with statistics.","It can be used only if clearly indicated, and only if some data points have been identified as statistical outliers (due to e.g. measurement error)","I think this probably happens quite a bit, and is often the result of not doing proper study design / power analysis before a study is initiated.  I tend to do this sort of thing with simulation studies, where e.g. I expect a bias of 0% but end up getting a bias of 2%.  Most likely, it's Monte Carlo error, so the easiest thing is just to run more simulations.  Then again, I'm rarely looking at p-values.  The worry there is that you could be biasing things by only be collecting extra data for certain ranges of p-values.","If the practice is used to coax positive results out of analysis where their existence is iffy, it is harmful. If researchers use various techniques to see which gives the most insight, good. ","Maybe I do not understand the question, but if you are the only one knowing the problems, of course you must report them to help the reader interpret your results. / If the problems are some unwanted statistical properties described in the literature, it might be okay to not mention them again, unless there is a reason to think they are particularly acute in your case.","Maybe this is more common in social sciences (surveys, such as this one e.g.). I introduced some mean values to a couple of missing points in a survey study once and did not report it. Maybe I should have done it, but it was more that I forgot. When the mean is introduced however, it won't affect the coefficients (as far as I know). / Have not encountered it in ecological data.","The choice between 'once' and 'occasionally' is a bit limiting, as 'occasionally' can be interpreted as twice or 10 times."
"128","If tests don't reseach the threshold of statistical significance, they should be reported, but if they aren't germane to the main research topic, I don't see the harm in ignoring them. It would be tedious to read about all the nonsignificant results that had no impact on the main argument of a paper. That being said, if the test in any way is relevant to the message of the paper, nonsignificant findings should be clearly reported without obfuscation.","if not significant they do not have to be in the final model; however it should be stated that they were tested.","I think this is almost taught as part of traditional science training. There is such an emphasis on a need to state our hypotheses at the outset, but much of research is exploratory, or findings are incidental. Many researchers therefore think it's more important to appear as though the hypothesis was proposed from the outset rather than admit being surprised. In principal I don't think there's anything wrong with this. Inappropriate use of statistics is the second half of this problem. Passing off an ex ante test as an a priori one is misleading and dangerous, but if statistics aren't involved there is no problem with being clear that the research was exploratory, in my opinion. I also think you need a 'no opinion' option, but I'm assuming that leaving it blank implies this.","I use R which is open source and has many independent packages. I like to check to see if the results are similar. They often are and I present a set because it is common practice in journals. I would present all models if it were common practice and space in manuscripts.","If one sets a threshold, then one cannot round down. That is, p=0.05 is not p&lt;0.05.  If one only reports p-values to three significant figures, one should report p=0.0101 as p=0.010.  If the threshold for significance is set at p&lt;0.01, than p=0.010 is not less than the threshold.","It can get ride of the biases.","I truthfully don't know if people can do that, but this is impossible in retrospective long-term studies or studies using historical data. Perhaps this happens in experimental studies, if money and time are available. / Increasing sample size can indeed lead to ""significant results"" with some statistical tests (e.g., t statistics depend on sample size); the problem is that there is no theory to adjust the alpha level to sample size... /  / ","If there is a compelling reason to pick a particular analysis because it is the most appropriate, it should be used. If this means that a more traditional test, which is non-significant, is put aside, I suppose I accept that.","Maybe statistics is difficult and confusing and even sometimes subjective for many scientist (sad, but is the reality),  and therefore I can understand (partially) that some bad practices occurs. But if there are problems in the method and analysis, or problems with the data quality that potentially impact conclusions, THOSE PROBLEMS MUST ALWAYS BE REPORTED. / ","Misrepresenting simulated data as gathered data is misrepresentation and is unethical. Full stop.","The focus of this survey is about statistical/reporting practices, but I believe that the underlying problems that require more attention are philosophical. "
"129","If the experiment was under-powered or you were testing many variables, then it generally makes sense to highlight the significant results. However, I think it is best to include all statistics for all hte variables and experiments in the paper, even if just in the Supplementary information","If some covariates are not good predictors, can be removed from the model, after comparing the AIC.","I think this is an unethical practice of post-hoc testing that reverses completely the scientific method by gathering data, implementing statistical tests and elaborating post-hoc hypotheses that justify the observed pattern. Unfortunately, my impression is that the literature is full of such reports, probably because unexpected significant results are more appealing to journals. While some os these practices are relatively easy to identify due to the lack of coherence of the explanation that supports the expected hypotheses, it is hard to quantify how many researchers have been successful in deceiving the audience. ","I use this as a insurance that different approaches provide the same outcome. Extension of data inspection. Of course, consistency (and model assumptions detected for residuals etc.) rather than the most appealing outcome should be targeted! ","if something is close to significance it can be stated as trend, let's stay with the facts","It could be used to guide further data interrogation but never simply on a statistical threshold criterion","I understand this behavior, and think it should be used cautiously because there is an increased chance of getting spurious results.","If there is a good reason why a new analysis is more appropriate (ex., controls for phylogenetic relatedness when you weren't before), then I see no issue with switching. If something not being significant in an initial analysis leads you to a more appropriate method of analysis, great. If two types of analysis seem equally appropriate and one gives significance while the other does not, then that actually suggests that one of them is more appropriate, and you need to consult a stats person to work out why they're different and which one you should trust. It is not okay to just go with the significant analysis without a rationale. ","misleading","missing data is just that, missing.","the intention of this survey is so clear that i suspect you will get few responses. can't wait for the article you will write bashing ecologists... "
"130","If the reason for excluding them is simply the p-value, then it shouldn't be done. Of course there might be many other reason during an ongoing research to drop variables/studies.","If something is a co-variate it was used in data analysis - therefore it should be presented in results","I think this practice is more used when research hypothesis were not clearly stated, i.e. were open questions. Journals often require declarative hypothesis so authors may construct an hypothesis on the basis of their resulkts when they had no particular apriori expectation. This approach is not done when results are contradictory with apriori hypothesis. / I think it is not dramatic as results are not changed, but more linked to journal requirements of precising open questions (which is from my point of view not fully relevant).","I used this approach when younger, supervisors were okay with it. I would not use this approach in my own laboratory because we usually should have an hypothesis and associated predictions that dictate the models to use from the start (even before the data is gathered actually) as we work mostly in experimental settings","If the p-values are that close to being non-significant then a few decimal places either way shouldn't make much difference to a reader. Researchers should stick to reporting p-values to a fixed number of decimal places throughout a paper","It could only be justified if the analyses before and after excluding the data point were presented and the process of exclusion clearly explained. If not, it leads to the same general problem I mentioned in response to the previous questions.","I understand this practice as an important guide to decisions about fully embarking on an expensive piece of research or not.","If this is done, then both tests need to be reported, and appropriate interpretations of results made.","Misleading and bad practice ","Missing data is part of ecology. Obviously, making up data is inappropriate and unethical","The questions are sometimes not hard to answer because it very much depends how you interpret the questions. In a strict sense you should not change your experimental design or even the hypothesis during the study. Practical reasons, however, may force you to redisign your study. In a strict sense you should then run a new study with new hypotheses etc."
"131","If the result is not significant, then it's not of real importance to report. Of course, null findings can also be important. But if it doesn't add to the narrative of your paper then it's not essential. ","If the covariate does not explain anything and improve the power of statistical test, why it should be included? Yet, I generally report in the text that it (the covariate) was tried, but excluded from the final tests for the above reason. ","I think this practice is rather a by-product of scientific writing that steming from stats.","I would like to draw a distinction between extensive model selection in complicated analyses that inevitably result in exploratory modeling (exploring different error structures, comparing generalized linear models to generalized additive models, etc) that inevitably get left out of methods, and 'p-hacking'. I think the first is almost entirely unavoidable in complex analyses, and is even a good thing (e.g. I think investigators using linear models should test out cubic predictor terms as part of model validation to make sure their final model fits aren't missing non-linear effects). The second, involving non-hypothesis driven fishing for significant predictor variables, is probably practiced at one point or another by 50+% of researchers, to most of our retrospective shame. But I think it is a separate phenomenon.","If the pre-specified threshold for significance is p &lt; 0.05, then p = 0.054 is above the threshold and the result is not significant.","It depends on the data you have - if you can justify why a data point may be deleted, it is ok. Nevertheless, you should allways report (e.g. in the appendix) that you deleted some data points.","I used it when the preliminary finding WAS significant and I wanted to be able to claim a replication. I'm guessing your question referred to situations when the initial p was NOT significant; in that case of course the practice is invalid.  We have a technique that does that correctly -- ""sequential analysis"" -- that is familiar to everybody in biostatistics/epidemiology.","If used, it should be reported in full.","Misleading, could lead to false conclusions; the readers should know about the level of uncertainty","Missing data may be sometimes unavoidable.","The questions may be expressed to be more easy to understand. For example, giving simple examples."
"132","If the study is otherwise sound in terms of statistics and otherwise, this ""no effect"" is equally important to be reported than the ""significant"" ones / ","If the covariate is not significant due to lack of independence, and can be better explained without it, the covariate should still be reported, but interpretation can be limited","I think this practice should be encouraged because unexpected results are not ""wrong"" results and they often provide novel questions for future studies. ","Ideally you should report all models tested (or at least that preliminary analyses were used to decide on the final model, even if the results of all of the models are not given).  As in one of the previous questions, sometimes it is useful to explore some likely models, just to make sure you aren't overlooking relationships.  But as stated before, this should be somewhat limited to avoid simply fishing for significant results.","If the results are this close to the boundary then the result is already highly doubtful and little more confidence would be gained from rounding of numbers.","It depends which way round it is done. It is bad to remove points that change the significance of results, but can be useful to remove (or at least investigate removing) outliers that do not change the significance of results. Unfortunately this question doesn't separate out those very different scenarios. ","I would classify this as a part of power analysis. Efficient sampling requires conducting power analyses to determine whether the sampling size is sufficient or not. Calculating running means and assessments of variation as the sample size increases is considered good practice. It could be the case that the reason non significance has been detected is that the sample size was insufficient, so in that case it would be legitimate to collect more data. I have also sometimes repeated an entire study to see whether results were repeatable, which essentially amounts to the same thing, and is also considered good practice. ","If you specified a model first that didn't fit the data well, and then improved the fit (IGNORING the resulting change in p-values), it's fine. But you should do all proper model diagnostics beforehand. ","My guess is I have done this unintentionally.  And my guess is lots of other researchers have done it unintentionally, or intentionally given that signs of weakness in papers are a good way to get rejected!","Missing data should be reported as missing data always, and if extra analyses are performed with simulated data on the missing points, this should always mentioned in the text. Otherwise the data providing are not real but simulated, and this is cheating the data and the study.","the scenarios on wich the questions are based could have been explained better for example giving examples."
"133","If the study was designed such that non-significant results are not really informative due to the lack of power or a flawed design, the progress of science is not really limited by this non reporting. /  / In practise it is easier to get manuscript with significant results published, so I assume that non significant studies are delayed at least and sometimes never get to see the readers' eyes. Of course this is a shame, since others may set out to test the same thing, spending most efforts on these possibly unimportant treatments or relations   ","If the covariate makes very little sense (i.e. it is very likely not associated at all with the phenomenon under consideration), it can be omitted. But this will be rare: why measure it in the first place if it is so far removed? If you measure it, report it.","I think this practice undermines the hypothesis testing process. Researchers should be trained to set up testable predictions at the outset of the study.","Ideally, people would avoid this because chasing low AIC or a significant results yields over-fitted models. However, I often find that part way through a study, I learn that a data set has some problem or shortcoming and I need to change strategy, which results in a change in the model set. I don't like that, but I find it hard to avoid sometimes.","If the rounding off is 0.013 to 0.01   it can be ok. But 0.054 it is not significant and is should not be rounded. In these cases we should use three digits","It does not reflect reality. ","I would like to think no one engages in this behavior . . . but only a moments thought brought to mind someone whom I am pretty sure does just this sort of thing. Worse, I think this person *stops* data collection is the magic p-value is achieved. Either practice is, in my view, just this side of criminal, and again we face the issue of folks being hung up on p-values rather than, say, effect sizes or measures of uncertainty.","In a perfect world, we would all be so smart we'd pick the best model for the data/question before running any analyses. In this case it's fraud to switch. But sometimes people choose an inferior model on first attempt, and then try harder once they see it's NS, eventually settling on a better model. So, maybe there is an ethical grey area. The issue is that we have confirmation bias, and we're more likely to notice our model sucks if it gives unexpected answers.","Not acceptable, period.","Mostly dodgy and should not be used. The only exception would be explicit imputation methods (as currently becoming common in population genotyping), where it's clear what is being done.  Even there, it's often unclear what conclusions are drawn from the original data, vs from the imputed bits. In general, I'm not a fan, although I accept that the method has its place.","The survey seemed overly focused on very traditional NHST."
"134","If the test was a part of the original predictions, then it should be reported regardless of the outcome. But sometimes people test other things with their dataset than what they originally intended. If the result is significant, it's potentially interesting, but if it's not significant then it's probably inconclusive (e.g. because that's not what the original experiment was designed for) and I can understand that you might not bother to report it.","if the covariate shouldn't have been in the model in the first place the test probably should have been re-run. Welcome to the messy real world of ecological data analysis where we are never 100% sure of our model and where we often change our mind. If you want certainty you should be a physicist. ","I think unexpected results should be reported. However, it should be clearly acknowledged as unexpected. ","Ideally, you should have a model that you test. Rarely if ever do we even discuss a ""complete"" tested set. ","If the value doesn't reach the mathematical significance it shouldn't be changed. The value shows that it is almost significant... and one should comment on this fact.","it eliminates the chance to report a potential novel finding ","I would need to know the circumstances. Because a larger sample can improve statistical precision (e.g. smaller confidence intervals), then gathering more data can help to elucidate relationships in the data (for example difference in means, statistical significance, etc.). ","In an ideal world one would choose the ideal test first, but scientists are not prescient or that wise.  Sometimes one might show a result to a statistician and they would point out that another test is more powerful (and appropriate), then the test could be re-done and eureka, be significant.  But the problem here is the scientist's original knowledge of statistical tests, not an intent to corrupt their significance interpretation.","Not disclosing known problems in the method and analysis, or problems with the data quality, that potentially impact conclusions misrepresents studies.","Must report how data point was estimated.","The survey seems to assume that null hypothesis testing is the basis for statistical methods, which is strange - especially for evolutionary biology in the 21sy century."
"135","If there is no significant effect, and the study did not have sufficient power to detect a meaningful difference, then the paper would be hard to publish anyway, and there may be little value in publishing it.","If the covariate was in the analysis, it needs to be reported as so. ","I understand that as analyses progress, your research questions may change. But unexpected results shouldn't be presented as expected from the start.","If a model is clearly untenable, it can be considered, but dropped from the set later. Care must be taken to note models that are discarded.","if this is just rounding to the nearest decimal, this is actually the correct way to proceed","It invalidates conclusions, unless mentioned and justified. ","Ideally it would never be used. Pressure from higher ups to see ""how the experiment is going"" is hard to resist though","In general, the analysis should be planned from the start. However, the real world is more complex. Often there are a range of quite disparate methods available, none of which is the single right approach, so we often plan to use multiple approaches. They may differ in sensitivity and assumptions, so comparison of the results can be informative. And if a new method is published while the project is ongoing, we would incorporate it into our analyses. Obviously, the original results should still be reported, and choosing not to is misleading.","Not disclosing problems is an ethical problem and results may be compromised, setting back science.","Never heard of this. ","The whole rationale behind significance testing is inherently flawed. Significance is affected by both effect size and sample size and in consequence with a big enough dataset, one can almost always obtain a significant results, even if this is biologically meaningless. Ecologist have recognised this for over a decade increasingly do not use significance testing, opting instead for Information-Theoretic / Bayesian approaches."
"136","If this information could be valuable, it should be reported. It could be a useful addition or comparison to what is significant. If it is not relevant, I should be omitted. ","if the covariates are included to explore their effect and then you find that they do not contribute to explain residual variance, I think you could avoid to present these statistical results","I would say it is not uncommon that the results of statistical tests obtained orient the introduction of a paper, for example by posing hypotheses that could be answered according to the significant tests. / On the one hand, by writing the results of a study in a hypotheses testing framework while the initial goal was more exploratory without very clear a priori hypotheses, there is a clear risk of picking up false positive results.  / On the other hand, I think many interesting discoveries showed up as unexpected results and as / it is regularly asked by reviewers to present results in a hypotheses testing framework, a way to publish such results is to build a posteriori biological hypotheses.","If I understand the question the way it is worded, I do this frequently. Exploratory statistics rarely if ever gets presented in the final analysis, mostly because it's exploring interactions or using different variables. That said, i'm not sure if I understand the question as few papers ever report a ""complete tested set""","If we are using a threshold, say 0.05, we imply that we take as significant a value equal to or less than 0.05.  0.054 is not equal to or less than 0.05.","It is a modification of the data and thus, in general, should not be used. But sometimes the sample is not representative of the target population. For example, we are measuring human height in the Chinese population and then there appears a Dutch tourist in our sample. It may be reasonable to exclude this tourist from the analyses. I don't think the exclusion / inclusion should be based on the p value, though.","Ideally one collects a small sample size, performs a power analysis, and then designs an experiment with the right power. Often, however, the experiment is designed with an approximate sample size, that fails to have the right power. I think it is okay to increase the sample size a posteriori under these circumnstances.","In many cases more than one statistical analysis should be used. All of them should be reported unless one sees that their are not appropriate.","not good science","No reason to do this, no excuse.","There was a strong focus on p-values on the survey. I guess it is important to teach people what a p-value and that it's not all of it"
"137","If variables are tested in separate models, not reporting them may lead to issues with multiple hypothesis testing; if they are tested in the same (e.g. multiple regression) model, not reporting variables distorts the interpretation of the model. ","If the experimental design aimed at answering a research question or hypothesis involved the use of one or many covariates, they must be reported as any other detail of the experimental procedures. Even if the covariate is not used due to lack of significance,its testing was part of the data analysis, and should be reported as such. Lack of space in a publication is no excuse, since nowadays large amounts of data can be included in supplementary materials.","I´m not sure whether I have done this. To know, requires delving into one´s earlier expectations.","If model selection is a specific part of the analysis, then they all need to be reported as well as the criteria for selection. If some models are tested in a scoping sort of way but not used eventually I think that's fine. They may not be the most appropriate and sometimes help you think about alternative or better ways to explore the data. So again, I struggle with the way the question is worded and have answered it with that second interpretation. ","If you've established that you're rounding to a particular digit and then it's correct to round down (I.e. Rounding 0.012 to 0.01) then it's okay. ","It is acceptable - as a tool for discussion - but ONLY if the result using the entire dataset is also included.","Ideally, a power analysis should be performed to understand how much data needs to be collected.  However, if there is no data to conduct a power analysis, then researchers may want to conduct statistical analyses mid-way through an experiment to evaluate how much more data may be needed.","In my case, I switched to a different analysis when I discovered that I violated the assumptions of the original analysis and there was an alternative test that directly addresses the assumption violations.","not helping science move forward","Nope. Never.","this is a really important survey, thanks for leading it!"
"138","If you are using statistical significance as a criterion, then it makes sense that you might not report non-significant variables.  Whether to include them or not might depend on whether the analysis was exploratory or if you were specifically testing an a priori hypothesis about that variable.","If the implication is that researchers do not report a non-signficiant covariate in order to hide it, I have no idea what the frequency is, nor do I think that I have ever done so myself.  Reseachers may check to see if they need to be concerned about potential covariates that would be expected to affect their results (e.g., temperature, body mass, etc.).  If they find no correlation with potential covariates, they may simply move on.  That probably happens frequently.","Ideally the whole story of the research should be presented but it could be used for the sake of simplicity and clarity in the presentation of the results.","If multiple models sets are tested they should all be presented, otherwise we risk presenting misleading results by trying a bunch of stuff until one turns out to be significant","If you have made the decision to use frequentist statistics, then 0.05 is the cut-off to determine if something is statistically significant or not. It's disingenuous to over-inflate the statistical significant. /  / Significant should also not be interpreted in isolation i.e. should be matched to estimates of effect size. ","It is cheating ","Ideally, one should define a priori what biological effect a study design can identify with the given power. However, I find it fine to analyze a pilot study and then decide based on statistical evidence whether to invest in a full study with higher N.","In my mind, this is talking about how the test was initially chosen. Certainly, if you have chosen the best/most appropriate/powerful test given the idiosyncracies in you data, then you should stick with the results. New tests are always being developed and it can be worth venturing into those if they better suit the distribution or type of data you have.","Not reporting caveats and data problems is mispractice. It is however enforced by the journals, which frequently ask to shorten the manuscripts in the parts where these limitations are highlighted.","Not good I. Most instances but necessary with some time series data I use. Can adopt multiple imputation to assess impacts of uncertainty of modelled data on outcomes of analyses. If non existent then just easier to ignore and report a single analysis. ","This is a very interesting and necessary survey. Looking forward to seeing the results. Thanks."
"139","If you have a lot of variables /  And tou need to search biological meaning","If the results of the analysis you report have these covariates in the analysis then the author must say what else is included in the model, indicating that they were not significant.  Saying nothing about them is misleading.  I do not think that detailed results for them are necessary - depending on the amount of space that you might have in the manuscript - they could be included in a supplementary section.  ","Ideally this practice should not be used. But in reality, journals articles read a lot better with a cut-down version of the full analysis conducted.  And there is a big pressure on space. Both of these factors lead many researchers to tell a slightly altered version of events.","If not reporting all models, the reader cannot judge the significance of the results.","If you mean rounding down (i.e. 0.054 to 0.05), this is wrong. Rounding up is fine and I do it frequently (0.015 becomes &lt;0.05).","It is important to explicitly REPORT the effect of data exclusion/inclusion on the conclusions.","Ideally, you should design an experiment that you think will have enough power to detect a significant effect if its there.","In principal it is possible that the initial statistical test may fail to reject the null hypothesis and generate false negatives. An alternative test, if valid, may be robust enough to reject null hypothesis if indeed it is not true. So using multiple methods to dissect the data is to me a legitimate way of doing evolutionary biology, and science in general. ","Obvious reasons","not good science","This is an interesting survey and the information that you are gathering will be valuable. But there are market-driven incentives to bad practices. This survey is targeting individual researchers (the lowest level in the publication machinery). From my point of view, the reasons for bad practices are not in what we believe or what we want. The reasons are in the rules imposed by the journals, and our need to fulfill expectations to get / keep our jobs. "
"140","In a perfect world...  Most researchers have more data than they can publish. You have to focus on the work that has the greatest potential impact. Sometimes that may be studies that showed no significant results, but that's rare. Journals are getting more open-minded about this sort of work, but it's more about researchers and the time they have to write.","If the study was developed to test all covariates then all results pertaining to those covariates needs to be accurately (and honestly) reported.","If a finding from exploratory analysis, then presumably this was not the subject of the experimental design (and confounding factors may not have been controlled for properly); could also be indicative of p-hacking / dredging the data for any 'significant' results that can be written up as though they were the intended question of the researchers.","if other models are also tested they should be provided ...","If you mean rounding to get significant results, then no, I do not do that. If significance is p=0.05 that ANYTHING over that (p=0.051) is NOT significant. Rounding down to get significance is NOT okay.","It is not acceptable to remove a data point because without it you reach significance. However there are several instances in which it is clear that a datapoint is a measurement error. In this case it is ok to remove it.","if a power analysis has been performed and shows that statistical power of the test was not sufficient given the sample size and expected effect size, then it can be a good idea to collect more data. It would be ideal to do a poer analysis up front but this may not always be logistically feasible (because of unknown effect size range) or logistically preferable (may be able to get away with less samples if significant trend is found with smaller sample number, and this may be important given sometimes enormous difficulties in getting data within ecology).","In principle it is not acceptable. On the other hand it is a hard to avoid: When you reconsider your first rash choise of statistic method it will be very hard to tell if you are influenced by the output.","Obviously this is basic scientific integrity","Not reporting made-up data is malpractice.","This survey brings up many good points -- it requires a lot of integrity and honest to do science, not to mention carefulness. Many of the questions here would be answered in the affirmative for many out of either ignorance, carelessness, or willful neglect of honesty. That needs to be split out of this sort of survey: p-values are a great example. Or being a slave to every point and not realizing that data's generally dirty and needs cleaning, but not manipulation to make a point or confirm a hypothesis."
"141","In addition to the same issues with excluding covariates, all variables tested should be reported regardless of statistical outcomes. Further, not reporting research that failed to meet cutoffs for statistical significance leads to the ""file drawer problem"" when topics are reviewed or explored in meta-analyses.","If the variable is not helpful to the analysis, there may not be a reason to report it, unless there was an a priori hypothesis about it.","If it is an unexpected result, then it is dishonest to say it was predicted from the start. ","If some models performed very poorly (in terms of AIC for predictions, BIC for inference, etc.) and the models weren't biological interesting or made questionable statistical/mathematical assumptions, then I don't immediately see a problem with not reporting such models.","In almost every ecological publication I've seen, researchers report P&lt;specified threshold rather = a value","It is possible that the outliers should be excluded - but they should be excluded prior to doing the statistical test and not 'after the fact'.","If a sequential analysis was specified in the study protocol, this is a valid procedure to follow.","In science, we are looking for patterns that can be explained by theory. There are many patterns that can be observed, considering a dataset. All of them should be tested.","Of course, I say ""never"" about myself, but the term ""known problems"" is ambiguous. I have never engaged in this behavior if I knew of a problem, but I may well have engaged in it when the problem was known to others but not to me. (This sort of thing is likely to be common. An ecologist could, say, build a plausible ecological niche model with MaxEnt using system defaults yet be wholly unaware that MaxEnt has certain problems and assumptions about which the researcher would be ignorant with reading a good deal of specialist literature.)","Not sure I completely understand this question but from the wording it sounds like making up data points from simulations and passing them off as observations. Definitely fraud.","This survey is a very good idea"
"142","In an exploratory study it is easy to make hypotheses that in retrospect are not justifiable and it is not worth reporting the ""non-significance"" of such analyses.","if there is a biological sense in testing this covariate then it's important to report it even if it's not significant so we know first that it has been tested and not forgotten but also in order for the data to be usable for meta-analysis for instance, where most of the time negative results are underestimated","If it is unexpected, it should be treated that way.  In addition to being honest, if we knew more about why results were unexpected we would learn more about natural phenomena!","If the author states that this is the ""complete tested set"", that is what it should be. Everything else seems to me dishonest - especially because the methods allow to detail why specific variables were or were not used in the end.","In ecology our field measurements usually don't have more than two significant digits, so why should our p values?","It is scientific misconduct to keep only the information that supports your hypothesis","If fully disclosed, and results from new theoretically justifiable insights such as a redone power analysis, I'm fine with this. If it is to stop a study by lowering the sample size that seems really wrong.","In some cases this could result from a progression of insights, but one should be very careful here.","On the contrary, problems should be highlighted and discussed.","Obviously, this should be reported. Terrible practice otherwise","this survey is very timely."
"143","In an ideal world it would never be used, but in practice limitations of journal length and time mean that this short cut may be the lesser of two evils. /  / The reality is that lots of negative reports, whilst scientifically complete, would clog up the literature and take up too much time to process/review/assimilate.  /  / In practice value judgements about what is important have to be made. Good ones stand the test of time...","If there is a reason to believe that a covariate is biologically significant, but it is not statistically signficant, the result should still be reported. Failing to report negative results can lead researchers to waste time searching for effects that aren't there.","If papers allowed for complexity this would not occur but the reality is that page limit constraints and the need to make papers read well as a coherent story means this probably happens a lot","If the different tests lead to the same result, I think it's OK to not state all the tests conducted. ","In most cases if all that matters is if p &lt; 0.05, or some other specified value, it's exact value is not that important.","It makes it tempting to remove data points to get the desired result. Ideally removing outliers should be conducted blind to the result. /  / Mind you, I totally did this in my undergraduate project...","If I understand your question correctly (and I may not), you are asking if the original data set was shown to be too small to address a question, and if you think so to gather more data. I have done this many times. Sometimes with more data the answer continued to show insignificance and I then reported this.","In some cases, running a more specialized analysis that better fits the actual error structure of the data (e.g. glm, mixed effects model, etc.) is warranted, but often ecologists (particularly grad students) will not make the effort to learn that analysis until they have failed to get a significant effect with a simpler model where their data aren't actually meeting the assumptions of that model.  I think this is OK at the early career (i.e. student) stages, but once you have learned about more appropriate analyses, you should just start with those and skip this step.","One omits caveats in press releases, but includes them in primary peer-reviewed papers.  Period.","Oh my god this possibility never occurred to me. Yikes.","This was a very interesting survey, though some of the questions were somewhat ambiguous (perhaps intentionally?) I would be interested to see the final results once they're collected."
"144","In an ideal world we should report every test as a protection against p-hacking and a posteriori hypothesis; and also because it would be an incentive to thinking harder before testing ideas. / In practice, I find extremely difficult to write down everything I do (blame on me), and to report lots of non-significant results in length-limited publications (blame on the publication system).","if there is good theory for the covariate than non-significance should be reported. but if there is none, and it is not in itself relevant to the work, why riddle it with unnecessary detail? this is the gist of what I tried to write in the previous question","If something is odd, it is noteworthy.  What makes it science though is if the oddity can be repeated. /  / My experience with ecologists is that they love the odd.  So, if they discover odd they will then celebrate it.","If the study design is complex, this practice is inevitable. I often realize which model is the best for testing my questions only after exploring my data and understanding my results. It is especially true when there are many potential covariates invovled. ","In most cases it does not lead to serious error","It may happen that there are outliers in data set, and one way is to look the effect of leverage. It is a recommended practise. Outlier can be error (removed) or a individual sample not representing the population  / Removal of correct data points is a form of misconduct, and should never be used.","if no power analysis is performed a priori this is a good alternative way to get a feel for statistical power... mostly for explorative analyses","Increases Type I error rate. ","Openness about challenges in the analysis or problems with the data are critical to ensure trust in science, therefore hiding such things is bad practise","Oh my goodness I hope this doesn't happen!","Thought provoking questions.  I found myself in the ""gray areas"" more than I would have thought."
"145","In complex statistical models, one often includes several nuisance variables that might be important (e.g. day in a study or which technician collected data) that end up not being significant. In some instances, I don't think it's necessary to report these results. However, if you tested a hypothesis that wasn't supported, this should be reported.","If you list that you tried them, I don't think it's important to report the stats. Or sometimes you try things and the answer is not that relevant to the paper - so it depends on the question. Better to list that you tried them, though.","If the results are unexpected, this should be clearly stated when reporting them. ","If the study is hypothesis driven (hypothetico-deductive), a single model can be enough. / Otherwise, modelling is testing large number of hypothesis, and the approach is variable selection. / So, there is not single correct answer to this. / ","In my opinion, it is not necessary to stick religiously to the pivotal levels of p-values (0.01, 0.05). We must always give the exact p-values. Your example is somewhat ambiguous in that sense, because in reality the rounding procedure is not wrong (the rounding of 0.054 is 0.05, and some journals require not to give more than 2 digits.","It should never be used except when you know 100% that those data are biased by external factors that are not part of your treatments.","If part of a power analysis.","indicates trying to confirm an expected outcome regardless of the contrary findings","part of the discussion should always be on the limitation of the study, nothing is ever perfect...","Please tell me that no one does that. Isn't that falsifying data? That just seems like blatant lying to me. I'd seriously have problems if I heard of anyone making up data.","To put my comments into context, I'm not a paragon of virtue, just a statistician who doesn't have as much invested in a single analysis as ecology researchers. I think I'm pretty typical of statisticians in being callous (even secretly happy) about P = 0.06. Also, reliability (particularly in relation to powering studies) is one of my interests (https://www.slideshare.net/pcdjohnson/presentations)."
"146","In exploratory studies, one cannot report everything. It makes no sense to report the results of every correlation that can be done","If you test a covariate you must report it. If you go the the trouble to measure it, there is a reason for that, and it's interesting to your question even if it's non-significant.","If the unexpected result can be well justified post hoc it may make a clearer paper to present it as hypothesised. ","If the tested model is far superior than other candidate models it is reasonable if one chooses to report only the results from a superior model.","In science I always recommend expressing p-values with only one digit. The second digit is never an important source of uncertainty compared to all the other sources (i.g. the practises above)","It should never been used because it is a manipulation of data to meet your assumption.  / But I believe researchers mostly do it because they do not follow the data analysis steps: they first do the test, then they look at the data. It can be defended then to exclude a point which has extreme values. But this should have been detected before the tests","If preliminary data suggest that there might be a relationship between two variables that is obscured by a limited sample size in the preliminary data set, a follow-up study could be designed to specifically test this relationship -- if I am understanding the question correctly :) I guess oftentimes going back to the field to collect more data is not possible","it's a matter of balance between bias and precision of different tests","People feel the pressure to publish as 'high' as they can.","Rare because it requires good stats knowledge plus deliberate fraud.","Very good poll and questionnaire."
"147","In fact, I have never missed to report unsignificant results in a single paper.  / But I do not write papers on studies that did not give (or give only few) significant results, because I guess that a lack of significance is not attractive for most journal. / (although sometime, but rarely, I find papers with no results published in very goog journal, which I think is a realy good thing) / ","In a complex study, including every covariate in the model may mask the truly important sources of variation.","If this is occuring, it should be reported - the answers does not fit perfectely to this kind of question....","If you are going to report multiple tested models, you should report them all. Reporting the method by which you arrive at the final model is also OK.","In some cases, the third decimal of the p - value can look like to be ""too exact"" given some other features of the study (small sample size, need to make weak inferences, pilot study)","It should not be done because outliers provide important information. I think young researchers do it out of desperation to publish positive results.","If researchers can collect more data, that is always good. There is no reason to suspect the new data will bias the results in some way. The researchers should however be honest about the second round of data collection.","It's better to try multiple methods (if the methods are widely considered sound) and report all the results, rather than start with one method and switch to another if the answer is not what you want.","Poor form.  And after doing it once, I've not done it since because it is dodgy.  Pressure to publish a particular paper led me to do it.  ","Researchers must always be clear where the data they are using has come from and whether it was collected as raw data or simulated. This should be explicit in the results section","Very interesting statistical questions. Very curious about the results and how you will treat the data and which statistical tests and thresholds you will apply. "
"148","In my opinion the exclusion of variables or experiments that don't reach the statistical significance should be avoided in most of the cases, to show not just the significant results but also the ""negative ones"", showing in this way the complexity of the subject of study. The publication of only significant results also give us a bias point of view of our field.","In a designed experiment, I would always use the covariates that were used to 'block' the units.  This is also true for the stratified survey -- the stratifying covariates should always be present in the model. /  / However, when the goal of the analysis is not to 'test significance of covariates' but rather to provide predictions or an exploratory (hypothesis generating) analysis, then I am much more comfortable with not reporting covariates that were not included in the final model.  Typically I would try though (although other things may 'bump' this).","If you forgot to make hypotheses and predictions before collecting data you should tell that this was just an explorative study and supplement with new independent datasets. It is just much easier to write a paper as if you were prepared.","If you don't spend some time looking at your data and potential covariates that could affect the results based on your knowledge of how the study went, even if you have a well designed experiment, there's no way of knowing if your results are the best description of your data. If you think other things could influence your results, you should check. ","In the cases listed above, it seems to be a case of significant digits.  These preferences are often specified by the journal. I think there's too much emphasis on p-values, in any case, rather than looking and thinking logically about the data and how big of a difference between treatments is a reasonable expectation.","It should only be used if someone finds an outlier, but these outliers should be found in preliminary review of data (frequency distribution, graphing, etc) rather than after the statistical analyses (but it can happen, with a student for example, and then it could be done). The outlier removed must be indicated in the paper","If sample sizes are small and the study is part of a pilot project. But in this case, the p-values are not important, I would collect more data anyway (or change some aspect of the experimental design). ","It can be used, not by changing the statistical procedure, but by adding covariates whose effects has been previously unexpected","Potential problems in Materials and Methods should always be disclosed and/or discussed so other researchers don't fall again on these problems.","Same answer as before: this question may be relevant to specific areas of research. / I am working with missing data, and there are methods specifically designed to handle this.  / Imputation methods in the Bayesian framework are based on simulations (e.g., MCMC methods), and this is perfectly Okay...What would not be OKay is not to specify this in a paper, but again, in my area of research one of the main issue is to handle missing data and nobdy would try to hide the efforts made to design appropriate statistical models to overcome this issue.","Very stimulating"
"149","In my opinion, p-values should be reported as important if between 0.05 and 0.10, perhaps even higher if additional supporting evidence is present. I also think multiple analyses (e.g., ANOVA, Regression, Factor Analysis) should be performed before a definitive conclusion is drawn.","In a multivariate world, it can be a useful exploratory tool to test for hypothesized relationships. Reporting these is out of the scope and interest of most journals.","In addition to being truthful, it is also a lot more interesting to state that the result was unexpected.","if you say this is all you tested than this is what you show. if this is not the complete set saying it is is just a lie","It's biasing the distribution of p-values if you round it depending on the actual result.","it should ONLY be used if you say this is what you have done. One should never delete things that should be in an analysis and not report it. But if you do it and report it you show that the results of an analysis can depend on very few highly influential data points and this is important information","if statistical power of the test is really low, more data are needed and I feel it is correct to get some provided the experimental design is included in the model afterward","It can yield a falsely significant result. On the other hand, when more than one statistical test is appropriate and the initial choice of test is arbitrary, there is a temptation to get value out of a data set and avoid obtaining a falsely negative result by exploring alternative tests.","precludes future discovery","Serious question? Surely people don't do this??","Why so p value focused? What about misuse of aic or bayes?"
"150","In my opinion, reporting or not studies or variables that failed to reach statistical significance depends also if the ""failed"" significance makes sense regarding the topic or objectives of the publications. / Moreover, I think that it is not necessary to report a non-significant variable if is part of a huge multi-variant study (some times in ecology a multivariant dataset can have hundreds of variables). However, it is essential to show the non-significant variable if it forms part of the main hypothesis to be tested. / Yet, I consider that publications with ""negative results"" must exist. Can be equally interesting and moreover will save a lot of time to many researchers. / ","In complex designs there may be many covariates, including some that present themselves a posteriori. Writing a scientific paper requires editorial judgment by both editors and authors. There are times when including every possible covariate in a paper would make the paper effectively unreadable.","In biology, there are often multiple answers to the same question, and we cannot always list all hypotheses at the start. Sometimes when our results show us a significant effect, we can look back and determine why that would be, and thus frame our papers around this answer, assuming that it is biologically meaningful and robust ","If you state that the set of models tested was complete then it should be complete. However, often there are studies undertaken when the variables that may respond to a treatment are poorly understood. You consequently try to measure everything that is possible that is relevant to the hypothesis. People will then hone/rephrase their hypothesis to make a more exacting hypothesis - basically they have learnt about what is relevant to the organism through undertaking the project, so they are more able to place the study in its relevant context (i.e. relevant to the organism's biology). Still, the subset of variable chosen in the final presentation should be exhaustive within the subset chosen- not cherry-picked (e.g. recorded behavioural and physiological variables, but later realised that it was silly to measure the physiological variables)","it's dishonest","It was a sin when I accepted that an ""outlier"" was removed, just because it would make a long, distracting story to explain what we realized went wrong with this specific sample.","If the experimenter believes that there is a biologically significant trend, and that the nonsignificant result is due to a lack of statistical power, I don't see a problem with collecting more data or re-doing the experiment.  However, if further data collection gives a negative result, this should also be reported.","It depends on intent. Sometimes additional forms of analysis get suggested in the course of the study. One should always look at data before beginning formal analysis, and that can result in a change in the intended analysis. ","Probably often done unknowingly.","Shouldn't be used. These must be identified as simulated.","Yes.  Be careful in your summary of this not to cast aspersions on an entire field because of some perceived lack of statistical purity on your part.  It can have political implications, raising doubts in the public about science when in fact the vast majority of science published is done well, rigorously and honestly.  In other words, please be cautious about your interpretation of this."
"151","In reporting other studies, I think it is hugely important to represent both studies that have found an effect and also any that have not. A good way around this is to reference a well constructed meta analysis that does this already for you. In terms of variables, as I mentioned earlier I think in ecology there are so many potential variables involved in some studies (in particular field based ones) that often researchers need to make a decision as to what variables are most important to include. Therefore sometimes some variables will not be quoted in a final manuscript. ","In evolutionary biology researchers include variables that they deem to be interesting, regardless of whether they end up having a significant effect; thus, reporting the finding will be informative no matter what the outcome is.","In general this should not be used it presents a skewed view of how the ideas are linked with the results. However, I can picture situations where initially a result seemed unexpected, but after reading another paper, speaking to colleagues, attending a conference etc. it becomes apparent that the result could genuinely have been predicted from current knowledge. In the latter case, which should be rare if pre-project reading and thinking is done properly, I think it could be reasonable to do. In essence, such statements should reflect what could be reasonably predicted based on the current knowledge of the field, not necessarily from the researchers own knowledge (although obviously there is usually strong overlap there).","If you test many many models reporting them all is crazy. Report the most interesting ones, or the ones that are the most commonly used, etc.","It's dishonest. I just use create a constant rule (e.g. report p-values rounded to 2 significant digits) so I never have to decide.","It´s an inexcusable deception.","If the lack of data determines whether a value is significant or not is factor, it should be remedied prior to publication. But effect sizes should be reported.","It depends. It is reasonable to try the simplest test first and then try more complex and sensitive tests afterwards. ","probably should not be used, but including everything would make data publishable...","Simply fraudulous!","You did not ask the most important questions, and this may be so because you are statisticians. You can do perfect stats with junk data. So how do you control for the quality of what you enter in a model in the first place? And second, data stratification: I see absolutely crazy stats done with highly sophisticated models, simply because the data set was not stratified in a qualified way. Over the past 10 years I can say that 90% of the meta-analysis published in my field is seriously flawed. Not technically, but by the way the data were grouped, permitting the most abundant type of data to dominate the model. Yet the most dominant are not necessary the strongest. In essence pot data will always win over forest data, because the first are many and the last are few. See my comment in Global Change Biology 2017,  DOI: 10.1111/gcb.13700"
"152","In some cases, non-significant results might be less interesting and hence less worth publishing. However, in most cases, I would recommend publishing as this might reduce current publication bias towards significant findings.  ","In general I don't think this is good practice, however working with big datasets the initial process of data exploration may involve reducing the set of candidate predictors, and this may not always be appropriate to report in detail in a final paper.","In my graduate days, I was encouraged to do this. I never do now, and I stress the difference to my students between exploratory and confirmatory analyses, and how each should be reported.","Implied in the description of the practice is that it is done in order to hide unfavorable results from other models. Assuming this, I gave the answer above. /  / However, when I thought about it a second time, I always run way more statistical models on my data than I would ever report, because reporting them all would be superfluous and only make the report confusing without added value. For example, if I am trying to fit a model to a species abundance distribution, I might first try out a hundred different models as an exploratory exercise, but only report the ones that are potentially informative, relevant to my hypothesis, or are not redundant for the sake of clarity. In this case, I would change my answer to a.) ~80%, b.) I almost always do it, and c.) I think it should be used often.","It's irrelevant.  P values are very misleading anyway.","its dishonest","If the sample size is too small (e.g. given large variance) , only way to overcome it is to increase sample size (it should not be question of fishing significant p-values, but ensuring that sufficient power is aquired to reject a hypothesis)","It happens that I try different analytical approaches to a question, and I don't design an analytical plan which I then alway respect - if stuck with one approach, I will change. However, the decision to use one or another approach is never, by any means, driven by the results. Rather, the structure of the model and its fit to the data are the decision criteria.","Problems should be disclosed but not everyone agrees on what constitutes a problem. ","Simulated ""missing data"" point are calculated using the existing data point, thus it's not independent from the rest observation.  They should never be used.","You raised some interesting questions, particularly the dogma of the critical p value.  0.05 is often seen as god by some disciplines but less so by others.  I am in the less so camp and will publish the p value because if it is not less than that, the obvious question is by how much did it miss?  What about some other statistics like ""effect size"" used in social science - there is a place for that too in ecology."
"153","In some observational type studies (e.g. investigating species or genotype distributions) where there are a large number or potentially explanatory variable I think this OK. Probably not good practice when performing manipulative experiments though. ","In general it's best to provide as much information as possible so the reader can evaluate the work, but there are often space constraints that limit the ability to provide all info","In my opinion it really depends case-by-case, sometimes can be used to better sell a research while not harming the scientific content. Unfortunately, this is what researchers are increasingly expected to do nowadays because of the high competition for visibility (impact factors, citations..). / ","In an ideal world, all information about alternative models should be openly accessible but this goal is at odds with the way our current publication system (still) works","It's just not right.   /  / Not least because we should be avoiding null hypothesis testing on statistical grounds in favour of multiple working hypotheses with weight of evidence and maximum likelihood methods. ","Manipulating a dataset for the purpose of reaching significance is really bad practice!","If the sampling strategy or the number of replicates (in experiments) are well planed in advance, I see no reason to collect more data. Again, collecting more samplings only to reach a p value is a way to hide negative results.  / However, if after finishing the sampling the researcher realize that his/her strategy was not correct, and more samples were necessary, I see no problem in collecting more samplings.","It is difficult to distinugish between exploratory data anlysis and what you are implying here. I frequently use partial correlations for deciphering interactions. My first step is always an ANOVA; but that is never the final step nor goal. If you define that as switching, then I am guilty. I think looking at the data and getting a feel for it using simple methods (ANOVA, linear models) before going on to more complex analyses is fine. Your question implies that it may be 'switching' methods, when in fact it isn't in my view.","Problems should be reported. But pressure to publish means that issues are always likely to be glossed over, to some extent.","simulated and real data should always be clearly identified","You seem to be focusing on misconduct, but a much more important and fundamental focus would be on the inapplicability of the standard null-hypothesis-testing framework itself. This framework makes no biological sense even when done correctly. That's a huge problem. Look into that please. Biologists should be estimating meaningful parameters, with confidence intervals, and no testing null hypotheses. Even the tiniest and most biologically insignificant effect can always be shown to be different from zero, with whatever p-value one desires, if sample size is large enough. Null hypothesis testing is a silly game scientists play, a game one can always win if pockets are deep enough to make huge samples."
"154","In studies involving many variables or experiments, selective reporting is almost inevitable in the face of length restrictions and journal policies against publishing non-significant findings. / ","In general, all covariates that were tested should be reported in the methods. Other than that, I think it is not neccessary to dwel on them if they are not significant","In reporting such a result, the authors must be clear that the study was not originally designed for that purpose.","In general, researchers should report all tests. But when space is limited, it would be very convenient (and not deceitful) to report only the results of the most meaningful tests.","It's not honest reporting of the results. Science is about finding the truth so we have to show what the data says, precisely. Anyways, those thresholds are somehow arbitrary so 0.049 or 0.051 don't make any biological sense. I believe we should generally move away from using thresholds.","Manipulation of data is inexcusable. Real scientific advances could begin in questioning why these data points were outliers.","If the study isn't blind, then there can be bias in the study. So no, I wouldn't suggest to do this.","It is difficult to know a priori which type of statistical test will perform well. Simply using p-values to decide which statistical test to use is definitely wrong though. Model validation must accompany any kind of test trials.","Problems that are known trade-offs of methods (e.g. precision vs speed) don't need to be exhaustively detailed in every paper. However, if you know of problems in the data or analysis that you don't disclose, that is dishonest.","Simulated data are not real data.  The investigator should indicate if simulated data are used.","you should have let us say don't know for number of ecologists engaging in this practice"
"155","In the early stages of an investigation, one tries many things, which may or may not be supported. At this stage, p values are not evidential and there is no reason to report them or the details of how one arrives at a hypothesis. When one tries to confirm a hypothesis, only then does it become important to report the results of all attempts at confirmatory studies, including the non-significant ones. Of course in reality one should never use p values, but rather should use meaningful measures of effect size, with confidence intervals.","In my opinion, all covariates should be reported in the methods section, even though they are not included in the final models because they did not reach the desired threshold of significance.","In science, there is always a clumsy face, where you are exploring, thinking about your system in different ways. You are usually puzzled by different facts and you use your intuition to keep seeking for explanations. At some point, some unexpected finding makes you click: finally, you have come up for a reasonable explanation for the whole picture. Although it may be pedagogical for some audiences, I don't find necessary to always explain your findings in this chronological fashion.  ","In macroecology, it is common to explore different statistical relationships between the phenomena of interest.  But I can see that it could be a problem for estimates of significnace","It's OK if the p-value is used only as a guide to the weight of evidence; it's not OK if the p is a decision threshold.  Using the p as a guide just to weight of evidence is rare, hence my answer above.","Many statistical methods have underlying assumptions of data distributions. Either you abide to the assumptions and get rid of extreme values, or you violate them by including all data points. Which practice will bring more damages?","If used it suggests a misunderstanding of experimental design and statistical power.","It is dishonest and biased. ","Problems with data quality, or methods, should not be hidden and should always be discussed in the resulting manuscript or report","Simulated data points are completely different from observed ones. This is a case of data manipulation. It should be banned from science.",NA
"156","in theory it shouldn't be use and everything should be published. In practice it has to be since there is no platform for publishing negative results and no incentive to do so.","In preliminary data analysis, I sometimes decide not to pursue covariates further, e.g. if it was one that I considered as a covariate on a nuisance parameter (effort on capture probability).","In the exploratory approach, clear a priori predictions are sometimes difficult. It may be rather vague idea but with no good background justification. It is hard to draw the line when something ""was predicted"" vs. ""vaguely imagined"". ","In my area of research people often use AIC weights: weights depend on the entire set of models considered. Sometimes people decide not to provide the complete list of models and exclude those whose weight (or sum of weights) is very small. It is mostly the pressure for space in journals that incites us to do this, but we should not. / I believe that the issue is a serious one when people build models and exclude them from the model set a priori; these two situations are different.","It's reporting untruths as truths, to obey an arbitrary idea of ""significance"". Double-bullshit. It's not as though we can't afford the ink.","misleading. At that, it is possible to eliminate outliers, but again requires proper reporting","If you're lucky enough to be able to collect more data to elucidate suggestive but not highly significant patterns, I say do it! If the patterns are ecologically relevant but the process is noisy, I see nothing wring with clarifying parameter estimates with increased data collection. If you're p-value is 0.54 and you want a 0.49, you're missing the purpose of a p-value. ","It is forcing a particular result","Problems with data should always be acknowledged.","Simulated data points must always be identified as such.",NA
"157","increases false positive rate","In some designs, a wealth of covariates / Are tested. For clarity, reducing to a relevant set makes sense for reporting, but full lists should go to supplements.","Inference suggested by the data should be stated as such, and it easy to note what tests were posthoc","In my opinion, this is OK to do when you try out for example a range of models and select the best fitting model (not most statistically significant!). e.g. if you reported results based on a negative binomial model, but also tried out a poisson model and it was a poorer fit. This generally should be commented on in a paper, but all the details of the different model types used would not have to be included. ","It's rounding. As long as other p values are reported at the same level of significant digits and as long as the = sign is used, then this is a nonissue","My advisor and I disagreed on this point. I was told you can only remove an outlier if it DOES NOT affect the statistical outputs of the analyses. My advisor believed the opposite, you can remove outliers even if they change the statistical outputs of these analyses. I don't know what is the right answer.","If you already after the collecting the first 100 independent data have a highly significant result, it would be waste of ressources to continue with the next 900.","It is legitimate to switch method if you realise there is a more powerful (but valid) alternative. this frequently happens, especially in collaborations where people have different skills and stats training. Also if there are several complementary approaches then taking several is also OK - provided you report all results not just the ""best"" ones.  ","Problems with the analysis should be always given to give the reader a good basis to judge the results.","Simulated data should be clearly identified.",NA
"158","is a filter not everything gets published or warrants publication and the reasons for not publishing are numerous","In the Methods, all covariates that were considered should be listed. However, in practice researchers often explore their data, inlcuding implausible potential covariates. These are usually left off when reporting, whereas the a-priori potentially important covariates are - or at lest should be - mentioned.","Instead of answering this question, I want to point out that I think the opposite behavior is more common---that readers/reviewers brush off reasonable findings as obvious, when they aren't obvious if one considers all possible alternatives and realizes that many are reasonable.","In reality statistics can be an art as much as of a science, and it can be worth trying out different things, or the same models with different packages, sometimes to get a better understanding of how things fit and to test that the set of variables used and excluded (e.g. if there is collinearity) is the most appropriate one. It's just too much to try to include all of these 'exploratory' models in the article.","It's unethical.","My suggestion is if there is an outlier show both the results with and without it.","If you do a pilot and then a full study doesn't this always happen?","It is not correct to change the analysis approach until statistical significance! The test should be selected according to the nature of the data and the design.","Pros and cons of methods and data should be reported and this info must be as accurate as possible","Simulated data should be stated ",NA
"159","Isn't this a duplicate question?","In the past I have collected a multitude of covariates, but was unsure how to report the model selection process properly. These days, there are many good thoughts as dealing with this, and variable measuring should not be discouraged. ","It's 100% wrong to do this blatantly because it misrepresents probability. Survivorship bias in the results. But it's not always so clear-cut. There's explicitly stating it was predicted at the start and there's 'kind of giving that impression'. I had to struggle with these questions.","In some cases the results are similar with the different models, so I don´t think it is useful to report all these results. When the output change according to the model employed, I think it is important to inform them all.","It's unfortunate that there has become so much reliance on the P value that people would use this trick to claim their finding is significant rather than saying there was a trend but it didn't reach statistical significance.  Effect sizes and transparent data presentation are critical in this regard.","Never do that!","If you get a null result because the confidence intervals on the effect size are wider than you expected, or the effect size lower, then I think it's entirely sensible to collect more data to get more precise estimates of effect. But you should fix the new sample size a priori (based on power analysis and the confidence intervals you'd be 'happy' with) and report that you'd done this.","It is now standard practice for statistical pipelines to spit out multiple analyses, and makes it easier to succumb to the temptation to pick one that supports your hypothesis.","Quality problems should always be reported.","Simulation is NOT data. Period!",NA
"160","It's an arbitrary but widely used threshold.  It should be used to report results of a major hypothesis tested (i.e., when support was P &gt;0.05).  It can be a useful way of determining which minor hypotheses or lines of enquiry can be discarded (i.e., results where P &gt;0.05) when reducing scope and focus of a paper.","In theory everything should be reported, but in practice this is not feasible because results sections could be extremely lengthy.","It's a lie.  Lying in science (as in life) is bad.  In a statistical sense, this would also determine whether results are viewed as a posteriori or a priori, with associated corrections, etc.","In the context of Exploratory Data Analysis, one might test a model or two that you later conclude are inappropriate for reasons other than that they didn't conform to expectation.  In other words, people do naively test models that they later realize they should not have used for good reasons.  Those would not typically be reported.","It can be used when the rounding-off does´nt correct and meet the specific threshold. This is not the case in the examples because the p values were higher than threshold 0.05 or 0.01.","Never ever!!!","If you see a trend, collecting more data to see if it gets significant is not unethical. Sometimes you simply need more samples to have a large enough effect size to measure a significant result.","It is obvious that this is common practice and this should not be the case. But the pressure on ""novelty"" and ""convince the reader of the relevance of your results"" is too much. This is what journals ask for and our work is evaluated based on how much of our stuff journals buy. ","readers need to know the potential flaws /  / I do have some sympathy though with people writing in journals with such tight word limits that it is hard to fit these kinds of issues in.","Simulation is OK but it must be declared.",NA
"161","It's best to be completely transparent about which analyses have been done, which includes identification of non-significant factors. However these should still be reported. What may happen though is that in an exploratory analytical phase, a range of factors might be analysed that turn out to be non-significant. Again it would be considered good practice to report these, but if there are a lot of them, they might not all be reported. ","including covariates in the first place implies you have reason to think it relevant so should be mentioned","It's both dishonest and unnecessary. Why not just provide possible explanations for the unexpected finding, as part of the discussion?  /  / On the other hand, the pressure to publish these days can be intense, and often a cohesive ""story"" is needed to publish. If an important result, which should be disseminated, can only get published in the form of a story where the authors predicted it from the start, then this is forgivable in my opinion.","increases likelihood that only positive outcomes are reported and thus false positives. But p-values can be adjusted for multiple testing to prevent that bias.","It can lead to wrong conclusions","Not all data is good (due to possible errors in measurements, typing etc). Often, visualizing data allows to point out bad data. But those can only be removed because there is evidence that there was a measurement error, typing error, or other type of error that makes this data invalid.","If you statistically account for repeated sampling and be honest about the process, I think more data are better. But hopefully, with proper design, this shouldn't be necessary.","It is often named ""marginal significance (p&lt;0.1) This should be dismissed as a non significant result because it is a post-hoc decision. / ","researchers should be honest about whether their results are robust, but the pressure t publish and therefore argue against reviewer criticisms drives defensive practices.","simulation use some how a developped model. It becomes circular",NA
"162","It's important to report on trends as well as statistical significance so omitting non significant results fails to provide full picture","Including every possible covariate will eat up degrees of freedom and therefore obscure patterns. Also as long as you say you are presenting a reduced model, it seems appropriate to remove non significant interaction terms ","It's certainly not best practice, but given the time it takes to collect the data and the limited opportunities to do so, throwing away a bunch of results that may be informative is in my opinion worse. Of course, you have to be careful when interpreting, but if caution is exercised with interpretation and conclusion, then this isn't the boogey-man that many people make it out to be.","Increases the rate of false positives","It could only be justified if there were a uniform policy to round off p-values to two decimal places. The problem enters if researchers apply policies selectively depending on the results.","not ethical. Should only remove true outliers.","If you think your question will be improved by more data, add more data.","It is OK to do it if you had originally chosen an inappropriate test.  If the test is inappropriate you should also do it even if the first test gave a significant result!","results and conclusions could be false","Simulations are fine. Real data are fine. Mixing the two without explicitly stating so is misleading.",NA
"163","It's the unfortunate reality of publication bias.  If you come up with a solution, or an army of trained monkeys to help curb this tide, please do let me know.","Insignificant covariates (i.e. negative results) are just as important as significant ones, and this is practice discards valuable information. I understand that sometimes insignificant covariates can complicate relationships that ecologists try to clearly and simply describe, but again, supplementary material can still provide info of ALL covariates while allowing the author to focus on significant covariates in the main body of the article.","It's data dredging. Hypotheses should be formulated before testing. It's basically lying.","Is better to provide the complete list of tested models, as well as the cirteria of selection.","It depends on the context of the analyses being performed and the types of outcome that one is looking at, p-values are not always the most important output from a statistical model","Not sure of question, but I think you mean for example if there is an outlier or something that risks violating statistical assumptions, testing whether it's exclusion changes the result and reporting both? In which case the above is my response. Often it is a useful response to reviewers to provide such a check.","If your experiment is properly designed, then you should be able to determine proper sample sizes. However sometimes you figure out your stats model is incorrect and realize you have lost a bunch of D.F.s so need to add. Other times reviewers aske for more data when p&gt;.05 even if there is a large effect size, so then you are forced to collect more data","It is sometimes important to show that the choice of model does or does not affect the conclusions. Again, as long as this is clearly reported, it can sometimes be important to show.","results may be misinterpreted. ","sometimes it is necessary to fill missing data, but of course one should report precisely how this was done.",NA
"164","It affect scientific development. We should be aware of well conducted studies that failed to find significance.  / Note: I am not sure I clearly distinguish between part of this question and the first question of the survey. My answer corresponds to reporting of studies.","Insignificant result is also an important result.","It's dishonest to present post hoc results as a priori predictions, and makes the evidence seem stronger than it really is (e.g. Texas sharpshooter fallacy)","It's deceiving. If models are not preented, there should be a clear reason for and it should be states","It depends on the number of decimal places to be used, but shoould not be used to ""trick"" the P-values to significance (as your first example)","Not why we ignore outliers.","Improving the statistical power of a promising experiment following pilot data collection is a good practice.","It is the same as multiple testing. What you find in the end should be corrected for testing multiple hypotheses","Science needs to be transparent so that it can be replicated.","Sounds very much like data fabrication",NA
"165","It all depends on the context and on the reasoning behind why a variable was tested.  In addition, text length limits used to militate against reporting all results - they can now be included in supplementary material.","is easy enough to leave non sig corvariate in the data set as publish the dataset as a digital appendix....i think is a a good thing to do as others make learn from it.....non sig results are still results","It's hard to keep people from doing this, especially since this will often get you into a better journal, but it is technically dishonest.","It's generally post-hoc interpretations masquerading as theory-driven. I have done this in the past, but generally only when responding to reviewer comments that say more variables/controls are needed. I begin with a set of models derived from theory. ","It helps distinguish differences between datasets","obviously inappropriate","In an ideal experimental design, testing only hypothesized outcomes, perhaps it should not be done.  In reality, one may not have the resources to make a prior and completely separate test of variance.  In my own case, the only times when I have done it are when I detected a significant but unexpected trend and decided to collect further data to independently verify it.  In reporting in the manuscript, I was essentially convinced by reviewers to consolidate testing all data at once, but in actuality I had done separate tests on independent data that yielded similar results.","It makes sense if one is trying to discover the quantitative relationship between values or phenomena.  But if one already has a clear hypothesis that was not supported by the relevant analysis, then it is not valid to go searching for a method that gives a significant result","sciences are not dogmatic information, sciences always are prone to future testing, refutation and improvement. Reporting potential problems is a way to preserve the very nature of science and may save time and effort to future researchers ","Super dishonest. It casts doubts on the entire profession in times that really do not need it.",NA
"166","It biases reviews, meta-analyses and the overall impression of what the literature supports. ","It's difficult to answer this question because it's much dependent on context and research question. At an explanatory phase a researcher may consider a large set of explanatory variables and then make a selection of the most meaningful ones, to avoid colinearity or variables difficuly to interpret. Unused variables will often not be mentioned I think.","It's misleading about the design of the experiment, and lends itself to cherrypicking of results. It's more honest to include it in the aims as a question that was asked of the data.","It's not great statistical practice but sometimes you have to do it when optimizing parameters for a model. ","It inflates TYPE 1 errors. Prior to about 2000, I would say it was an excusable error (but an error nonetheless). However, with modern computer based statistics one can give the p value exactly and so approximating p values is unnecessary, and therefore this type of rounding error should have been eliminated.","Obviously this is never OK, if the reason to exclude is to achieve a desired p-value.","In ecology we rarely have the luxury of re-visiting the field, so I don't think this is likely to be a common problem. Also I am not sure why it should be a problem so long as the work is continued within the original design framework and effect sizes and degrees of freedom (not just p values) are reported. ","It makes sense to keep assumptions simple, but that can sacrifice power. Sometimes adding an arguable additional assumption can make sense, but both results need to be described and the differences discussed.","See earlier responses.","That's a crime and disruptive to the confidence in science",NA
"167","it biases the literature towards significant results.  /  / It's very difficult to convince collaborators to pursue non-significant analyses","It's easy now to put in NS results into Supplementary files","It's misleading so should be avoided. However, sometimes there is no obvious a priori prediction so wording becomes important here.","It's not ideal but usually in early data exploration stages you're not quite sure how you're going to best demonstrate the relationships in your data. ","It invalidates a priori choice of alpha-level.","Obviously, hand-picking data to give the results a researcher wants undermines the very idea of using statistics. ","In my field data collection is virtually always constrained by logistic factors or the environment itself, no statistical considerations. Up to some reasonable point, more data are usually better. Depending on goals of a study, preliminary analyses along the way to assess when enough is enough can be OK. ","It should be used only if you realize a problem with your initial method (e.g., assumption violated, lack of fit), independent of the p-value you obtained. / More in general, it is p-hacking and lowers the reproducibility of your research.","Seems clearly wrong - misleading readers by omitting information needed to properly interpret results.","That's straight-up fraud.",NA
"168","It can be misleading.","It's good to know when experimental treatments, etc. had no effect on the response variables, as that is also useful information, but on the other hand, there is only so much room in the manuscript so it's difficult to include many negative results.","It's misleading. However, there is strong incentive (often insistence from reviewers) to do so due to the obsession with framing all results in light of specific hypotheses (e.g. presented in the introduction of papers, which makes it seem as though the prediction of the finding preceded the study).","it biases the literature","It is a misuse of the people value","Obviously, it can be argued that outliers could be removed. I think any removal of data needs to be reported and a rationale given.","In my opinion this can be used if there is evidence that the statistical analysis (whether the results is statiscal significance or not) may be biased by the size of the dataset","It should be used ONLY when the first test seems to be wrong and this was noticed by an awkward result.","Should be self-evident.","That is horrifying to think people might do this!",NA
"169","It can be used to simplify the presentation of the results, never to hide negative results. ","It's impossible to test for everything and give all the p-values - I typically give a list of what I tested for and that they were not significant factors.  This gets into conflation between covariance and causal patterns... ","It's not ideal, but it simplifies a lot putting together a coherent story for a manuscript. It might actually be beneficial in that the reader might get a clearer message.","It can be used if it is clearly justified why over models have not be retained","It is better to give the answer to more significant figures than to round off downward.","Occasionally outliers are removed (always stated in methods). Testing model prediction with or without outliers.","In my research field, we are constantly doing partial and preliminary analysis. There is rarely any study to set a specific sample size depending on the expected effects (as in drug testing designs). Thus, I see no problem in collecting more data to have a more robust conclusion by the end of the study.","It should be used when necessary. For instance, when a multiplicity of tests or indices are available (which happens often) and there is no consensus on whether an optimal analysis exists. Again statistics move forward rapidly (faster than approval of grants, generation  of results, etc) and its hard to keep up to date. One should always check whether one is using the most updated or best analyses and whether a wrong statistical choice may be part of the failure to observe significant results. The question on whether we have made appropriate choices (again upon existence of multiple methods) should always be asked. At all times we should keep research integrity. ","Should never intentionally be used (e.g. to deceit). / But, in practice, there are nearly infinite ways in which the data and the analyses could potentially affect the conclusions such one cannot report them all, and it would not be useful to do so even if it were possible.","That is misleading.",NA
"170","It depends entirely on the expected reasons for a lack of statistical significance.  Studies with high variation or a poor design or insufficient replication may lack the power to detect significant differences statistically.  In this case, a statistically insignificant result is uninformative, because it does not indicate the absence of an effect, but simply that the effect was not detected.  Where a robust test is undertaken and the results remain statistically insignificant, they should be published, as this is at least strongly suggestive of a lack of an effect.  Where many variables are initially tested in large models, best practice should mention all of the variables that were considered, even if highly non-significant variables are later dropped, but I think this is increasingly recognized and followed.  There is still limited scope for the publication of studies with entirely non-significant results, and so I suspect the many such studies still go unpublished, but whether that is appropriate or not depends on the initial design.","It adds to researcher degrees of freedom: the number of covariates tested influences the probability that some covariate or other will be statistically significant by chance. Moreover, the failure to report (non-significant) effect size estimates eliminates from the literature quantities that could later be leveraged in, e.g., meta-analysis, which has the potential to detect real but noisy effects across studies","it's not the truth","It could easily be done by people who do not understand the models.","It is cheating","Of course this is not allowed, only before the analysis if you have a reason to discard outliers.","In order to give more statistical power to an analysis, a study can be started again (independently fro the first one) with more data","It should be used, but together with explanation and exposing the basic data set and the FULL statistical methods.","Should not be used. This is lying.","That is severe misconduct and everybody knows it; if it is done, it will be performed by one individual in instances when no one else can verify how the data is treated.",NA
"171","It depends highly on whether it was a variable of direct interest or not. If it is a main variable of the paper, it should always be listed. If it is something of a side analysis that is potentially of marginal interest then there may be reasons why that side analysis need not be listed. ","It can be useful to explore a data set for possible relationships that you may not have thought were important, just to confirm that you were not missing something.  If these relationships were not significant, and you had no a priori reason to think they would be, it would not always make sense to report those tests.  This kind of data mining should not be taken too far, however. Ideally, the final analyses should be based on a priori hypotheses, and all results reported regardless of significance level.  But with a large, observational data set, I think some data exploration can be useful.","It's simply more robust to report unexpected findings as such. This has happened with one of my models and the fact that the phenomenon had not consciously been built into the model lent significance to the model.","It could lead to hide non significant results or ""unwanted significant"" results.","It is cheating. I do round the other way (presenting p=0.04 as 0.05) to simplify presentation, though even that is wasteful and questionable.","Often one does not know which points are actual outliers until conducting the statistical analysis. Identification of which make the most difference to significance is often interesting and tells me something about the data set and that particular data point.","IN order to have a robust statistical output, a sufficient sample size should be met. If analysis shows that sample size is not enough for robust testing, more data should be collected. ","It should never be used IF the model chosen first was appropriate to the data structure. If say after looking at the model a different data distribution family such as negative binomial or random effects structure should be used than I say it is fine to switch until you have the right model for your data. /  / I will also sometimes run or more complicated analysis for a reviewer but than present a simpler one if the results are qualitatively the same and it more likely the audience will understand the analysis. I will always state the other structure used in the paper and state that the results were qualitatively the same.","Someone else will do this for you - either a reviewer or a future researcher.  ","That is simply fraud.",NA
"172","It depends on how this is done but non significant results have value but are difficult to publish.","It depends on the main questions of the given research.","It's the whole point of research, finding things out we did not expect! We could write it saying 'we wanted to do something else but found this interesting observation', but what's the point of that?","It depends on how epxloratory these unrerted models were, or which set of variables they tested, or if they represent ""sub-models"" nested within more inclusive models in the final report.","It is clearly fudging the data. I have a practice that I always round to several digits beyond whatever arbitrary alpha value I'm using. I think this is pretty standard.","Often outliers can be dealt with statistically. However, there are concerns about bad data - e.g., it was entered incorrectly, the respondent did not answer the question, etc. If you tell me you threw something out and why, I am often OK with that. / Honestly, we don't always clean our data well before we run our analysis. If we don't find a significant relationship we often then make sure our data is clean. This is often when we discover mistakes. Fixing them is fine. Throwing out outliers, that are good data, and pretending they never existed is not.","In particular studies it may be important to cross-check whether initial statistical significance is valid or not by increasing sample size. By increasing data, initial statistical significance may be lost or improved and this can be informative.","It should not be used provided researchers are sure to make the choice of the adequate statistical method at first.  / Very often, when statistics do not match what graphics shows, researchers ask help to statisticians or other researchers and get aware their initial model was not adequate to the data. ","Sometimes there's simply not space in a paper to discuss all potential issues tested for...","That seems crazy.",NA
"173","It depends on the nature of the study and the variable - if the study is well designed then null results are absolutely publishable, if the study is not well designed, then maybe its not informative to try to publish null results because the results could be because the factors/variables of interest were not that informative, or because of poor power to detect their impact. Moreover, within a given study I think its natural to focus on those variables that did show or explain differences, though again, if a variable is associated with a good plausible hypothesis, then reporting or focusing on the lack of an effect associated with the variable can be very interesting.","It depends on the purpose of the covariates. If these were nuisance variables (i.e., not part of the object of the study - they were  examined only to try to reduce unwanted variance) then it is appropriate to ignore irrelevant ones. However, if they were the object of the study, they should always be reported. For example, if studying effects of landscape variables on presence of an animal, and weather at the time of the survey was examined as a possible covariate, there is no need to report if it is not significant (though it could be mentioned). However, if purpose is to look at effects of weather on behaviour, then all weather variables examined should be reported, regardless of significance. ","It can be due to that you didn't think of a possiblity that is known in literature","It depends on many things e.g. the idea of study.","It is deceiving readers. Also, more info about the p value allows readers to make better judgements about how convincing they find the work- so using several dps is always a good idea for p values.","Often you see the outlier after you do the stats - but better to remove it before the stats. I'm sure some people don't - of course it shouldn't be removed to make the study significant, BUT there might be reasonable reasons as to why you should.","In principal this is bad, but as a primary researcher I often built my models in R as I was collecting data so I had time to prepare the stats (and my stats training) for the final data. I therefore DID run stats on preliminary data, but I had already designed the study and had a fixed sample size to complete, so my decision to stop work was NEVER affected by the findings of my preliminary (scoping) analysis. It would be naive of us to expect people not to be curious about their data - and so analysing a preliminary set isn't the problem. ACTING on knowledge of the preliminary (non-comprehensive) body of data is the problem. In ecology I think most people are working at the lower threshold of what they consider to be a suitable sample size (n=30 'gold standard'), so I doubt people would stop at 10 if they found a significant result. What might happen is that some people continue to look for significance when they don't find it after reaching their minimum n threshold. If this is clearly stated and a new end point defined I'm not sure I see a problem with it. As long as people are aware that you shouldn't stop because you find a SIGNIFICANT result, then I think it's OK. But I've not thought carefully about this, so there may be problems. ","It should only be used rarely.To use a very conservative model for biological studies sometimes no results. But you could learn more about biological systems if you use a not so conservative method. Sometimes we need a hint where nature is going to. ","Sometimes these ""problems"" are difficult to explain briefly, posing a problem in writing. However, it must be done and most of the time is not that hard.","thats just objectively fraud",NA
"174","It depends on the research question, but in principle one should give all the info available also the non-statisticall significat once...","It depends on the specific context","It can be used because sometime, unexpected findings can open new perspectives resulting in new starting hypotheses.","It depends on the specific context.","It is dishonest","OK to exclude data you think is problematic for a specific reason, but only if you decide a priori it is problematic","in some senses, this is like doing a post-hoc power analysis for study design","It should probably only be used when the initial model finally deemed inaccurate and needed to be adjusted for correct inferences...","Sooner or later, you will pay for the dishonest behavior. ","The fundamental problem here is lack of transparency.",NA
"175","It depends on the study but failing to reject the null hypothesis is an important result, especially if there are multiple competing hypotheses.","It inflates probabilities of type I errors","It can conceal the number of tests/investigations that have been carried out and therefore give more weight than is due to a result.","It depends what 'the complete set' means.  I almost always end up trying multiple analysis approaches (eg fixed vs mixed effects models for 'year', multiple ways of handling variance), and usually no one set is perfect.  So I pick one way, present all those models, and maybe say I tested another group in the methods, but dont neccesarily go into all discarded techniques in detail.  I think that's not what you mean, and that you mean not mentioning explnatory terms that were tested and discarded? but I cant tell so cant answer","It is dishonest.","Omission of a part of data is not unusual, and my point is only that it should be done before knowing the result of statistical hypothesis testing including the data.","In the context of preliminary experiments. Of course your full experiment is going to be larger, and, based on the expected strength of an effect from the literature and preliminary experiments, power analysis can help you decide how big a full experiment would need to be for you to detect an effect if it was actually there. If you need heaps of data to detect an effect, though, it's probably not all that ecologically important, even if it's significant. This will come out in the strength of the effect when it's reported, so even when the practice is used outside of the context of preliminary experiments, I don't see it having a large impact on our understanding of ecological systems.","It shouldn't be used because it is ""p-hacking"". The only time it is acceptable is if alternative approaches are actually better suited to the analysis and the researcher was not aware of that prior to review or consultation","Such issues must be raised in the discussion. Occasionally, however, it is quite likely that during the review process texts must be shortened so much that there is no space to cover all critical aspects. ","The practice is fine but only if you identify the interpolated points. Otherwise it is artificially inflating your sample size.",NA
"176","It depends on the study, but if there are lots of tests for individual variables, it is often sufficient to report only sifnificant results. ","It is a textbook sample - testing for significance of covariates and if they are not significant, they should not be used. The exception is when your hypothesis includes a covariate - then it must be reported.","It confuses the issues at stake and obscures the state of knowledge in the field. /  / It makes the author(s) look stupid and dishonest and reduces credibility overall.","It encourages people to find the model that's right for them (i.e., yields the answer they want).  If other models are not also disclosed, it gives the false impression that the finding is ""highly significant"" and was confirmed on the first run of the model.  Other models should be disclosed in supplementary materials at least, given that preregistration of studies is a long way from becoming standard.","It is dishonest.","On the advice of a statistician, sometimes outliers can be excluded if they skew the data. But should be done cautiously.","inappropriate statistical results unless this procedure is modeled","It shouldn't be used unless there is a valid reason why the first test was not suited to the data and the second suits better. Alternatively, both results can be reported.","That would be totally dishonest and hold back scientific progress, because it means other people following up on the problem will make the same mistake.  Also a waste of resources and time.","The practice of this isn't the problem: to learn what is going on in your system this is something that often needs to happen. The problem is that it's hard to report this right now. We need to fill those gaps with code and share the code and data so that it's clear what was raw and what was simulated. ",NA
"177","it depends on the study. In either a descriptive study or a natural experiment, one might examine many variables that may or may not have significant effect on the response variable. I personally like including everything that I have tested and put this in a supplementary document. However, I feel that generally, in the interest of conciseness, I am encouraged to leave out non-significant results. In a manipulative experiment, I definitely think everything should be reported.","It is common practice in Ecology journals now to not have non-significant values showing, as a way to avoid cluttering. ","It depends of the kind of study. In a experiment designed to answer a question, unexpected results should not change the first hypothesis and the introduction as if the experiment had been designed to answer this unexpected results. But in the analyses of field experiments or field data, hypothesis can be more open or wider, and unexpected results can give the researchers a different picture of the study system and a different way to focus the research when it comes to publish it.","It gives me a broader view of the problem and the empirical patterns. ","It is dishonest. ","Only if there are independent reasons to believe those datapoints are abnormal. Ideally should not change significance but better reflect effect sizes.","Increasing the amount of observations may help to confirm or not a response pattern that has been found a restricted sample. Field sampling can be difficult, expensive, and time-consuming, so in some cases the use of such practice can be understandable","It turned out the original analysis I did was not well suited for my data in that the ANOVA model I used was not correct. Using the right one that partitioned the variability differently did change the significance. ","The drawbacks are an important part of the process.... I would reject papers that have not covered all the issues. One would hope the reviewers would always pick this up.","The thought never occured to me to do that. REally?",NA
"178","It depends on what kind of independent variables are not explaining results of the dependent variables. In multivariate analyses sometimes there is a lot of noise in the data and one needs to choose what to report and what not (of course based on stats such as PCAs etc.). I think it is VERY important to report non significant values because other scientists could rely on these results to not try to test the same things and try to look for other explanations (i.e. i.variables).","It is good practice to be entirely open and transparent about which covariates have been tested","It depends what you mean by ""predicted from the start"". If you mean this to refer to the prior expectations of the individual researchers during data collection then I don't think this is particularly an issue provided that it is done carefully (e.g. if you find a signal that is predicted in the literature/from a priori deduced logic but didn't motivate the original data collection/analysis then you may need to redo the analysis to be less exploratory and more focussed on the newly recognised hypothesis and only proceed if the effect is still supported by this analysis). However, if you mean results that are unexpected to any a priori supported (testable) hypothesis in the field (even if it is only identified post analytically) then this should never be done.","It inflates the Type I error. I can accept this practice if different models were tried for choosing the best error structure (without evaluating the fixed part of the model!), and details of this process is not published, only the final model.","it is easy enough to report a p value as less than a threshold. Rarely do I personally report them as 'equals to' but in that case researchers should include 1 significant figure beyond their target threshold.","Only if there is something wrong with that individual should the data be excluded (e.g. It died right after the trial or was missing a leg), or as a researcher you screwed something up. Otherwise both models with and without should be presented","Increasing the number of datapoints sounds like a good idea in general. It should give better estimates of effect sizes. / Quitting a study when no significant effect appears should be fine as long as the study itself is reported. Other can use the informaton the design their experiments and make sure the number of replicates is sufficient to detect a certain effect size.","Journals like low P-values.","The impersonal formulation of this statement is problematic: does ""known"" mean ""known to the researcher in question"" or ""known to someone""? It also raises the question of whether that ""knowledge"" is contested, and perhaps wrong.  If problems are widely enough ""known"" (e.g. the way that P-values depend upon sample size), I don't think it's always necessary for a paper to ""disclose"" them.","There are acceptable correct ways to overcome problems of missing data / Some larger datasets do contain missing values, and their omission would harm more than / using the set with imputed missing values (see the stats literature) / ( I don't know the meaning of simulated here) / it is though possible that in some top journals these details are not reported fully",NA
"179","It depends on whether the variable in question was important in framing the hypothesis being tested or how well the study was conducted. I don't see a problem jettisoning peripheral data or those that the researcher has less confidence in. Alternatively, core data that are reliably collected should always be reported, and the numbers given above would change substantially if these qualifiers were specified in the questions above. ","It is good to also give negative results which include important information.","It does not accurately represent the scientific process used in a particular study.","It is always better to give the reader the full information about what tests were tried.  However, word limitations often make this difficult or impossible.  However, if the unreported tests change the way the reported tests are interpreted (e.g., because of corrections necessary for multiple tests, etc.), then there is no excuse for not stating at least the correct number of tests.","It is incorrect behavior. The actual value has to be recorded and interpreted/explained.","Only if there is strong external evidence that the offending point(s) is/are wrong. In other words, you analyse, you see outliers and then you investigate them. Note - you can't be pure with this because the analyses you use to check for outliers (e.g. generating model residuals) will frequently give you P-values, so you have no choice but to see the p-value...","Instead of searching for significance, many colleagues have conducted power analyses and then recollected data based on these findings.  ","Less common, but something that people learning stats (particularly complex stats e.g. mixed effects models with non-Normal error) probably often do. I have done it but never in any published work. It's not always clear what the BEST statistical test is - there are sometimes options. People need to be taught that all choices should be a priori.","The problem is that there are often imperfections in a study that could potentially impact results.  The author should disclose those that are the most likely to have an important effect on results.  Others that are more minor should be judged by the investigator and might not be disclosed if thought trivial.","There are multiple statistical methods to fill missing data in order to save other variables gathered and their application must be explicit in order to set a clear message on the strenght of the data. The ideal must be to do both analysis with and removing missing data and see the effect of these on your conclusions",NA
"180","It depends what is meant by ""not reporting"".  I regularly identify that variables q, r, and ... did not significantly affect a dependent variable (p &gt; 0.05 in all cases) without providing more detail (e.g., estimates partial regression coefficients).  I have followed this practice because of limitations of journal page space.  With increasing use of supplementary material, this practice could change, but I'm not convinced of the benefits of doing so.","It is important to report ""negative results"". For example understand general patterns (e.g. for meta analyses).","It doesn't really matter whether the authors predicted the results, it is the results that matter because that is what readers should use to make their own interpretations, and it can make for a more succinct and easy to follow manuscript if there is nice logical flow to it.","It is critical that readers understand what was compared, if one is to evaluate the results.","It is intellectually dishonest.  If you are using a particular p value as a standard, you should be honest about whether your results meet it.  However, I think that it is fine to talk about results that are close to the standard (while admitting what the actual p value was), as statistical significance levels are somewhat arbitrary and should not be an all or nothing thing.","Only justified in the case of outlier testing","interest in identifying interactions may warrant the need for greater sample size","Many types of data can be correctly analysed in different ways. For example, there is nothing wrong in the analysis of normally distributed data by non-parametrical test. As long as the assumptions of the selected analysis are met, I do not see any problem in choosing between different applicable types of statistical analysis.","The problem with this is that often word limits are such that methods must be so short these days. I usually put this information in supplementary materials, which are not peer-reviewed. If the journals gave us the space, I would include it in the methods, where it should be.","There is a vast literature on imputation and multiple imputation.  Why not use it?",NA
"181","It distorts the meta-analyses, and repeating these tests by others who think is has not been ever tested wastes energy","It is no fault to find insignificant results, and there is some added value to know which factors are used, but not related to the current question. ","It gives a false impression of our prior understanding of the biological system; the cart is leading the horse.","It is easier to explain when including all the models tested, and being throrough is part of good scientific practice. Should be expected by scientists for themselves and when reviewing the works of others.","it is misleading","Only when it is statistically proven that certain data points are analytical errors for example. In any case the value/s should be presented as well as the justification for removing any data","Is this not analogous to a power analysis? Most ecologists don't have the resources to collect more samples, but that doesn't mean it shouldn't be done. I think what shouldn't be done would be something like look at some preliminary results and then go collect a non-random set of samples to confirm your pre-existing ideas, etc.","Maybe you choose the wrong test.... oh, the uncertainty!","The quality of the science depends upon the data and the analysis used; if there are problems with these steps, then this should be made clear to the reader such that they can assess the study properly. This is especially difficult for reviewers, who often have to trust that the data and analysis as shown by the authors is sound.","They are not the actual dara",NA
"182","It is a little dishonest to only report studies with significant values, but I have definitely done it ","It is unnecessary to report *everything* that was tested if it does not contribute to the narrative; however, those values should be made available in a supplementary table or code","It helps highlight a specific point/message.","It is hard to avoid during exploratory data analysis, but it invalidates the assumptions of a priori hypothesis testing. ","It is misleading, since we generally report P values as &lt; X.","Or, rather, it should be used *only* if exclusion of the point is stated clearly (i.e., there is transparency) and there is a justifiable reason to exclude the point. It is often the case that apparent outliers are not outliers at all but tell us something interesting about a system that could not be appreciated otherwise.","Isn't the reverse just as bad? If you stop taking data at the point that it becomes significant? /  / Anyway- like most things this probably depends on the experimental design. If you take preliminary data and then do a power analysis to determine the replication you need than of course you will take more data after doing a statistical test.    /  / For my experiments this usually isn't an issue because there are only so many experimental plants. I can't just conjure more to take measurements on after looking at a statistical test. Perhaps for field surveys this is a bigger issue.","Methods differ in power, how parameters are counted, rates of misspecification, etc. I will often use more than one, whether the results are statistically significant or not because it is not yet clear which is least biased.","The reader has to have the possibility to assess the validity of the study. Else it is worthless.","This equates to scientific fraud",NA
"183","It is bst to report all results and effect sizes and avoid setting arbitrary tresholds to what might be significant. On the other hand, when citing other studies, it is impossible to know what they have not said, so omission of studies with nonsignificant results is unavoidable.  / Focus should be on hypothesis testing, and results reported whether or ot they support the hypothesis. Sadly journals discourage this.  ","It is used because journal space is (was) at a premium. It shouldn't be used if there is suspicion that the model was overfit or misspecified. ","It helps making the framework clearer, as long as the alternative hypothesis are presented at least in the discussion","It is important to explore your data using a variety of models, while also keeping clarity when reporting.","It is misleading.","Out lier analysis exists and are used. Still it makes me uneasy when there is no explanation for the outlier and yet it is removed. OK, as long as it is mentioned in the stats analysis. /  / I assume this question is about outliers, and not about simply omitting data points till you get a certain desired outcome; that is claerly fraud and should never be used.","It's an issue of deciding whether you have the power to detect an effect. I've also gatehred more data and had the effect go away!!","might be that some methods are more demanding than others and that you want to test for different methods to compare results and then decide what to use","The reader is not going to have dove as deep into the data so you have to be transparent about issues.","This in a sense data fabrication, should not be used.",NA
"184","It is difficult to publish not significant results....","It leaves open questions in the minds of readers as to whether those covariates were even considered.","It inflates type I error rates. If you get an unexpected finding, definitely explore it but collect new data.","It is impossible to report all the tested models.","It is not acceptable to round down a p value to meet a specified threshold - rather, we should be questioning why we are still using these specified thresholds in the first place.","Outlier analysis requires a reason for excluding each outlier that is specific to its own circumstances.  Disagreement with the hypothesis is not such a reason.  The _ocus classicus_ of this discussion is the Millikan oil drop experiment.","It's called two stage sampling ","Model selection should have to do with overall model fit, not cherry picking for ""best"" results. I think it's fine to change to a different model than was originally planned if it proves to be a better fit, regardless of how it changes the results. ","The statistical problems (sample size, freedom degree, etc.) and experimental limitations should always be included during the description of the Material and Methods or in the discussion highlighting the limits of the study and how this could be affect the results obtained.","This is academic fraud.",NA
"185","It is important also to know which variables failed to reach statistical significance and discuss why, because it may reveal properties of the studied system itself, rather than inappropriate statistical design.","It masks the identification strategy of the model and the results.","It is data dredging. Multiple testing is performed without adjusting p-values","It is intellectually dishonest.  I don't think that all models should necessarily be reported, but one should not test just a subset and then claim that it was the whole set.","It is principally very problematic; if thresholds are used, P should be stated as P &lt; rather than P =.","Outlier detection and quality control is an art. I wish that we all performed outlier handling without any knowledge of how decisions affect our statistical inferences. But we're human. I wish I didn't, but I think most of us probably do at some point. ","It's important that additional data are collected following the original sampling protocol (i.e. sampling sites must not be specifically chosen to ""confirm"" any patterns that the initial sample revealed); but if this is observed, more data mean a stronger analysis. ","Models and testing procedures should be chosen based on the form of the data and the particular type of inference one wants to conduct.  Choosing it based on results is a clear violation of academic integrity.","There're are always data issues. It's important to be transparent about what they are, but some issues -- like teh time teh data collector got stung by a bee -- or somebody spilled coffee on their data sheet -- well not evry suspected source of error can be reported but important ones must be.","this is an outright lie",NA
"186","It is important to report on all areas of scientific progress, even those that do not fit neatly into the story crafted by the researcher. Additionally, publishing non-significant results can help other researchers save time and resources.","It potentially makes p values look better than they really are. Also, giving full information about analysis should be standard- knowing a factor didn't have an effect in a study might be useful info for someone one day.","It is dishonest and does not represent the true scientific process ","It is misleading and therefore poor ethics.","It is scientific misconduct to report forged statistical results","Outlier detection is an important part of statistical practice. What you do with outliers should be justified.","It's not about reaching for a P&lt;0.05 threshold- it's about checking whether something might be going on, and assessing whether you have enough statistical power to definitively answer the question. If my data are ambiguous, I will try to collect more to get a clearer answer.","Most common - switching to non-parametric methods when one had planned to use parametric approaches. This is fine as long as (a) you inform the reader; and (b) you modify your significance level to account for the multiple comparisons.","There's a trade-off between disclosing all issues with the data and analysis and the clarity and brevity of writing. In data-intensive research, if researchers report the most important issues and problems, I think that's acceptable and leaving out some minor issues can be acceptable.","This is cheating your data",NA
"187","It is just as important to report what doesn't work / isn't relevant as what does work / is relevant. However, ideally such results should be integrated with studies showing what does work. I.e. show report the negative with the positive. I always do this.","It should be used only when the covariates do not correspond to hypotheses that should be taken seriously a priori -- ""challenges"" to a specific new finding based on ransacking the literature.  It should not be used for covariates that were the topic of serious arguments on the part of others.","It is dishonest, and I don't see the point. Unexpected results can be interesting.","It is necessary to present all analyses conducted with their respective results, even as supplementary materials.","it is simply incorrect","Outlier removal is rampant and probably one of the biggest issues. The only legitimate reason is biological or technical, not its effect. ","It's rare for anyone in my field to do a power analysis before collecting data. This would ideally indicate how many samples you need to get an effect, assuming a particular effect strength. ","Multiple comparisons is a problem. It would be best if statistical steps were disclosed and the classical approach of designing your experiment for a specific test, running it, and accepting the results was followed. The reality is modern ecological experiments are extremely complex, things rarely go completely as planned, and as a result models and statistics are extremely complex and often designed to fit the outcome. ","there are always problems with both, and we can only describe so many problems in a single paper","This is cheating!",NA
"188","It is no fault to find insignificant results, and there is some added value to know which factors are used, but not related to the current question. ","it should be used when appropriate. We also need to consider biological reality not just statistical purity, which is naive.","It is dishonest.","It is not always necessary to show the results of all models tested (i.e. ok to exclude preliminary models, models with very poor fit, etc.), but that must be acknowledged when reporting results. ","It is stupid if people think a  result is important if the p-value is 0.049 and not if it 0.054. These are arbitrarily selected numbers to begin with! I report the p-value and people can make up their own minds","Outliers are a thing; they don't just wreck p values but also parameter estimates; if you don't catch them (as you should) during initial plotting of the data, then you take them out when you realize they're there, and redo the test. You mention that outliers are removed.","It cannot harm to collect more data to run a more proper statistical analysis. However, in general the sample design should be first designed and then tested without any intention. I find it problematic if all tehse measures are done by intention because they may affect the results...","Never, it totally ruins the statistical logic!","There are always some kinds of problems, and all sorts of model validation statistics should be presented in Online Supplements. People rarely do this.","This is clearly a misconduct of scientific research.",NA
"189","It is often a question of time--why publish something that shows no results when there are so many other papers you need to work on that do show results. Of course, in the best of all possible worlds, one would publish the results of all studies.","It should be used, because if we tested a covariate, we should report the result. We should report negative results.","It is important to distinguish between exploratory and confirmatory analyses, as the distinction influences the strenght of resulting inference.","It is often good practice to compare models, working out how best to transform a variable to meet the assumptions of normality (checking residuals etc...), which can mean dropping candidate models. However, ultimately the method used to decide on the final model must be described fully.","It is unnecessary.  P values should be reported to 3 decimal places.","outliers can be discussed, but shouldn't be deleted from the study","It could be useful if initial results are inconclusive (e.g., p-value of 0.051) but should always be mentioned. And if more data are collected and they make the results less significant, the new data must be included anyway.","No statistical model is perfect, there are different ways to analyse the data. Often it is needed to get first a feeling for the data before choosing the final statistical method fitting to the data structure.  / From my experiance changing the methods only affects the results if the presumptions for the initial analysis were not ,eat.","There can be problems in any study, and they should be discussed.","This is clearly unethical. I have imputed data before when needed to run programs (like SAS programs that require all cells are filled in) but have always reported this.",NA
"190","It is often difficult to discard the lack of statistical power when having non-significant results","It should not be used if p-values are reported or the study was not ""pre-registered"".  If the study was clearly exploratory or observational, then it may be forgiven, but I think that at least a list of covariates explored should be provided.  ","It is intellectually dishonest.  There may be gray areas in this, however, in which an investigator may find something unexpected from exploratory analyses, but then realize on thinking about it that it is what they would have expected, given more complete information about the situation.","It is potentially dredging: if the other models were genuine candidates, then they should be reported as well. ","It may add the soundness.","Outliers could be excluded, but p-values for the models both with and without outliers should be published","It depends on the context in which this happens - I think collecting some preliminary data and then doing a preliminary analysis is fine.","Null results aren't appreciated so why waste good data? As long as you aren't misrepresenting the results it's fine.","There is a really difficult trade-off to make: disclose all details and be shot down by a reviewer and colleagues, or disclose most details (including those that would impact the results) and have the research judged by its scientific questions. Reviewers right now are not tolerant to fully detailed analyses and this is sad. I would prefer to fully disclose here.","This is completely unethical and is basically data fabrication.",NA
"191","It is often frustrating reading litterature to not have the exact Pvalue associated with all studied regressors / factor, as say 0.07 is very different from 0.80. Then, space is limited in a publication and authors are pushed to publish the most interesting results ... ","It should not be used. We should always report it. Even not reach the threshold, it can still give information.","It is misleading for the reader of the study. ","It is sometimes inevitable in exploratory analyses...there is not enough space in papers to report every analysis that was conducted to narrow the scope of possibilities /  / It is shameful when the study claims to be confirmatory","It might not accurately reflect the nature of the observed phenomenon","Outliers do need to be removed sometimes, but people should always report the model results with and without removal.","It depends on the nature of the experiment.  So studies are meant to be pilot studies. Based on those results, a larger sample might be taken.  I think reporting r2 and effect sizes are very important because they more than can be determined by a p-value alone.  ","obviously inappropriate","These issues must be discussed!","This is data falsification and should never be used. ",NA
"192","It is part of the normal experimental method when exploring new research questions within an established field, as a control that you experimental system behaves as expected in accordance with current knowledge, before pursuing new ideas. /  / Ideally, it should not be used when exploring new questions, but it is close to impossible to get negative results published alone.","It shouldn't be used as it's important to mention what things were tested - the fact that they weren't significant is a finding in itself... however you don't want to clog up a sometimes already lengthy manuscript with 'non-findings'. ","It is nice to be fair with readers. ","It is very difficult to report everything. Usually the initial hypothesis is not so well formulated to include a strict definition of the model you are trying to test. I will often test more and more complicated models to test assumptions I am making and eventually pick one that is a compromise. I will explain by reasoning, but I might not show the results of all the models.","It should be used if it properly conveys the actual uncertainties associated with the assessment. E.g. p=0.01567 would not be sensible if the estimate of p was itself uncertain. /  / Or an L infinity of 110cm might more properly reflect uncertainties than Linf =107.865 /  / I don't think this kind of thing is done enough, including by me.","Outliers happen. If they exert a large influence but they may be flukes of measurement error, an unknown cause or some factor extraneous to a study (e.g., a pathological case) then they should not be included in a study. If, however, they are due to factors relevant to the research question, they should be included and the same analyses can be done including and excluding them so that the effects of a single unique event are considered but do not overwhelm a finding that holds for all other species.","It depends on what the question is - I see no problem with it for some questions (at least if stats aren't the only thing motivating the additional data collection), and of course it depends on how the data are presented.","Occasionally a strong signal in the observed data can be masked by an inappropriate statistical model; then it is necessary to change the model used to analyze the data.","These points are often discussed by reviewers.","This is dishonest, and unnecessary. It can help your study, but you really must explain which data points are genuine and which are not. I recently had a strong private disagreement with a colleague who does this routinely.",NA
"193","It is the significance or reporting bias that even has to be corrected for in metaanalyses. Non-significant studies are less submitted, because less published.","It will be Only a try and not a fastidious mean.","It is not ideal, but unfortunately because of the emphasis on hypothesis driven research, sometimes interesting results that ecologists and evolutionary biologists stumble upon are often not acceptable presented as such. ","It seems inappropriate for a study where only one independent variable is of interest, to test a specific prediction.","It should never be used if researcher is changing their standards of decimal point use (journal-specific, personal or otherwise). But of course, if it conforms to their standard (e.g. journal norm is to have 2 or 3 decimal places in P values) and the rounding happens, then absolutely fine.","Outliers may be excluded but the criteria and analyses must be included in the paper, and differences in results with and without the outliers must be clearly discussed","It depends on whether the decision to collect more data was based on the lack of statistical significance (in which case, this should never be done), or if the analysis was just of preliminary data and the additional data collection was already planned (that seems ok).","of course, this is massaging the data. On the other hand, different tests are tuned to different situations, and it is not always obvious which one will work. Again, careful reporting eliminates the misleading effect","This also happens frequently. The peer review process is not perfect and it often encourages authors to de-emphasize weakness or risks associated with a published study so that it can eventually be published.","This is dishonest, it should not be done.",NA
"194","It is the task of the researcher to identify the (meaningful) variables of interest. If meaningful variables do not reach significance, they should reported. However, not all test statistics of all variables do need to be reported.","Its use should depend on context. If the absence of the effect is irrelevant to the inferences of the paper it can be OK. You need to make this decision taking into account such matters as false positives due to type 2 errors, overfitting or multiple comparisons effects.","It is often not possible to predict what will be significant in ecology due to the high complexity of systems and the myriad factors that impact them. Determining what was significant and then constructing a balance of evidence for and against its likelihood and wider applicability is more important.","It should be used because it gives readers a chance to see the same alternative models that the author tried.","It should never be used. We should all be reporting p values to AT LEAST 2 significant figures (ex. p=0.054). If your significance threshold is 95% then you should never be rounding. I have reported results such as p=0.054 and have stated close to significance. Doing otherwise and rounding is lying.","Outliers should be independently identified. Impacts on test results should not play a role in outlier detection.","It depends on whether you have a good idea of the effect size and an appropriate sample size before you start. Some studies have problems with creating abberent results through eg freak weather conditions which are not relevant to the study. However careful consideration needs to be given as to whether this is justified or if you are just trying to get the right answer.","Often data is skewed from normal and requires experimentation with other methods.","This can bias the results and mislead the reader. ","This is dishonest.",NA
"195","It is useful to know the terms that failed to reach significant, as this brings us greater understanding of the system ","Journal space is limited.  I'd say it's fairly common to report which effects are 'significant' or have large effect size while simply mentioning that other factors considered [presumably mentioned in the methods] were less well-supported.  Some analyses have many candidate models with too many covariates to discuss individually.","It is permissible in the case where this is due to something which might have been predicted from the start, had more information been available","It should be used when relevant. No stat method is one size fits all.","It should NOT be used to make something significant, but it can be used if your decimal points go to far e.g. P = 0.00014. Is ok to make this P&lt;0.001","Outliers should be treated as such. But if the only definition of an outlier is its effect on p-value, the author is crossing a line into fabricating/editing the data set.","It introduces bias. A new study should be conducted.","Often in the first cut of analysis, I will for instance not transform data. Oh look, it's significant (or not) -- what happens when I transform -- oh looks better in terms of satisfying assumptions, residuals, and that is what drives the public report. /  / It's part of the analysis.","This crosses a line into ethical misconduct.","This is dishonest. /  / If the data is simulated, this should be flagged up and reasons given - the reviewers can decide if they are justified.",NA
"196","It is very hard to get a study with no significant results published. In terms of non-significant variables/analyses, of course it is best to report everything you tried, but journals have short word-count limits and so sometimes you don't have space to describe everything you actually did (e.g. in initial data exploration phases etc).","Just state 'only these variables were found to be significant' or similar. That implies that all others are non-significant. Not a problem.","It is the purpose of exploratory analyses. It is how research is made, finding unexpected results is common and then we try to clarify it and make sense of it. / To publish it usually it is easier to cut it short and having clear prediction even if at first it was really expected... But it depend of the result, framework and so on, when it can be avoid it is better of course","It should not be used, because a full statistical rationale should be presented. Therefore, if other models are tested, this should at least be mentioned, even if not detailed. For instance, saying that models were selected on the basis of meeting residual assumptions, in my opinion, already provides enough information regarding the exclusion of candidate models.","It should only be used at the third decimal place!","Outliers/artificial values can be excluded if there are good reasons (mislabel, erroneous measurement, etc). However, this should be done pre-analysis, not to ""massage"" results post hoc.","It is a good approach for pilot studies and conducting initial power analysis","Often people start analyzing their data with simple/quick stats, and then switch to more complex methods that might be more powerful.  However, ideally you would select the most appropriate method for your question and data, and then stick with that.","This is a main reason why 2/3 of all papers are doing more harm than benefit and should not have been published. I can't guarantee that it never happened to me","This is egregious fabrication of data! Of course it shouldn't be used.",NA
"197","It may be used when the sample size is very small / It should not be used when the sample size is large enough that a significant result would be considered of interest","Lack of significance is not evidence of lack of effect. Focusing on things that are significant within the context provides the most clarity. Meta-analyses can then look at why factors are significant in different contexts.","It is unexpected when it occurs. But in retrospect is becomes clear why it should be expected. This is good science","It should only be used if the reported models represent strict and general results and not a personal biased choice.","It should reported to the accuracy determined at the beginning of the analyses. If it's almost significant, that should be reported and discussed. The threshold is arbitrary, so there's no need to ""cheat""","P value is not everything. ","It is a poorly understood statistical gaffe to do this. It completely changes the likelihood of finding significant effects.","Often there are different ways of tacking the same issues. It should of course be used with caution and provided the correct theoretical assumptions","This is a matter of fairness, one can overdo it relativating everything, but I believe such problems in the data quality are quite often ignored or hidden. ","This is faking data. Period.",NA
"198","It provides a threshold against which datasets can be compared. However, I have used higher and lower values as well. It depends on context.","Lack of statically significant covariate is still a meaningful result","It makes the result look more convincing than it really is, which is fraudulent. /  / It is easy to admit something was from an exploratory analysis- so people should do it.","It shouldn't be used because this way you are hiding the process behind your study, and focusing only on results.","It shouldn't be necessary to use. Strict adherence to the value of significance thresholds are harmful, since they don't acknowledge the arbitrary nature of those thresholds. You should trust your readers to be able to understand the meaning of those values enough to interpret the actual value.","Personally I find it very hard to discard data points for any good reason.  I think there are justifiable reasons in some circumstances though. Again, people have to be ethical.  ","It is better to do a power analysis first to assess needed sample sizes","Okay for exploratory non-parametric","This is again maybe a more practical journal consideration. I list page after page of sources of error for every study I have ever read, but I think the reality is reporting ones that you think could have a large and significant impact on the findings. I think it ultimately up to the reader (or maybe the reviewers/editors of journals as a first filter?) to decide how many of these problems and/or data issues are acceptable.","This is falsifying data and is absolutely wrong!",NA
"199","It really depends on the situation. For example, many preliminary experiments give null results, but they're not designed to be published--they're meant to help us understand our methods and what we may be missing in our ideas or understanding of a system/phenomenon. In addition, a null result could indicate many things: potentially that an effect/relationship doesn't exist, but also potentially that an experiment wasn't designed properly. I do wish there was a centralized place to report null results, so that meta-analyses could be more precise. Sometimes getting a null result in an experiment designed to test one hypothesis leads to testing other hypotheses with the data instead, and we're often not transparent about this. ","Leaving this out doesn't give the entire picture. I think if researchers leave this out, they should provide a justification in the manuscript.","It may mislead readers","It shouldn't be used but journals and reviewers very often force for simplicity in the manuscript. Giving extra information sometimes is not well appreciate, or can be misunderstood by reviewers and editors, and unfortunately the politic of ""publish or perish"" push people to do that kind of actions to see their manuscripts accepted as fast as possible and in the best journals as possible.","It shouldn't be reasons to rounding-off results. In evolutionary studies the complexity of the subject should be accepted and results close to significant should be show like that. ","potential to bias data - make decision a priori","It is better to set up a new study","oly justifiable if there are other reasons i.e. suspected lack of power in the approach to use another one - even then should think the analyses through before not after ","This is also known as 'telling untruths'. Or, if done with the aim of deception, 'lying'.","This is falsifying data.",NA
"200","It reinforces the notion that results that aren't statistically significant aren't interesting or worth reporting. ","Let say that in observational studies the effects often can result from several correlated covariates, and theoretically most relevant can be supposed to be of primary interest. In experimental studies it should not be used, if the question is about manipulated factors.","It might be partly justified when you have not a strong belief on your initial predictions and your findings are interpreted in the view of new ecological paradigms, which you did not consider in the beginning of your research. This helps telling the story. Otherwise it should not be done.","It shouldn't because reporting only partial results of statistical tests is a way of biasing those tests. In practice, the generally low experience of biologists in the field of statistics leads them to run several tests before finding the appropriate one.","It shouldn't be used because p-values are arbitrary anyway so it doesn't really matter whether it is 0.05 or 0.054","Pretty serious IMO","It is fine to collect more data if the main factor limiting the ability to make strong inference is sample size","On occasion, and only after things aren't fitting well or producing results that seem errant, have I thought more deeply about the analysis, consulted with statisticians, etc and realized that what I was attempting to do wasn't quite right or might have been masking an effect by not providing an appropriate structure for it to present itself.  Fine line here between getting good fits and simply making things work out as you hoped they would.","This is bordering on fraud.","This is fraud and should never be done. It is okay to extrapolate, estimate, use regression or other methods but it needs to be reported.",NA
"201","It results in huge biases in knowledge - massive problem!!","Many times ecologists collect an exhaustive list of covariates, many of which turn out not to be important.  It seems less important to include the exact coefficients and p-values for these (although it should be mentioned that they were collected, so that the reader can see if multiple comparisons are an issue).","It might be something you hadn't thought of, it doesn't mean it's not supported by some theory, etc. So if you find enough background and logics behind this, why not explain it that way","It will be a good way to add the soundness of studies.","It shouldn't be used simply because it's a lie. If a given researcher doesn't believe, let's say, p&lt;0.05 is a good standard, just don't use it.","Removal of any ""outliers"" or other ""anomalous"" data points should either be done before analysis, or two analyses (one with, one without the removed points) should be reported.","It is logical to increase sample size to get more power in the statistics. However this is often not feasible ","One should use the appropriate test for the problem and data available. It should not be a 'shoe that fits' type of process. /  / I think this happens sometimes due to ignorance (i.e. the researcher may not be statistically equipped or do not have access to a statistician), but most times there will be a more nefarious motivation. However, if the data are well described, a good reviewer should pick up on this and ask for another more appropriate test. In saying that, the more complex the dataset and the problem, it may well be impossible to really get a feel for whether the analysis was appropriate for the dataset/problem.","This is clearly unethical.","This is Fraud!",NA
"202","It should be avoided and beyond noting an interesting trend for further investigation, no conclusions should be drawn from those variables that were reported in the study","misleading with regard to degrees of freedom not to report","It misrepresents what you knew ahead of time, and what your analysis was designed to test.","Let me explain my choice of ""use often"". I think everyone does 'exploratory' analyses on their data set, and most of those results are not reported. And I don't think that is such a terrible thing.  ","It shouldn't be used. People do it to make their results look ""better"" than they actually are. P values are a bit iffy in the first place, and this practice is a direct result of people scrambling to get something &lt;0.05 - regardless of what the effect size or importance of a variable is. ","Removal of data points should never be based solely on impact of statistical significance. I answered 'rarely' here because outliers might be removed while checking their effect, but this should only be done if it improves model fit greatly and the effects from before and after removal should be provided to the reader.","It is not always possible to estimate the desirable sample size at the planning stage of an experiment. Thus, there is nothing wrong in collection of additional data. Some statistical methods are specifically designed to work with two-stage or multi-stage sampling.","only if the data is boardline approptiate for the original analysis should another test be trialed. ","This is clearly unethical.","This is fraud, but I think it is hard enough doing this that it does not occur that often.",NA
"203","It should be used because the lack of significance is also a result, but infortunately it is very hard to publish such results","Model validation usually involves exploration of additional covariates that sometimes do not make it into the full methods of the manuscript (squared covariate terms, etc.). If it wasn't interesting enough for a reviewer to ask 'why didn't you test for X?', this seems like a forgivable sin. P-hacking without hypothesis driven model building as more serious. ","it muddies expectations of the field and is probably just an ego boost for the individual","listing all statistical tests are cumbersome, presenting the best models is usually sufficient","It simply is misleading: the criterion for a certain a priori p-value is not reached!","Removal of data should NEVER be done to chase statistical significance. ","It is perfectly valid to do this if P&gt;0.05 is combined with a power analysis that reveals there is insufficient power to reject Ho.","only to make sure that most appropriate stat procedures are used","This is dishonest","this is fraud.",NA
"204","It should be used sometimes, depending on its importance to the questions of interest. I have dropped auxiliary analyses that were not statistically or contextually significant to the study, especially after receiving reviews of ""overwritten"" papers where I had included exhaustive analyses. However, I have -not- omitted primary results simply because they failed to reach a threshold of significance. Doing so obfuscates results and creeps into unethical territory. /  / (By the way, I'm putting 50% when I don't know how often other evolutionary biologists engage in this practice. I don't have much context beside what I've practiced.)","Much more important than the last one--I understand this question to ask about reporting covariates that *are included in a regression* but are not associated with statistically significant effects.  The regression estimates for the remaining parameters are impossible to interpret without knowing what all covariates were included in the model!!","It must often happen that a research question is changed or refined in the light of results.   The implication of the question is that this is a questionable practice.  I take the point, but it may be difficult to distinguish it from the procedure of testing hypothesis X1.  Evidence Y1 refutes it.  Propose modified hypothesis X2, which is consistent with Y1.  Is Y1 a test of X2, exactly?  Probably not, but is the order of operations critical, if the fact is that X2 correctly predicts hitherto not correctly predicted data Y1, and has not been refuted by other tests?","Listing everything tried can be a bit uninteresting to read. / Ideally one should know beforehand the best models to test; in practise my understanding of the dataset develops during analysis and may cause a switch of models. In the end you need to report what you honestly believe to be the most suitable model. No need to take the reader on the full journey to get there. More important, if there are competing models with rather different interpretation, that should be mentioned of course / ","It would be better to report 0.054 than 0.05 but sometimes in tables space is limited and you have to round values to fewer decimal places. Since 0.05 is an arbitrary cutoff and technically 0.054 should be rounded down if you need to round it I don't think it is a huge deal, people shouldn't be interpreting 0.054 and 0.05 as very different results anyway.","Removal of outliers is a dangerous business","It is rare that I can do this because I work in the field and cannot necessarily sample more individuals. However, when a p-value is border-line and samples are small, I don't see it as a bad practice to try to increase sample size in order to reach a clearer conclusion. ","p hacking!","This is dishonest and in the long run counter-productive to the author(s) as it damages their reputation. /  / It may also undermine confidence in conclusions that are actually robust, if the problems later turn out to be unfounded or corrected. /  / It fails to point the way towards open research questions concerning method development.","This is fraud.",NA
"205","It should not be used but it will never be published, so everyone does it.","Much of research articles have a pile of data sets that have been measured and it is very common to present a variety of variables in one article, even though they do not answer / provide novel information on the precise study questions. Therefore, I would recommend selecting the variables meaningful for the research question and leave other variables even completely out of the articles - to possibly be included in other papers. This leaving out of data could be avoided by more careful study and sampling designs...","It seems just a tad dishonest.","Lying is not OK","It you are rounding off correctly, as  you presented in the example, I don't see any problem. P-values are again descriptive tools in our attempt to gain understanding. The are not design to show cause-effect relationships, which are at the center of our scientific inquire. ","Removal of outliers is always a thorny issue and there are justifications for doing so (and I have done it). However, these need to be based on a judgement of the quality of those data - usually this is the problem - such as poor readings from a machine and the fit of the model (never the p value). If outliers are excluded for reasons of model fit, this needs to be described in the Methods and Discussed.","It is risky, as the additional data may not support the first statistical tests.","People want to publish... / At the same time, sometimes you discover that what you had planned for was not the best.","This is dishonest.","This is fraud.",NA
"206","It should not be used, as it fails to inform about *all* aspects and results of a study. Current publication practices (space limitations, journals/editors lust for novelty and ""relevance"") mostly forbid exhaustive reporting non-significant results","Negatative results are still results. ","It seems like it would be more interesting if you hadn't predicted it, so it's not clear to me why someone would pretend they did predict it.","Makes no sense at all to do this","It´s inaccurateand unnecessary.","Removal of outliers is fine, but should be done based on some other criterion, not how it affects the significance of your test.","It is very reasonable to conduct pre-studies before collecting major data sets.. This is also often needed when student works (e.g. Masters Thesis) are further processed to scientific articles. ","Possibly if there is a good reason (e.g. wrong error structure) why the first analysis was suspect; but in that case, why not fit the correct model initially? I might change models if assumptions are not met, but not to meet arbitrary P thresholds. Although sometimes outlier analysis might fall into this category.","This is hard to avoid as there almost always are problems and issues with any method. There is also a big grey zone in what a researcher should reasonably be expected to know. ","This is fraud. I don't think anyone does this. ",NA
"207","It should only be used rarely if the journals would like to publishe negative results","Negative results are as important as significant ones ","It shapes the research conceptual framework a posteriori, instead of replacing the results within the framework of known state of the art.","many of us do exploratory analyses of data, not all of which make sense in the end, and not all of which","Just give the exact value then","remove outliers at the initial steps of analysis. Anything else is obfuscation of data.","It makes sense to redo an experiment/sampling with more replicates if you realize the power of your test is not sufficent to answer your question.","Presumably there is a statistical method that is best or optimal for a particular question.  If there is a change to a different technique from that originally used it should be because the new technique is actually more appropriate.","This is like falsifying data. Problems must be reported.","This is inappropriate and potentially an instance of research misconduct. ",NA
"208","It should only be used when the result is not among the core of the variables and the information is not useful to tell the story.","Negative results are important to document","It should be made clear that it was the result of an exploratory study","Many times it is necessary to do some exploratory tests on the data. Many of those test are discarded because they are inappropriate or do not correctly represent the data. Reporting every single test applied would be confusing and too long. We should however avoid reporting a viased set of models.","just report the value. P values are subjective to begin with.","Removing data outliers is fine as long as these data points can be effectively identified as outliers (real artifacts due to sampling errors or mistyping of values). However, this often is not possible, and by removing outliers we may be removing interesting data points. Although when I was learning to analyze data the removal of these data points was seen as normal among my colleagues and myself, nowadays I am more wary about that and try not to remove them unless it is unjustified. Particularly the data I generally work with is rather noisy, so I wouldn't know where to put the limit. I think a good practice is to use graphs that show individual values in addition to summary statistics. / That said, I do not think that systematic removal of extreme data points is necessarily bad (unless, of course, if this is done with the target to achieve a specific result), because if it is done properly it does not necessarily alter the conclusions extracted from the data (if the later is collected properly). This is, in fact, the basis of robust statistical methods.","It may make sense if your sampling size could be influencing the results for being too small","Problem: Only significant data can be published. ","This is not good practice because it hides unreliability in the results. I think researchers should be honest about the problems, but reviewers should not block publication unless these issues completely invalidate the results.","This is inexcusable. ",NA
"209","It shouldn't be used at all, but there is a high selection bias in scientific journals towards those studies that report statistically significant results, and that forces researchers to do some sort of filtering when reporting their results. ","Negative results are important too! If a covariate was included initially, there must have been some reason to think that it might be important. The result that it was not important is interesting!","It should be reported honestly as a result of exploration rather than an expected/predicted outcome.","Maybe not avoidable in analysis of survey. However I am award that you minimise the need correction for the test significiance ","Just use &lt; or &gt;","Removing data points is serious business, and I advise against it at all times, unless there is a very good reason for removing it (e.g. an error in the method for that data point)","It might be okay to do that and analyse all the data together if you report it and the readers lower their trust in your results accordingly. /  / It is excellent to do it if you analyse the second set of data independently, as a confirmation of your first result. That is a proper use of hypothesis-testing.","publishing pressure and other pressure probably drives this — changing approaches to find something significant","This is pretty common but no experiment is perfect, so constant self-criticism in a study wouldn't be overly helpful either.","This is inventing data. It is a fraud",NA
"210","It would be better if people always report studies where they obtained a negative result - they put the work into it and a lot of times the negative result is interesting because it rejects or challenges some debated ecological phenomenon.  However, these articles are not always 'popular' with peer-reviewers and the scientific community at large, so get put aside and that is unfortunate in my opinion.","negative results are informative","It should be used if it helps communicating the observation in a straightforward and clear way (hypothesis - test). This should not, and need not conflict with the statistical analysis (pseudo one tailed tests, ad-hoc priors etc.). So difficult to say how often it should be used / ","Model selection using information-theoretic approaches (e.g. AIC) that have model sets is more of an art than a science, with no clearly accepted approach to dealing with complex combinations of covariates that inflate model sets (>50 models).  So some shortcut is needed when model sets are large and sometimes this results in exploring covariate combinations that  are not reported.  ","Less stressed about this as exact p-values are not really that important. But wouldn't encourage it in any students as it runs against my philosophy of full transparency.","removing outliers during data pre-processing based on device malfunction or recorder error is totally legitimate. Additionally reporting subsetted data to explore whether there may be a sub-pattern to explain what you see in the whole data is also ok.  But removing data because it doesn't fit with your storyline is not cool.","It might be reliably used in some limited circumstances in which pre-planned steps are used in the analysis. I doubt many ecologists have resources to go and collect more data (in many cases), so I'd guess it is less prevalent.","Question is incomplete. One should attempt as many statistical methods as are applicable and practical, not settle on just one (and there are always more than one 'good' ways to analyze the data). /  / The question is whether researchers stop when something is significant only.","This is problematic, because most often real-world data typically brings with it more caveats than could ever be disclosed in the length of a single paper. Those caveats which have clear differential bias should always be disclosed, though it may not always be necessary to disclose caveats which relate to losses in precision without bias, or those which are generally accepted and well-known as typical of that field/methodology.","This is just cheating!",NA
"211","It would be great to have a free publishing venue for non-significant results. the problem is also identifying why they are NS. is it a good test? or did you mess up?  ","No include covariates that don't reach significant values is in general not a good practice, and kind of make up the statistics when the researcher have knowledge of the interaction of traits or variables he/she is working with.","It should be used sparingly","Most analyses in my experience are iterative -- its not always informative or useful to present every iteration of the data analyses experience, although the final set of analyses should always be accompanied by reproducible data and code","Limits accuracy of meta analyses, but journals sometimes specify the number of decimal places.","Removing outliers is sometimes justified.","it seems like this is a safe way to test a theory and then get more information to learn more (unless I'm missing something about this)","Rarely solely because it may be that an inappropriate technique was used initially or that the research were told of a more powerful technique after the initial analysis, such as being introduced to isotonic regression---in effect a one-tailed ANOVA, for ordered expectation---after results from a ""standard"" main effects ANOVA (which are always two tailed) was ""insignificant."" I doubt this happens often, and I doubt that many researchers truly fish in this way, chiefly because most have limited statistical tools in their toolbox.","This is related with the idea of relate not significance with failure. In fact, to report problems is very useful for the scientific community in order to avoid them in future studies. Again, I think that part of the problem is the acceptance of this kind of manuscripts in the scientific journals","This is just faking results",NA
"212","Journal won't publish not significant results. It would be helpful though, to other researchers that might otherwise conduct similar studies without knowing that it has already been done with no significant results. ","No opinion","It should not be used because it changes a study from exploratory to confirmatory, it's misleading to say the least. But I suspect it's often done, where results are put forward as hypothesis whichg are then confirmed / disproved based on data.","Most training and texts in statistics that I have used to understand data analysis often use ad hoc testing and model refinement; I am led to believe this is a viable way to use stats as a tool to understand data.","Loses information about what the P value is representing.","Resulting P value is biased.","It should be reported as a two stage research","really depends on the intention - improve the power of analysis - I would accept; - e.g. student uses Mann-Whitney test in thesis (p=0.07), supervisor can analyst data using e.g. Poisson distribution (p= 0.04)","This is simply bad practice. I see this commonly with 'standard' tests, where many researchers often do not seem to understand the model assumptions.  Often these tests are used in settings where the assumptions are breached.  That's ok, but only if the effects of breaking the assumptions is explored and reported.  It almost never is.","This is just wrong.",NA
"213","Journals and reviewers don't care about null results. Also they'll question whether its truly null or just underpowered. ","non-significan effects, be they co-variables or not, always deserve reporting. Disregarding in a multivariable model is one thing (parsimony rule) but not mentioning is not ok.","It should not be used because it is dishonest; it subtly bolsters belief in the finding by presenting it as a prediction instead of a discovery (which may be influenced by noise). Unfortunately, the commonness of this practice is reflective of how scientific culture has been shaped by poor statistical training. Reviewers and readers in ecology (and many other scientific fields) are largely trained in the null hypothesis statistical testing framework and are not receptive to exploratory data analysis; writers overstate their results and change the sequence in order to get published. I don't see a good solution to the broader problem, but ecology is fairly good at taking on board specific statistical critiques, so this problem may start to wane soon. ","mostly it should be used but doing so makes paper laborious","Luckily, I work with quantitative ecologists who don't let this happen, or I'm sure I would have done this before. In my opinion, it's best to present the p-value to the third decimal to avoid such short cuts","Results should be formally presented with and without outliers to confirm results are robust/resistent to influential datapoints.","It should be used more often, mostly in field experiment or field data, where results can change faster. The reinforcement of the results with extra data to confirm our results should be a most frequent practice, but it takes time, sometimes and extra year if you need to wait for the next field season, and sometimes academia don't have time for it. In other cases people is afraid that results will change with extra data... precisely for that, extra data should be good, but bad results are not as ""sexy"" as positive significant results.","Really, what should happen, is the analyses should be defined before the experiment is even run. ","This is tempting but I can't think of a justification for doing it.","this is scientific fraud.",NA
"214","Just to clarify: in my case (and in many other cases I believe) I still mean to write about these results (published in my thesis at least) but other studies got the priority as they had more potential impact... I think this practice ideally shouldn't be used the process of writing and publishing papers takes time and the pressure to publish ""high impact"" studies (especially in early career) makes other studies take the priority.","Non-significant covariates can be a distraction without changing meaning of reported results. The negative results are good for others to know about, but when space is at a premium in journals there is little alternative.","It should not be used because this is a form of outcome switching. But when writing papers, I have often received feedback from reviewers that the story does not work as an exploratory analysis, and the predictions should be included from the start. So to get these papers published, we are forced to pretend that this was the intention from the start. It is a practice that is not statistically sound (if you pretend that this is what you wanted from the start, and if you do multiple exploratory analyses, and only report the ones that showed something interesting), but that reviewers seem to want, to make a nicer story.","multiple testing without adjustig p-values","Make sense to approximate to the next value ","save time","It should be used to increase the power of the test. The power should always been informed.","Researchers should always use the best and most appropriate analysis for their type of data. There are rarely two equally good options for data analysis that will give different p-values, and the best and most appropriate analysis option should always be used (and preferably decided before the data are collected)","This is tough - people don't want to start criticising their own data/analysis because of the pressure to publish. There are no guidelines on what to report in stats in general and no-one is going to tell you to go all A-level student and start pointing out the problems with your work. So yes - it should be clear what issues might exist, but authors need support (from trainers, editors, peer-reviewers) before this could ever be expected to be common practice.","This is scientific misconduct.",NA
"215","Lack of significance can be due to various issues. Reporting a failed experiment, or an experiment which had so low a sample size that it is unlikely to find a signficant effect even if a real effect exists, does not help science. Non-significant tests should be reported, but only when they have sufficient rigour, sample size and estimated power that the negative result is convincing.","Not desirable but sometimes one has checked that no inferences are affected for the covariates of interest and there is a word limit to deal with","It should not be used because we all understand that the scientific process is very unpredictable and that is itself exciting. I believe the justification that is used for this is often that it makes the paper more of a ""story"" or ""flow"" better, but it really has been counter productive in this regard. Another problem of our own creation.","Multiple tests are almost always done and reporting the entire process would make articles difficult to read and write.","Makes little difference to conclusions","See earlier responses.","It should be used whenever it is possible to get additional data...there is nothing a priori wrong about using more data if there is an opportunity to do so.  ","See earlier responses.","This is very hard to estimate how frequent, but definitely the most killing action of good research. This is scientific misconduct and should be absolutely banished.","This is simply making up data - obviously very bad practice!",NA
"216","Leads to systematic bias overall. ","Not reporting a negative result is depriving the scientific community of evidence for a lack of effect. It is an important result, but, unfortunately, it is not a result that appeals to general audiences, usually being devalued by reviewers and editors, increasing the chances of a study to be rejected for publication. Some situations where I have used this practice, and when I believe it is not a problem, is at an exploratory stage, where one has more than one proxy for a specific concept which I view simply as a methodological adjustment for such proxy.","It should not be used since it will give different weight if it was expected beforehand. When unexpected, it is clear that it should be checked before believed that it was something else than co-incidence.","Multiple tests invalidate p-values. Its fine if you report it as exploratory or acknowledge all the tests (at which point reviewers will ask for multiple comparisons corrections).","Manipulation of data is inexcusable. You can always report the results as ""almost significant""","Seems rather dodgy to hold out some of your dataset to achieve a result or vice-versa.","It should never be used because if the study was well designed then you may waste your time and effort looking for something that is not going to happen","See previous answers on other types of p-hacking.","this is what I said two question behind...","This is simply making up data and pretending it was actually observed",NA
"217","Like with the last question, sometimes there is only time or space to write up a given number of topics in a manuscript.","Not reporting all covariates that were used within a single model misrepresents the results of that model. Insignificant covariates within a model are as powerful at explaining to outcome as significant ones.","It shouldn't be used because it is an editorial practice rather than a scientific test. ","My feeling is that a bunch of investigative stuff is done. I mean what about the ""conceptual modeling"" i.e., thinking about what's going on? should we report all of that, too? /  / A qualifying statement that these are thr best models -- we looked at others and discarded them....","Many journals only allow p values to be presented to a certain number of decimal places. Therefore whenever I see a p-value of 0.05, I automatically presume that it could be up to 0.054 and would hope that other scientists would do the same. As p = 0.05 is only on the threshold of significance anyway I would not count it as a particularly strong result. ","Shouldn't be done unless you're trying to get fired.","It would be nice to have the resources to collect more data, usually this is not an option. ","See previous answers.","This make reproducing the results almost impossible and should be avoided at all costs.  Having struggled to recreate results from previous analyses by other workers I find this most annoying.","This is straight-up deception.",NA
"218","Many preliminary studies have extremely low statistical power, or it becomes clear that there are methodological issues that cause the results to be unreliable. These studies are highly inconclusive - it serves no purpose, even from the point of view of meta-analysis, to publish them.","Not reporting covariates that fail to reach statistical significance misrepresents studies.","It shouldn't, but because we are not machines unexpected results get you thinking, and get less and less unexpected as you work on. Eventually, there's support from more than one angle and you start believing in it.","My guess is most researchers do this as 'exploratory' analysis and don't think that these 'exploratory' models need to be reported.  But really this shouldn't be done because it is clearly an unintentional form of p-hacking. ","Mispractice. You must always report the true p value obtained.","Simply report the outliers, and present the results with and without. ","Its a bias, yes. But the nature of ecological experiments often results in small samples sizes per cohort/block. If one sees a possible effect size, and can demonstrate that that effect size would satisfy a significance threshold if it remained robust with more replicates, then I think this is justifiable. Blinding is essential for further data collection.","Seems reasonable if you're asking a slightly different question, or if it helps you realise you're doing the wrong thing.","This may be the most important comment I give when reviewing papers. Everything that can impact conclusion should be discussed.","This is terrible. ",NA
"219","Negative result is an important finding. But papers reporting only negative results should have low impact, which makes me feel it is not cost-effective to write the paper reporting the result. This can cause publication bias, which is a serious problem.","not sure what the difference between this and the previous question is","It sometimes goes better with the flow of the writing. I honestly don't care about this one.","Nearly all data (especially data from experiments that have multiple factors and their interactions) can be tested in several ways (which usually all give more or less the same end result). Of course you can try what ever you want with your data, as in the end it leads to most meaningful way to represent you results.","Most statistics are rounded. Rounding correctly and using an equal sign (but not a ""&lt;"" !) is not incorrect. However, providing only one decimal place in the example is misleading.","Slippery slope.  Shouldn't do it. In my case, the only time I did this is because it greatly influenced the form of the function at the upper limit of the range of the data.  Upon, inspecting these points more closely (data were not collected by me), I realized that these, in fact, differed from all other data points in how they were collected (response wasn't counted, rather estimated because there were too many to count).","Its worth knowing how promising a hypothesis is before investing too much in it","Shopping around until one finds the ""desired"" result. I guess this is less common becasue ecologists often have a somewhat limited selection of statistical methods available to them.","this maybe has short-term benefit for an individual, but really hurts science as a whole","This is total unethical and should be always avoided.",NA
"220","Negative results are also interesting and important and should be published/reported. Meta-analyses and reviews will be biased if only significant results are published.","Not sure: what if you did not see that there was a covariate, because you did not even think of testing it?","It sometimes happens, that you have collected data originally for some other purposes and find out later (i.e. you learn something new on your way!) that with that data you can answer to many other questions as well. I don't understand why you should not then utilize the data and write a paper with proper hypothesis (i.e. those that you ""learned"" while you did your research)?","need to qualify this - you should not run loads of analyses and then cherry pick which ones are interesting (at least not in a frequentist hypothesis testing sense), but considering alternative formulations of complex statistical model when the aim is to optimise fit or generate predictions (as opposed to finding P values) is essential.  /  / So whether it should/shouldn't be used depends on context, bets practices for effect size estimation and model prediction vs ""finding a  P value"" can be different  ","My opinion is that p-values are a gradient of strength of evidence, rather than as clear cutoffs. So I as much difference between P=0.051 and P=0.049. ","So long as both results are presented, this is not a problem. In some cases researchers might be concerned with whether or not a single datapoint or small number of datapoints are driving a pattern (or causing a lack of pattern). If the two sets of results are both reported, and the procedure for including or excluding the datapoints made clear, then readers can make their own minds up about the patterns. If the datapoints excluded are not mentioned at all in the papers, then this is clearly very bad, and should not be done.","Larger sample sizes are always better, and in some cases make the P value increase.  What would be wrong is to stop collecting data when p reaches 0.05","Should almost never use null hypothesis testing of any kind. Science is about measuring the actual magnitudes of meaningful parameters, not yes/no questions about an imaginary null hypothesis that can always be falsified if we are willing to throw enough sampling effort at it.","This often isn't done in a vindictive manner, but in short word count papers with large multisite studies you tend to go into the individual wrinkles of each site.  There is normally one site in one year that missed the target cut date for a grassland by a week or something like that.  In the grand scale of things minor normal stuff, but on the whole I would not expect most people to write all of that in a paper.  Major screw ups should be mentioned, but the rest ends up as unexplained variance.","This is very bad practise, since it misrepresents the results",NA
"221","Negative results are important and should be reported. Only non relevant data should be omitted.","Not used because this is essentially fishing for results. You will get high false positive error rates.","it violates the scientific method AND it leads to boring papers ( no surprises)","No opinion","no idea how many times occurs but I believe this is close to unethical behavior. close to threshold significant values are in practice unsupported ideas","Some outliers can be explained and may therefore be excluded.","Less so perhaps in ecology, but there are occasions still where resources demand small sample sizes that only justify follow up if the results look promising. The question makes this sound more insidious than it is pragmatic.","Should be used only if the initial analysis is not appropriate (but then it should not have been attempted, I guess!)","This practice is a bit more ambiguous than the previous ones, so it is harder to condemn it outright. In particular, ""problems with the data quality"" is vague.","This might happen in modeling studies, but I think most researchers understand the importance of clearly indicating simulated vs. observed data.",NA
"222","Negative results are just as important as significant results","Not using this practice enables to have a better idea of the number of variables tested for and would be very helpful for meta-analytical approaches.","It will be a promising way.","Not a great practice, but inevitable if one is exploring a complex field dataset","no loss of information since it is really just an estimate","Sometimes it makes sense to exclude an influential outlier or two from an analysis - with full disclosure that this was done in the manuscript.  Sometimes characterizing a population is most informative if you're not trying to also explain a few odd-ball data points with the same model assuming central tendency.","Limited statistical power is a perpetual issue in ecology and evolution, and lack of power increases the frequency of erroneous conclusions in the literature. Increasing sample sizes after analyzing results from a pilot experiment or mid-way through data collection (e.g. with a power analysis) can allow researchers to select a more appropriate sample size that will improve their power, and thereby increase the reliability of their results.","should be used when the assumptions of the test were not met","This practice is dishonest and borders on research misconduct. it should never be used (though I fear it is reasonably common). ","This should get you fired",NA
"223","Negative results are often useful, and sometimes it can be useful to report a dead end to keep someone else from wasting time on it.  However, most of us do a fair bit of exploratory data analysis and preliminary tests. In many cases it does not (due to significance tests and any number of other factors) lead us where we want to go, so we don't try to publish the results. There are already too many worthwhile papers to read, and too many poor ones published--we should not encourage publication of each & every test & exploration!","Null results just muddy the model. Also, they'll question whether its truly non-significant or just underpowered. ","It would require also to report on the what or why you were exploring, which is not necessarily useful information. Also it could make the introduction and discussion section less to the point.","Not all candidate models are adapted to all datasets, so it can happen sometimes, but should be argumented","no need to do this, as p&lt;0.05 is an arbitrary value anyway... And people slowly realize that the p-value doesn't really matter all that much.","sometimes one learns that statistical significance may be due to outliers. Excluding these may give more reliable results.","Many data are rather hard to collect and we are used, especially in field work and in experiments with animals, to conduct small-scale data collection and, if these trials are conclusive (ie if the protocol seems feasible at a larger scale and / or if small datasets already show some trends), to expand the study to gather sufficient data for appropriate statistical analysis, or to include more treatment modalities.","Should decide on statistical analysis before examining results.","This practice should never be used as it is contrary to the scientific method and can account for the range of variation seen across some types of scientific studies, e.g. behavioural ecology in particular. It has led to this field having a bad reputation across the sciences. ","This would be data forging. ",NA
"224","Negative results are still important information to the field.  However, often it is difficult to get this type of result published. ","often used to control the response variable, I see no sense in not reporting that we try to account for covariables, unless the quality of the data is not enough good","its dishonest","Not all models are appropriate. Realistically, researchers must always choose what parts of the research process make it into a paper. If the paper is not on development of a model, it may not be needed to report other models.","No opinion. I'm not super attached to p-values","Sometimes these points car be error but this need to be checked really carefully. Points need to be removed only for a good biological reason.","Many studies are under-powered. I have collected more data when a marginal result has been obtained to determine one way or the other.","Should not be done as a reaction to P value, but as written this question does not say that. Sometimes one realizes that a GLM does not fit well, changes distribution and link, gets a better fit, P value changes. Nothing wrong there, just a consequence of testing GOF","This practice should not be used simply because it is fraudulous... If scientifics loose their reputation because of such practices, then science looses it reputation too... Then we would all be as reliable and accountable as astrologists...","This would be fraudulent. / ",NA
"225","Negative results are still relevant results","Omit covariates hiddes important information","its not honest.  i think it gets used as it makes a more concise story some times (and the obvious no one likes to be wrong).  but its not the worst offense, just a bit unneccesarily ego-boosting","Not being honest about the full extent of tests is misleading and undermines credibility. There is no need to present all the results, but we should describe in broad terms what has been tested.","Not a big deal--rounding is rounding, and significance levels are arbitrary anyway. I chose ""occasionally"" because, though I cannot recall doing it, I may well have. ","Sometimes, outliers skew your data and may not reflect the statistical norm.","maybe if ethics/permits didnt allow you to use more animals - then you get more permits later?","Should only be used if other test is valid. Preferentially, both test results should be reported.","this seems ethically inappropriate","This would be misguiding and constitute in my opinion fabrication of fake data. As I mentioned before the data must be presented non-manipulated to let the results be re-analysed by others",NA
"226","Negative results are still results that can in some cases help advance the field. ","One should state that one tested for them and might consider to exclude them from the final model.","Journals almost require it, but it makes nonsense of 'a priori' hypothesis testing. ","Not reporting all models considered bogs down researchers in debates about which model is best rather than focusing on ecological relevance or outcomes. / ","Nothing bothers me more than seeing extreme precision in estimates. It is also preferable to reader to not be interrupted by 3498459856 decimals every time there is a statistic reported in the text","That's bad practice and misleading - there might be reasons to exclude data points BUT the p value is certainly not a reason at all","More data = more statistical power. Generally can't go back though.","Should only be used when the newer model is a better fitting model for the data, as assessed via model comparison techniques (i.e. not by comparing p-values of variables of interest). Otherwise, it is bad practise","This seems like fraud.  I see this as different than describing problems and then using an approximate statistical test.  I guess that is disclosure.  ","This would be misleading. This should never be done.",NA
"227","Negative results can also advance science and should be reported.","Only covariates that were relevant to the hypothesis of interest should be explained the main text. However, I do think that all covariates should be explained in the supplementary material.","Just use question or objective based design! I do not understand why there must be an explicitly based hypothesis. Studies which attempt to answer a question one way or another are much more useful. Also, if your hypothesis was correct, who cares? The results are what is interesting.","Not sure about the phrasing of c above (a neutral option would be better, in 'it doesn't matter if its done'), my belief is that in practice very rarely is the first model you try the one you end up with.  For various reasons you think of every other parameters, you reconsider you random effects structure, you alter your error term to reflect residual plots.  If you reported every model you ever did a paper would be fucking awful to read.  ","Number of significant figures of statistical outputs should be specified a priori, consistent throughout a paper (or even set by the journal guidelines)","That is scientific fraud","More data can help to more clearly confirm (or reject) a relationship.  The collection of additional datra should be reported.","Similar answer to some of the previous approaches. The result of the first analysis can lead you to re-appraise whether it was the best analysis after all. You need a bit of an internal honesty check though before you take the next step...","This should not be done because it is dishonest.","this would be serious cheating/fabricating data if that was done!",NA
"228","Negative results can be interesting sometimes. Researchers are certainly able to decide when it is the case. They should also be able to publish them more easily.","Only where a researcher may have done some initial, exploratory analyses and looked at lots and lots of potential explanatory variables, many of which may be correlated anyway (multicollinearity), and where it would make no sense to report literally all variables that were explored early on. Statistical significance PER SE should not be the reason to drop some in the final reporting (which necessarily may have to be brief), but a combination of it with common sense. Sometimes you can't  include everything - the reader does not necessarily have to see all test results carried out over a study's lifespan. A balance has to be struck between what you include, and don't include, in the final version of a paper.","Let's face it, science is messy and if you never reframed a manuscript following a confusing and contradictory analysis, you're probably missing the story in at least SOME of your data. ","Not sure if I'm interpreting this correctly - I will often fit a range of models of different forms as I'm exploring the data. But reporting only a subset of nested models as if it is the complete set seems wrong to me.","Obviously rounding down 0.054 to 0.05 then reporting as significant would be sloppy.","That is unfair","More data is generally good - just that my data doesn't tend to be analysed until all field work is complete, so I can't do this. ","Similar to determining a priori the hypotheses to be tested, the analytical tool (i.e., software, package, test) should be determined at the planning stage of a project. Even if the test has to be learned during the process, it is something that definitely should be decided before implementing and checking the results. ","This should not be used, but there is space limit in everything, and journals want happy faces in their twitter accounts, not sad faces.","This would decrease the reproducibility and the reliability of the conclusions.",NA
"229","Negative results can be just as informative as their positive counterparts. It is important to thoroughly report the results of all tests performed, though I imagine this doesn't happen frequently, if only for brevity's sake.","p values for covariates in model outputs are not very meaningful. The presented relationship between the dependent and independent variables is wrong if not all variables are provided","Making sense of ecological data is a messy process, and I thus believe that exploratory data analysis is very important. I think that the research process should be clearly reported, but believe that authors are sometimes punished by reviewers for such honesty - in following academic writing conventions, we all construct ""clean"" stories of our research process and findings.","Not sure what 'set' would be. I've defined it as any rejected model, even through model selection.","Occasionaly (in the past, not recently) journals restricted reported p-values to what used to be tabled values (0.05, 0.01, 0.005, 0.001, etc). Journals would round at copy-editing. With modern computer generated p-values, it should never happen. It is a hold-over from the days (30 years ago?) where you looked up p-values. Journals are just slow to change. ","That should not be the criterion for removing a data point.  It amounts to biasing the data set to get a desired result, corrupting any interpretation.","More data should be collected to strengthen the conclusions. Given time and financial constraints, some experiments have to be focused on important variables based on preliminary work.","So long as the second model is theoretically a better model of the process I don't see a problem. In the review process reviewers often point out better ways to analyze the data that the researcher didn't initially of. ","This sort of information should always be disclosed, it affects the impact of the results","totally dishonest and faking!",NA
"230","negative results can be very informative, but only when the negative result has a reasonable explanation other than poor design/low power. nevertheless, poor design/low power is something that should be reported in scientific papers to help other researchers in the future know how to design better studies. ","Papers cannot be long lists of unimportant variables. Sometimes, to focus the story, unimportant pieces of the story need to be left behind.","Many studies in Ecology are exploratory. This practice helps to make the text easier to follow, although I agree that this may introduce some bias.","Obviously best practice is to report all tested models.  However, I feel that there are a few situations where it is acceptable to omit some models to avoid cumbersome reporting: / --When a previously unappreciated line of a priori reasoning makes it clear that some models aren't reasonable (e.g. a colleague provides friendly peer review and points out that the candidate set included models with interaction terms and no main effects) / --When initial simple models are run primarily to ensure code is working properly, prior to running more complicated models / --When simplistic models are run for quality control purposes--i.e. to ensure that the data don't say anything ridiculous--before running a set of more realistic, complex models.","Of course it should not! P-values are supposed to be inferior to a certain threshold. ","The best thing to do is inspect your data ahead of time and look for outliers. Check if there are any reasons why those points could be legitimately excluded (e.g., forgot to water that pot).  /  / That is when I discard points, if I do.  /  / I have reported results with and without unusual points in which the unusual point removed significance. ","More data would almost always be better.  Most grad students are required to collect ""preliminary data"" thus, their will alway go back to get more if p &lt; 0.05.  I think is may is especially important to collect more data when p &gt; 0.05 in a preliminary (i.e., low power) situation.  ","Some analyses are not suitable for given datasets but this is recognised late, possibly only after visualisation.","This strikes me as purely dishonest, and is distinct from the other questions in that there is no defensible reason other than self promotion that you could do this. I've listed long tallies of problems and known that it would knock it ""down"" a journal, but that's fine by me. ","totally fraudulent",NA
"231","Negative results should be considered as important as positive ones. Publication pressures however limit the interest for a researcher to try to publish negative results (i.e. studies that fail to reach statistical significance), as those are much more difficult to publish.","people should always state which variables were initially included.","Many times we find unespected results both in the field and in experiments. My question is how to hide them if they deserve been shown? If they are put within a theoretical context, I see no damage in reporting and discussing them. Sometimes it is very easy to report that ""after sampling xxx, we noticed that yyy occurred and then, we tested the hypothesis that www"". However, other times it is difficult to share a work in what we had predicted in advance and in what appeared after starting the work. In this situation, I think that hiring the new (and unpredicted) findings would be a negative action. This, for me, is different from changing the hypothesis after having the results, what is not an ethical practice.","often one learns that some of the models tested are theoretically nonsense","One problem is it is dishonest, and the other issue is that those p-value thresholds are arbitrary and should not be given so much importance.  / For percent who do this, I am guessing.","The decision to exclude data points should be based solely on whether they are outliers. If statistical significance is checked first, but other methods are used to identify and remove outliers, that is fine - but removal should not be based on significance (or goodness-of-fit) alone. ","More data, can increase anyway the robustness of the analysis.","Some statistical tests are more powerful to find differences between tested variables. Generally you should initially choose ""the best"" for your data.","This would be a case of deception","totally unethical",NA
"232","Negative results should be published (although I realise that this is often easier said than done - our culture around this also needs to change); it is also vital to report effects whether significant or not. All of these results help to move science forward, even if they don't appear to be as 'exciting' as statistically significant effects...","Prevents meta analyses. At least report them in the supplementary material.","Medawar said that a scientific paper is the world's biggest lie for this reason","Often researchers try to find a model that best fits the data.  Different models (e.g. Using different distributions for certain variables) can be considered data exploration, and it isn't possible to publish all of the possible permutations that were tried at one point or another, especially with large complicated models.  Note- I'm not talking about p hacking here.","One should always round to the nearest significant figure. So it is true that 0.054 = 0.05 if one is consistently only reporting to two significant figures. Our choice of 5% as a threshold is somewhat arbitrary anyway, so we should all understand that a p-value near the threshold should be seen with caution anyway. Changing the number of significant figures used to report a value is the dishonest principal here, not the rounding itself.","The decision to exclude points should be based on residual outliers and meeting model assumptions, not on p values.","Most researchers are not aware that increasing sample size to get a low p value is a bad idea. Much of that discussion is in the medical literature but it is rarely taught to ecologists. ","Some statistics can be just as grey as ecology, so there may not be a consensus on which statistical approach is most appropriate for the dataset.  If these different statistical approaches are explored and one yields a ""significant"" result while the others do not, it seems pretty clear that the scientist will use the model yielding significance if there's no consensus that the other models are technically more valid.  It is up to the scientist to disclose that the model they used was only ""weakly statistically significant"" to at least flag with the reader that the finding is quite tentative.","This would impede scientific progress","Transparency is important so that results can be appropriately interpreted by readers and reviewers.",NA
"233","neither scientifically nor ethically justified","Publication biais. However, all tested covariances cannot be published .... ","misleading","Often the full set of models is vast and impossible and uninformative to report","One should understand that there is always uncertainties in experiments/observed data sets. Therefore, we should be cautious how to present the results.  / If you have p-value of either 0.049, 0.05 or 0.054, which one is ""more significant""? I think there isn't any differences. The result indicates that there might be some pattern in the data, but additional data/research is needed.","the only exception is for a valid exclusion reason linked to metrics of leverage or something like that. And that should be done without any view of the final result.  To remove data points to get a significant effect is very dodgy.","My feelings on this are mixed. Collecting more data in order to get lower p-values only makes sense if they have good reasons to think that either they lack statistical power, or their data sample is biased. Both situations may justify collecting more data. It is definitely preferable to collect the right amount of data with the right sampling design in the first place. But when this is not the case, it might be better to collect additional data rather than publishing an analysis that is based on a data sample that is simply not appropriate. It would be good perhaps to encourage power analysis before data collection, as this might be a way to avoid such situations.","Sometimes a more appropriate test that was unknown at the start produces a more interesting result. But this can go in the other direction as well: a more appropriate test can produce a non-significant result, and this is just as important to report (and I have done so)","Those problems should be always discusses and disclosed.","Transparency is important.",NA
"234","No result is also a result. Silencing non-significant results leads to publication bias.","Question a little unclear - is this a single covariate in a larger model, or omitting to mention an entire model because all variables are non-significant?","Most findings are difficult to predict in advance and it is only doing a lot of exploratory research from where they can be detected. Communicating those findings is another story because while the process of doing science is somewhat unpredictable, the coherence of the story to communicate it should be more linear to facilitate the reader the understanding","Often you don't know whether you have the right model spec until you run it and check the output of the model (i.e. is the model performing the way it should). I think in this case, it would not be helpful to report all the prior analyses that were performed before the appropriate model was found.","only when editor or reviewer ask that","The only exception to never is when there is a highly-influential outlier, and it is questionable as a data point, report including the point and then with it excluded + an explanation, so caveat emptor. ","Needs a special statistical correction (as with Bonferroni). Not for people without statistical training","Sometimes a quick-and-dirty analysis is done on aggregated data (e.g. grand means) rather than the raw data, to get some quick insight into the main patterns. In such cases it is justified to use a more powerful method later on (e.g. a linear mixed-effects model on the raw data, with 'group' as random effect). In principle, results are 'robust' only when various statistical methods lead to the same conclusion - when different methods give different results, then they are not robust and should not be trusted. Also, when graphs are produced, they should already reveal the key patterns when examined by eye.","Transparency is incredibly important, and requires full disclosure of limitations, flaws, or any other relevant features of the methods and analysis relevant to the interpretation of the findings.","Undermines the very basis of science.  False data eliminate the validity of any study.",NA
"235","Nobody reports all their analyses, especially when we fail to find effects in underpowered studies.  Of course, for purposes of meta-analysis, it would be better if we did all report this stuff.","question is unclear. It's usually good to mention that some covariate were included, to control for some potential confounding effects. What would be the point to deliberately fail to mention those covariates if they were considered? Perhaps for saving space (on which there is strong pressure in most journals) or avoid including secondary or distractful information","My own experience with this was one of expediency: I attempted to lay out the actual chain of events, the serendipity, if you will, but met with a good deal of resistance from certain reviewers who did not (at all) look favorably upon a manuscript that did not follow traditional narrative flow for a scientific publication. I ""solved"" the problem by restructuring to fit narrative expectations and bitched about it pretty much the whole time. But I did it.","One should never claim something that isn't true.","P-thresholds are arbitrary.","The only reason I'd understand this practice is if the data in question are outliers for which a clear reason for their exclusion is provided.","Needs additional tests such as power analyses to validate this practice","Sometimes doing a preliminary analyses points out a reminder that a non-parametric analysis / may be more appropriate.  This first step is a way that points that out.","Transparency is the key to scientific progress - give readers all relevant information so they can make their own minds up.","Very bad.",NA
"236","Non-significance is usually as important as significance so it should generally be reported.","Question slightly unclear - if methods state a model simplification approach is being used then presenting only final model (ie with non sig covariates dropped) is OK, however I think you should report all parameters from the models used to draw conclusions. Supp materials make this easy to do even in journals with tight space limitations ","nearly always, the experimental design isn't as it would be if the unexpected result had been in mind from the start. ","One should report what was actually done.  There is a large grey zone here though, as the complete set is not necessarily well defined. ","P-value are very approximate.  They are themselve statistical sampling estimates.  Rarely would &gt; 3 significant digits be justified.  The real difference between p = 0.055 and p = 0.045 is practically nothing.","The only time one can justify excluding an ""outlier"" is when one has good reason to suspect that there was either a measurement error or a data entry error (i.e. a typo). ","Neutral about this; I know you are concerned with fraud, but researchers are driven by curiosity and this includes their findings.","Sometimes it makes sense to use a more powerful statistical test (that might also make more assumptions).","Transparency moves us forward when it comes to this issue. Subsequent experiments can check that results are robust to the methodological issues of previous experiments, but only if they know what to improve. These should be reported and discussed as caveats. ","Very dodgy",NA
"237","Non-significant findings are equally important and valuable. I have published papers on non-signif findings. I was not the lead author on the study I was involved that had non-signif results and was not published. The lead author wasn't interested in publishing the work.","Reporting both statistical significance and non-significance can enhance the robustness of the analysis.","No harm in that. This is more a marketing question. People may think they can ""sell"" their data better if they predict the result beforehand. In my opinion, this is certainly not true. / Generally, it is the data and repeatability that counts. Tested hypotheses, however, should always be formulated before the experiments/data acquisition.","Only in the case of mistakes, like overlooking a necessary transformation and having to redo the analysis. Or if there are whole questions you just decide weren't worth asking, so they and their models disappear. But for a given question, include all models.","P-value cut-offs are arbitrary anyway.","The practice of removing outliers is common in ecology. However, this should be carried out independently of tests of significance (although a byproduct of outlier removal is likely to increase instances of significance in some cases).","No easy answer here; some exploratory data might suggest more sampling might help.  I don't find it disreputable.  Most investigations don't have the luxury of collecting more data.","Sometimes one needs to test robustness of a ""negative"" result by checking if results remain non-significant when analysed differently.","Used once on a study where my co-authors refused to add such caveats","Who does this? This is essentially ""faking"" data!",NA
"238","Non-significant results are results anyway!","Reporting of non-significant results is important to show where associations do not occur and to inform other researchers. It becomes difficult to publish however. In cases where I have not reported them they have typically been ""non-core"" results that don't affect the primary findings and have been left out for considerations such as space. ","not necessary.","Only should be done in cases where there is a good exclusionary reason for the model and explaining the whole thing would be a huge distraction from the point of the research report. Again, this turns on the fact that performing science is both investigation and communication. Although we would all like to report everything we do in an investigation, no one would read it and the science we perform would become useless. So there is always a judgement call regarding whether it is important to report every twist and turn along the way to a result.","P-value thresholds are arbitrary anyway, but rounding specifically so that your result is significant means effectively changing your critical significance value (in the example above from 0.05 to 0.054).","The question is too simple. / Phrased differently: How often did you go back to lab/field book to check the quality of a specific data point after finding it to be an outlier? I guess that is fairly common. / I have excluded data points (in addition to keeping them in) when the plot showed me that there may be something wrong. I then found in the lab book that they were all from the same batch of tannin standards, which I overcooked in one of several trials. Thus, I find it defensible to exclude those data points. I reported both results and gave the reason for excluding them. The referees found that unnecessary: ""If you have produced data that are technically not trustworthy, simply delete them.""","Not great, but in a world of limited resources, perhaps understandable? Unsure about this.","sometimes one test is not significace while other test could be significance.","used to simply not have space to discuss methods in so much detail, but now that supp info is so common there's little excuse for this - ","Why this should not be used... Because this is not Science",NA
"239","Non-significant results are sometimes as important as significant ones. ","Reporting the ""statistical significance"" of individual co-variants in a multiple regression is bad practice generally.","Not only is it wrong, but it is makes the research potentially more interesting if an unexpected result is found. ","Open and honest and transparent is the way to go.","P-values are a quantitative entity, so the existence of 'thresholds' to begin with is somewhat silly. Engaging in convenient rounding to achieve arbitrary thresholds is therefore doubly silly.","the risk of introducing bias is too great","Not sure I interpret the question correctly. / ...If it was meant to imply that you first check whether it is worthwhile to get more data because it's likely going to be significant, then I think it's ok because as outlined before, you have a better chance to get the study published. / ...If it was meant to imply that after initial sampling the study is borderline significant and larger sample size might get it to become significant, I think it's ok too, because that stupid &lt;0.05 will make your life easier come publishing time","Sometimes statistical techniques lack power to detect actual relationships in particular study cases. It is fair to change to different techniques to assess the data.","We are in the discovery/truth business. It's ok for different researchers to differ in how they interpret the data, but the data itself has to be solid. No excuse for sloppy work","Why would anybody ever do that?",NA
"240","Non-significant results are still results - not publishing them contributes to publication bias.","Researchers may be limited by space or length of manuscript, but shouldn't misrepresent the results. Not every aspect of the analysis can be included, but results shouldn't excluded to mislead the reader. For example if temperature did not explain some response variable, it could be excluded as long as the bigger picture of the results has nothing to do with temperature.","Not really a prediction after the facts, but most people write the introduction only after having the results so even unconsciously ends up being a common practice.","openness about analysis is important. ","p-values are on their way out; seeking significance at all cost is bad practice","The simple statement here implies that the only reason for excluding the data was in order to achieve statistical significance. But (as with the question about changing the analysis), there could be other reasons for excluding data that just happened to be discovered after the first, n-s, analysis had been performed.","Obviously this is bad practice since conclusions are drawn from small sample sizes which affect whether or not more data will be collected. /  / In a more formal sense it reminds of sequential sampling - collecting data until a confidence interval has the desired width.","Sometimes the a priori choice of statistical analysis was wrong and after consulting with colleagues or statisticians they propose another one that is more appropriate. Note that I am not suggesting to explore all kind of analysis and choose the one that support your initial predictions. ","We are not statisticians we are ecologists, there are known problems with everything and an ecological paper should not be a textbook of statistical properties of common tests. If results hinge on a flaw in an analysis than that particular analysis should not be used. But discuss the evil that is p value (or AIC) for example when I am writing a paper on animal evolution? if these issues are well known? what's the point?","Will bias results ",NA
"241","Non-significant results can still be interesting and informative and therefore should be reported. This is easier to report within a manuscript that has other significant results","Same as above; reporting only a portion of the results is not telling the full story.","Not sure if understand this question correctly","People sometimes make poor initial judgments about a proper model and then change their mind about what constitutes a sensible model.","p-values are useful inferential tools. But statistical significance should not be conflated with a meaningful relationship in the real world. P-hacking is widespread and so the reliance on only p-values in a manuscript should be questioned. Let me see the data. Even if the relationship is not statistically significant, the association (or lack thereof) may be meaningful to discuss","There are circumstances in which it is acceptable to exclude certain data points (e.g. outlier analysis, experimental error, etc.). However, this should be done before calculating p values to ensure the author is excluding the point for reasons besides whether or not it conforms to their desired result.","Occasionally the sample size is smaller than expected for some unknown reason - for example DNA sexing reveals fewer of one sex.","Sometimes the process of analysing data shows that the approach you though to use is not robust, for example switching from fitting a linear to non-linear line.","We can have no confidence in scientific results with this practice.  It undercuts the validity of any study.","Without admitting you made up data? Nuff said.",NA
"242","Non-significant results will not be published. It is not a question of researchers choosing to report or not report n.s. results. Editors will not accept them for publication.","Same as before. Often a lot of surplus data is collected and tested, but not used because it is not integral to the story being told.","Not sure that `should` is the right word. Articles have a narrative structure, they are `framed` in a certain way using normative scientific conventions. Telling a convincing story requires that we frame our science in terms of hypotheses tested - this is also weakening as big data epistemology becomes more widespread and, consequently, exploratory (correlative) studies gain credibility. ","people use several analysis during data exploration. ","p-values carry little information in the first place. Rounding off to two digits is fine.","There are in fact methods used to exclude outliers, and if this is made explicit, and results are given both with and without the outliers, it is ok.  But outliers can be a sign of sloppy work.","OK for pilot studies.....","Sometimes there is need to re-think the analyses but not for the reason stated in the question. Rather, it may turn out that assumptions are not met with the data available, and that would naturally be a valid reason to change analysis. But the reason stated in the questions indicates that hypothesis are not clear initially whicjhis not good.","We don't often have a chance to be reflexive when writing/publishing manuscripts","Woah! I didn't know this was a thing that happened.",NA
"243","Non-significant values are also interesting and not everything in research is just p-values or hypothesis testing","Same as for first question. Performing science is a combination of adept science and adept communication. It is inefficient and unreadable in some circumstances to list, discuss, and describe all covariates that are not significant, especially when they are explored during a data exploration phase of a study. On the other hand, if there was a distinct and coherent module of covariates that were tested, dropping some but not all from reporting seems inappropriate.","Obviously one shouldn't lie about things in a scientific paper!","Potential inflation of TYPE 1 errors.","P-values doesn't matter that much. Such practices exist because some reviewers can argue e.g. that 0.052 is not significant while 0.05 is. This is the only time I did this. After being rejected twice because of this 0.052, I rounded it and didn't this comment anymore (and got the paper accepted)...","There are legitimate reasons to exclude outliers but it should not be done as a method to improve significance. If post-test removal of outliers is done it must be reported and both results published.","Once again, the fundamental problem here is significance testing. ","sometimes you don't start out with the right model!","We just need to be open about study limitations and let people see warts an all. This is a problem re high impact journals where honest caveats will get you rejected","would constitute outright fraud",NA
"244","non-significant variables, if included in the model or model selection process, should be mentioned","Same as previous answer","Obviously undermines conclusions about whether the results are statistically significant (i.e., unlikely to be generated by random chance).","Preliminary data exploration is an essential part of data analysis. Provided one is transparent in ones approach, I think this is fine","P-values in general are horribly over-used.  If they are to be used at all, they should at least be used honestly.","There are proper statistical ways to identify and exclude the so-called ""outliers"". The assumption here is that idiosyncracies of one or a few data collection affected its real biological meaning when compared to the pool of other data gathered. In such cases, I think there is no problem in this practice since it is explicitly disclaimed in the report. Malconduct is deliberately excluding data points for the sole reason of changing the significance of the results.","Only suitable if the initial data set is not used in the subsequent expanded analysis, and there is a justifiable reason - i.e. better exploration of a site x treatment interaction that the initial data set cannot sufficient explore.","Sometimes you honestly find a better method during data analysis, but mostly this is just done to try and obtain significance","We should not disclose known problems ","Wow - I'd be surprised if anyone did this.",NA
"245","Non rejection of a null hypothesis is often as important as rejection. Meta-analyses and Bayesian approaches can incorporate information from non-significant results. ","same as previous question!","Obviously you have to use the introduction of the paper to give context for your findings. ","presenting a results as complete when they are not is misleading","p-values need to be stated with a precision that is high enough to compare to the threshold. ","There are some observations that might include different sources of errors, and therefore detecting them and excluding them from analyses would be a way to clean your data. Some times, however, you are not able to detect this problem from preliminary exploratory analyses, but you realize it when you analyse your model residuals. Your decision about excluding data points from analyses at any stage should not be guided by getting a significant p-value.","Only time this should happen is if a power analysis indicates that you didn't have the necessary sample size to detect an effect of interest.  However the power analysis should ideally precede data collection, so this shouldn't really happen often.","sometimes you start with the wrong test.","Well this is the issue with all these questions. Are you hiding something important? That is unethical. Are you not reporting extraneous exploratory stuff that would just add length and undue complexity? thats ok. ","Wow. I hope this doesn't occur. Probably more in ecology than in evolution I would think. ",NA
"246","non significant results can be biologically important ... it also helps guide others in their research","Same as previous, report these ","Obviously, HARKing is a problem because it is a form of hidden multiple testing. However, part of the problem is the false notion that we should structure articles as if we have clear hypotheses from the start. It is not really acceptable to start an Into off by saying, ""I was playing around with some variables looking for X, and I inadvertently found Y,"" but this is often the reality. We should be allowed to present it as such.","Publications tend to be succinct summaries of methods and results packaged in a readable form. That format does not enable the entire gamut of data exploration to be presented. I advocate instead for sharing of data and analysis scripts on online repositories linked to the published manuscript.","P-values should be published without rounding","There are standard procedures for removing outliers but a data point should not be removed because it affects the significance.","Particularly if the results are unclear, I think that collecting more data to improve the analysis is valid. This increases robustness of results and is frequently demanded by reviewers. / However, it should not be done to decrease the p-value because of a larger sample sizes. Not sure if you meant this....?","Sometimes, it can be informative when different tests give different results, because they test slightly different hypotheses or look at the data in a different way. However, p-values should be corrected for usage of multiple tests (Bonferroni etc.)","Well, how do you know? There are these small things which may or may not have an effect. How much heteroscadasticity is defensible? How normal is normally distributed? The level of collinearity, whether to use AIC or BIC to select a model (model selection at all!), all these points are shades of grey. / Small data sets are fickle, and small changes in model structure can affect significance levels. Have I always carefully explored all consequences? Probably not. / Still, I regard that overall as a small problem in ecology, simply because there are more pressing issues, and many analysts are not aware which problems actually exist. So YES, important for p-values, but NO not important in terms of misconduct.","Yeah, this one is bad.",NA
"247","Non statistical significance are results and should be reported if they provide interesting information.","same as the previous question. Maybe I misunderstand them due to not being a native speaker?","Occasionally exploratory analyses can return insight that reframes the context of the work, necessitating a presentation that lays out predictions prior to the discussion","Question is unclear to me. I have never read something along the lines of 'this was the complete set tested', since that is fairly nonsensical. ","P-values should be reported as is, and only rounded to the same decimal place for consistency across the ms.","There are such things as statistical outliers, and if one point has lots of leverage it is often appropriate to re-test without this point (or few points). This goes for significant and non-significant results when the whole data set is used first. ","Pilot studies are meant to investigate if there may a a relation/response. I have seen many people dot this, for example for Honours' project.","Statistical analyses take time to get correct so one often switches tests several times until settling on the appropriate one.","Whenever I failed to mention some weaknesses of the analyses, it was because the article format prevented it - not to hide it. I normally do it very cautiously. I think as long as authors are clear and explicit with the limitations of their study, then it is less of a problem to present analyses that are not perfect. ","You cannot make up your data. ",NA
"248","none sig studies are hard to publish - but it would be good if this was changing, as they are actually very important / none sig variables are often reported, unless lots of them were tested in an exploratory design and reviewers often suggest to reduce the number of variables presented. maybe one option would be to present those in appendices.","same comment as previous","Often, things are interesting because they are not expected.  However, sometimes we get preliminary results that change our approach to reasoning the issue at hand at still an early stage of thinking about the project - in this case, the results of the preliminary data may become the new expectation prior to conducting the full study.  In those cases, I think this is an appropriate way to approach the dataset.  I do think we overuse this approach to appease reviewers and enhance the 'publishability' of our work in cases where the unexpected doesn't make a big enough splash.","questions is badly worded i think...it makes sense to experiment with different analysis options and present the analysis that gives the most precise coherent pricture of the results..... the limited space space in journals precludes the possibility of monographs in which various statistical tests are described.","p-values should be reported with significant digits. Language is discrete and requires a threshold for continuous things like p-values. This threshold is arbitrary.","There are ways to decide whether certain points are outliers, if those are used, then it can be ok to exclude points.","Pilot studies are useful to save time, money and affects on the observed species. However they do need to be done and analysed carefully.","Statistical methods should ideally be decided upon before the data is even collected and only switched if there are clear errors/problems with the analysis (ie. assumptions are being violated).  ","Would be fraud","you should indicate what you have done and why",NA
"249","Not all statistical results are meaningful. Sometimes in exploratory studies it can be advantageous to test many variables and then only follow up on promising directions. Especially in this exploratory stage the power may not be great for a given aspect of the study, thus making it arguably worse to report a result that is then taken at face value (and not necessarily considered for its lack of predictive power).","Same rationale as previous question.","Once the data are in, I think it's fine to frame the paper to present the unexpected results, instead of an #overlyhonestmethods chronology. I know a number of studies where the unexpected result led to an important and novel finding, independent from the original goal.","Reading papers I expect honesty. Researchers should be honest about not reporting the full set of models so that the reader can evaluate the risk hypotheses having been created a posteriori, and of p-hacking.  / I feel like it is more okay to not report the full set of models if at least you do not claim that you do; the statistical problems might be the same, but it is not lying (which is a greater evil than imperfect statistics in my opinion).  /  / ","P-values should not be rounded in either direction. They should be reported to 3 digits in the cases given above or when very small can be reported as &lt;0.001 or &lt;0.0001. To engage in rounding the values suggests that a researcher views P values as indicators of ""significance"" which is not how they should be treated or that they feel the results of their work will not be considered important in light of predetermined values for assessing Type I error. Regardless, rounding the values obscures the actual result and raises ethics questions.","There can be some justification for this if there are known methodological or bias in the data, but this should be done transparently.","Pilot studies are useful, and a pragmatic way to tee-up a bigger, more expensive study. /  / However, 'chasing' pet hypotheses is a massive problem. Finding that one dataset that 'proves' my pet idea appears to happen quite a bit in ecology. /  / Areas where data abound, these problems are possibly not that common. However, where datasets are small or difficult to generate/acquire I think this 'chasing' is a real problem [possibly related to funding].","Statistical tests (and approaches) have assumptions that are often only partially met and vary in their power.  So exploring data sets is a common practice.  In some sense, triangulation using results of multiple tests is a useful approach.   /  / ","would clearly corrupt the process of research",NA,NA
"250","Not all studies have the benefit of a hypothesis&gt;design&gt;analysis. Opportunistic samples can lead to exploratory data analysis and variables that don't make the final analysis.","see answer to previous question","One can report on such unexpected finding, but not as an intentional result!","Relatively few ecologists use multiple model approaches, and those that do almost always report its use. ","P &lt; 0.05 means just that.","There is a difference between (1) screening the data and flagging outliers that do not seem biologically realistic, and (2) running stats on the data and selectively excluding points to achieve P&lt;0.05.  The former can be important for making sure all the data are accurate, the latter is probably one of the biggest drivers of irreproducibility in science because it is absolutely common.","Pilot studies can be informative. ","Statistical tests should be chosen in relation to the type and distribution of data, not in response to a desired outcome.","You have to be honest to yourself. ",NA,NA
"251","not all variables need to be discussed in a multivariate studey","See answers about reporting thresholds.  Journal space often demands reducing results.  I often tabulate non-significant correlations in supplementary material, but I expect few readers look at them.","One should use the best narrative to communicate results","report all models! Add Supplementary Info files with all models tested","P value is not the most important","There is no point using statistical approaches at all with an artificially modified data set designed to reach a specific significance level","Power analysis is a much better approach.","Statistical tests vary in their assumptions and biases, and trying more than one can help us understand which assumptions might affect results.","You should practice full disclosure of everything you did to generate the results you are reporting.",NA,NA
"252","Not find support is also important ","See comments on why studies or results that aren't significant should still be reported.  Additionally, since the inclusion of covariates affects all terms in the model, it is important to be transparent about what terms were included or considered.","Only a small fraction of the patterns and processes in nature are predicted by apriori hypotheses.","Reporting a set of statistical models as the complete tested set when other candidate models were also tested misrepresents studies.","P values are arbitrary cut-offs, but that's the rule and we should stick to it. Journals should also relax a little!","There is no such thing as ""outliers"", only models that fail to describe _all_ data well.","Power analysis should be used at the design stage to ensure that there is enough power to determine 'significance'; but given the difficulties in publishing non-significant results, it might (perhaps) be prudent to collect more data to convince the reader that a non-significant result is the 'true' one...","Statistics is a language to convey information and there are many ways to approach it. Each has its positive and negatives but multiple options exist and should be tried.","You should refer our limitations / Number of populations, etc sometimes is not possible to sample again or is very expensive. Our replicates.  In ecology is a flexible field you have to account for not expected results",NA,NA
"253","Not reaching statistical significance conveys important and interesting information which should not be ignored.","See earlier responses.","p-hacking, the garden of forking paths (sensu Loken and Gelman), etc are bad science.","Reporting a set of statistical models as the complete tested set when other candidate models were also tested, means the reported one is not complete.","P values are just one test of verification of an effect. They should be reported exactly as they have been calculated just as one reports F, Chi, omega values etc.","There may be outlayers that were overlooked initially. After a proper outlayer test (mention in the report) there may be good reasons not to include a data point in the analysis. I would still show the point in the diagram in () but mention that it was neglected for xy reasons. I rather would blame people for blindly dumping data into a stats model. For instance, if I see an outlayer, I could go back to the samples and discover some irregularity that justify to disregard the point.  /  / ","Preliminary studies are often important for further significant or comprehensive studies.","Statistics is just a tool. If two statistical tests are both suitable for the type of data in use, I would choose the one that helps bring the message across.","Your pushed to do so because some will just not accept any doubts on data / methods. However, doubt are so frequent that it should matter  ",NA,NA
"254","Not reporting ""negative results"" wastes other researcher's time as they may also try to do the experiment not knowing that someone else has already done it and gotten results (regardless of the outcome).  /  / However - publishing negatvie results can be difficult as they are not seens as interesting or groundbreaking.","See my previous answer on small AIC weights (or sum of weights) and pressure from journals to write short papers.","Painting a target around the arrow... it happens.","Reporting all tested models improve reproducibility and knowledge on negative results. Even if not always interesting in the main text of a paper, it should be at least reported in an appendix. ","P values are not a religious dogma","There should be biological justification to remove outliers.","Preliminary work is always useful, and often critical to obtaining funding.  If a PI has the time and resources to test out a preliminary hypothesis, I don't see why this shouldn't be done.","Statistics is not a buffet. The design, then the data and its distribution constrain the analysis. There are not so many options once these constraints are set, and they point in the same direction if the assumptions of the different tests are met.","Your work should be roundly defensible if you want to publish it.",NA,NA
"255","Not reporting insignificant findings skews results towards significant findings. However, realistically, and when you're trying to make your article 'powerful', the focus is on the significant trends you did find.","See my previous comments. In this case, it is easy to list all covariates and remark that x, y, and z did not have a significant correlation with the response. I can see failing to list a suite of predictors or covariates for which data were gathered but were omitted from models, but I cannot see including predictors or covariates in a model and then acting as if they were not considered.","prediction is a powerful tool that must be used correctly","Research articles should be concise and straightforward, and sometimes presenting multiple analyses may be redundant or may turn the methods section too confusing. Although, if other analysis contradicted the main results, it should definitively be there.","P values are reported with three decimals by statistical packages. I don't see any reason to round them. ","Thinking exclusively about outliers and their impact on data distribution","prevents beta errors and is way to validate the data","Technically not sound, but often one uses the wrong initial test anyway. Upon further thought, a better test will give more precise results.",NA,NA,NA
"256","Not reporting non-significant results biases the big picture (e.g. meta-anlysis), mislead other researchers into thinking that a question is unexplored...This publication bias however, is obviously a result of the publication system.","See my response to the previous question. Whether covariates were dropped from an analysis should be reported in a Methods section (e.g. as part of model simplification).","probably the most important reason for why many results are not reproducible.","Researchers often need to exercise judgement as to how to present their results in the most meaningful way - reporting on all the models tested does not necessarily facilitate this, in much the same way as reporting on every single empirical measurement taken during a study would not generally be expected.","p values have a variation of their own. Reporting precise values gives a false sense of precision.","This could be the worst way to obtain a very biased results!","Research is a learning and adaptive process. Sometimes we start with one idea, collect the data and find something interesting, and then we collect more data to verify this or to explore the issue in further detail.","Testing can be exploratory: if the raw data indicates an effect, there may be multiple permissible models of capturing it. The first test is not necessarily the ""correct"" one, but neither is the one giving the ""desired"" p value.",NA,NA,NA
"257","Not reporting null-results leads to a publishing bias and therefore to an overrepresentation of positive findings and an overrepresentation of Type I errors in the published literature. This is not rue in rare cases where a null result is actually not interpretable due to the nature of the data. /  / In practice, however, it is extremely hard to publish everything, because null results are extremely difficult to get into any journal. ","See previous answers.","Provided you have confidence in the validity of the results - then it is important to publish an unexpected result.  That is an important way science can move forward.  ","Researchers often use a variety of tools to analyze data sets. In many cases, only the best tool is reported. It is not necessarily helpful to know the full set of approaches that a researcher used - we want to know what worked well.","p values should never be rounded down to meet a statistical significance threshold. The statistic has exceeded the threshold and is therefore insignificant.","This depends on the rationale for exclusion - generally, decisions to exclude outliers require graphing the data. Most software provides all the statistics at the same time. So if it is appropriate to exclude outliers (which is only true if there is a reason to exclude them), it is hard to do so without seeing the results first. Of course, outliers should be equally likely to be excluded regardless of whether they increase or decrease significance.  ","Resulting P value is biased.","That is data mining (or torturing).... this should never be used, but many researchers (particularly PhD students) are forced (unconsciously) to this practice because of the pressure for publishing and the need to report statistically significant results.",NA,NA,NA
"258","Not reporting the non-significant results can lead to the publication bias. ","See previous answers.","provides a misleading picture of how science works","Researchers should chose the most scientifically relevant model not the best explained model.","P values, to the extent that they are useful, should be honestly reported. In any case, P values that are close to a threshold, whether on one side or the other, should always be interpreted cautiously. ","This is a clear misconduct in science. Hiding data does not contribute to improve science, because they were collected. I see no problem in excluding outliers, SINCE the scientist inform the readers about that. Alternatively, analyses can be conducted WITH and WITHOUT outliers (I use this strategy) and the readers can evaluate themselves whether the findings are worth of using or not.","Sample size should be based on power and logistics.","The adequate type of analysis should be adapted to the data and some preliminary analysis is some time needed to find it; Then seldom really changes the level of significiance a lot ...",NA,NA,NA
"259","not reporting these slows down science. Somebody else will waster time and effort trying to study the same variable","See previous comment ","Publication bias","Same answer as on questions 1 and 2.","p<0.05 means p<0.05 not p<0.054. ","This is about checking for outliers or erroneous data points is it not? Again this is dependent on the situation. Is it a real data point? Are the extreme observations an important part of the story?","Sample sizes should be pre-determined.","The analyses should be determined by the sample design / experimental design. If one analyses is giving significance and another is not, then there is something wrong. More importance should be placed in the biological validity of the model and how well it fits the data. ",NA,NA,NA
"260","Not sure what you mean here, there are certainly statistical tests or experiments that fail to yield significant results all the time, and the reality of the publishing world is that many of these studies never see the light of day. I can't imagine any researchers doing a type of work where all of their results are always significant and of reporting value. Now within a study, I think careful study design and appropriate methods and statistical tests should alleviate the need to eliminate any tests, but it is our practice to report both significant and non-significant findings within a study design (i.e. environmental variable that both had and did not have significant associations with some particular response variable). ","See previous question, but probably the pressure is even greater.","Realistically, when performing large scale analyses of data, whether it's phylogenetics, morphometrics, or modeling, sometimes interesting patterns emerge that were unexpected and more interesting than the original questions of the current research. When that happens, it can make a stronger paper if the focus than shifts to the new questions that the paper raises.","Same as before. ","P=0.05 is an arbitrary value and its use as a specified threshold may not always be applicable to ecological studies. In saying this, I have always reported the exact p-value of my findings for review. ","This is against statistical best practice and should never be used. ","Science is an iterative process. If you calculate a p-value early on in a study, and then add a lot more data, a real signal should result in a lower p-value, whereas a false positive will not. So this is useful information to have an idea which way things are heading. I have also been very influenced by reading Cohen's papers on power and effect size. Certainly if you make enough measurements, almost anything will become statistically significant (e.g. a trillion coin flips will eventually result in a statistically significant result due to weight differences in the sides of the coin, but with a meaningless--from a human perspective--effect size). You need to reach statistical significance so that you can start discussing effect size, which is what we ultimately care about. ","The analysis should be thought through prior to data collection.  This is p-hacking.  I think this is done more commonly in an unconscious fashion.  ""the garden of forking paths"" Andrew Gelman calls it. I have probably done this, and I think it is very common problem.  ",NA,NA,NA
"261","Not too difficult to report on things that are significant, then at the very least put in a sweeping statement, that everything else we looked at was not.","see question one","Really makes no diffierence if you predicted a finding or if you could have predicted it: What is important is to get facts right. ","Same as for non significant results","p=0.05 is meaningless anyways. Report accurate p-values and if you would like to discuss a results which is marginal, do so, but honestly.","This is also deception in my book","See earlier responses. And this: http://www.nature.com/news/scientific-method-statistical-errors-1.14700","The choice of answers is too limited: / 1) I mostly do not know the details of my colleagues' habits (this applies to all other questions as well) / 2) in this case there is no precision as to what the proper statistical test is, the initial one or the alternative one. Ideally when in doubt different tests should be carried out.",NA,NA,NA
"262","Obviously, this is the file drawer problem. But we can't solve it until journals accept to publish negative or inconclusive studies. As unfortunate as it is, why would I waste my time preparing and submitting manuscripts that have no chance of being accepted, when I already have more results than I have time to publish? A related problem is that it is important to distinguish negative and inconclusive results. Negative results (e.g. p&gt;0.5 across a wide range of analyses) are quite different from inconclusive results (p=0.08, or p&lt;0.05 for some analyses and p &gt;0.15 for others). Inconclusive results are the hardest to publish, but are still important to put in the public sphere. /  / Even if we found a way for all to be published, there is still the problem that preparing a manuscript takes time, so as a researcher I must make decisions about what results are the most important. Only rarely will this be the negative or inconclusive results.","See response to Question 1, which I do not see as fundamentally different from this question.","Reporting a hypothesis that was generated after the results were analyzed creates a bias when interpreting the study.","Same as previous question: it is easy to run and discard a set of models from an early round of analysis.","People really shouldn't be doing null hypothesis tests in 2017. But if you are going to do a null hypothesis test, rounding is fine since thesholds are arbitrary and dumb.","This is blatant falsification.  It happens, but I don't think it is very common.  ","see no problem; if a pilot study shows an effect, one proceeds with a larger-scale study. If reported carefully, this is OK","the choice of statistical method (and there are a lot open to you, even within a given approach model simplification can take many forms and result in different results) needs to be decided upon.  Rerunning models without a good 'statistical reasons' (i.e. inappropriate error term) because it was not sig is inappropriate.  See comment about effect size, but in my experience if you have big effect sizes using different approaches make no difference, if you have borderline effect sizes then slightly different modelling strategies can get you significant effects, but the effect size would still be low and the model meaningless in practice.",NA,NA,NA
"263","Of course, it limits the accuracy of statistical significance.  I now generally work in a model comparison framework which places non-significant results in a more useful and reportable framework","shortage of space usually why. might just say in a short sentence - tried but not sign.","Reporting such a finding as having been predicted from the start is deceptive, and would be a lie. I personally find exploratory analyses useful to find potential associations and to generate further hypotheses, so I do not think there's anything bad in reporting a finding resulting from an exploratory analysis. The emphasis in that finding, then, would depend on the details of the study.","Same as the previous question.  An incomplete reporting is withholding 'negative' results.  The / story is less than full that is being reported.  ","Personally I don't think this is an egregious offense, but it is unnecessary. The 0.05 threshold is arbitrary, and P-values were never originally intended to be used for ""yes or no"" hypothesis testing (see http://theconversation.com/give-p-a-chance-significance-testing-is-misunderstood-20207). If I got a result with p=0.054 I would report it as such, and also interpret it as pretty good evidence against my null hypothesis. I think it is silly to believe that p=0.054 means the null hypothesis is true, but p=0.05 means the alternate hypothesis is true, when the two values actually represent very similar likelihoods for the alternate hypothesis.","This is blatant p-hacking.","See previous answers.","The choice of statistical technique should be based on the hypotheses to be tested and the kind of data set available, not whether it gives you the answer you want.",NA,NA,NA
"264","Often not enough space to describe all tests undertaken","Shoul not be used because it is also of interest to know which covariate have no influence. Bias towards positives results can be a problem.  /  / IMPORTANT: I understood that by ""not reporting"" you mean, not talking at all of the covariate in the manuscript. If you meant only not including that covariate in final models, then most researcher do it.  /  / ","Researchers learn as they perform the research, and the formulation of the question can change. I'd rather read and report interisting results on interesting question (even if sometimes arising a posteriori) thant boring results on 'rough' questions. There are enough papers of the secon sort.","same comment as previous question. the question is not clear. In many cases model are compared without using tests (AIC etc). Plus, many models may be considered in model selection, and only the best one reported, as they are the most important to discuss + several other constrainst (it is impossible to present all the combinatorial set of possible models). ","Precision is important","This is blatant p-hacking. Unless points are known outliers for experimental reasons (e.g., an error by the measurer or instrument), they should be included. Anything else is a misrepresentation of nature.","seems like reasonable approach to a pilot study","The choice of the right statistical model can be complex, and the outcome of the test is a part of this choice.",NA,NA,NA
"265","Often word limits mean cutting parts of a study entirely","Should be avoided if possible.","Researchers often go on a data dive and see what interesting patterns appear. Often its more interesting than the original idea, and so they craft the narrative around it. ","Same reason as previous question.","regardless of decimal place used, there should be a standard.","This is changing data for the outcome required.  The data are the fact, the outcome is the unknown. /  / However, I have done (and probably will continue to do) something similar to test robustness of analyses.  If I have some dubious points, which may or may not truely belong to the sampling frame under consideration, then I may perform the analysis with those points and without them.  If the results are the same then it doesn't matter about them.  If the results differ, then I have to explore those points more fully.","Several empirical data we work with are not sufficiently resolved or sampling efforts are quite low to detect a pattern and the robustness of it. Sometimes there is an additional effort to be made to explore the robustness of the patterns found with smaller sample sizes. ","The correct analysis should be used, even if it does not yield significant results",NA,NA,NA
"266","One can't simply flood the literature with negative results.","Should not be used as it is an incompete report of experiment","Results should be presented as they are and let them talk for themselves whether the findings were as predicted or not. OK to state ""contrary to what we expected...""","Scientific practice, more so than just about anything else, ought to be fully transparent. I venture there is never a good reason not to report on the full set of what was tested. Generally, the only reason to avoid doing so is to sidestep results that conflict with pet ideas being tested.","Report p-values as they are, the threshold is arbitrary and confidence intervals should always be reported. ","this is cheating","Should be used for pilot studies. If results are not significant, it may be due to lack of power","The data is the key. If it seems like there's a clear pattern in the data, but not all alternative tests give the same qualitative result, then I think it's ok to do different types of test as long as all are reported. Then the reader can judge for him/herself whether this is a robust result or not.",NA,NA,NA
"267","One, negative results are also important. / Two, it shows how many tests were actually done with the data, so how seriously to take the variables that were ultimately found significant. ","Should: during model simplification","Reviewers and editors want you to do hypothesis led research. I think this is naive, wishful thinking, that would stifle creativity.","See answer to first question. ","Reporting a rounded value as the exact result is bad practice. Just report it as Marginally Significant.","this is cheating, no doubt...","should be used when necessary, if sample size can be increased by a considerable amount, easily.","The experimental design and the best statistical methods should have been decided upon in advance of the data. However, as I have done, sometimes the maths can be applied in more than one way, e.g. the same kind of philosophical approach to finding truth, can subtly change the results. But I have not done hoping for a dramatic change, just further clarification that what I am seeing represents my data. ",NA,NA,NA
"268","Only where exploratory analyses at a very early stage included a very large number of potential explanatory variables, many of these highly correlated, and being very similar in biological or ecological meaning (see earlier question). Variables that fail to reach statistical significance should in principle be included; I do believe it is not always needed to treat these at very great detail, especially if they don't make sense ecologically or biologically anyway. In principle they should, however, be reported.","Shouldn't be used","Science doesn't proceed like we were taught in school","See comments to previous question","Research of people not reporting accurately should be questioned.","This is cheating. The only justification is if there are very strong, other arguments for excluding points. But even then, the relationship should still be significant when they are included. ","Should design your sampling strategy with more care!","The experimental design or type of data/study should guide the analysis. p-values should never guide the type of statistical analysis. ",NA,NA,NA
"269","P values basically just tell you that you have a large enough sample size to reject the null that precisely nothing is going on (e.g., r = 0.000, X1 - X2 = 0.0000). They don't tell you anything about estimates, confidence intervals, or the relative strength of evidence for different models. ","Shouldn't: hinders scientific progress","Science is a lot of discovery, but writing about it is very different given the constrained way it's expected to be presented as all hypothesis testing.","See earlier responses.","Research practice should be ""p&lt;x"", not ""p=x"".","This is clearly unethical.","Should not be used. If it is done it needs to be reported in the methodology.","The initial analysis may not be the most appropriate for the dataset, in which case one should switch. But the criterion for appropriateness should not be the outcome",NA,NA,NA
"270","Page limits by journals encourage a focus on 'novel' (= significant) findings, so less interesting variables are often dropped.  Competition is so high in the sciences that people will preferentially publish their most impactful studies and leave the non-significant studies in the file drawer until they get time (which is often never).","Slimming down of maximal models is a standard and probably legitimate practice.","Science is about testing hypotheses. If we don't state them and test them we're not getting anywhere, even if with hindsite we can see why we got things wrong.","See last question","results should be reported to the appropriate number of significant digits","This is definitely not right or ethical. Points should only be excluded if they are outliers, which should be done prior to significance testing and the reasons for the exclusion need to be given in the methods.","should rather do a power analysis beforehand - then again, it can be that there is more variation in data than assumed initially, yet minimising the number of experimental animals etc. is also a concern, it can be justifiable to see whether conclusions can be made or whether additional data is needed ","The key here is ""after the analysis initially chosen failed to reach statistical significance"". Changing the test initially planned is fine if your data winds up having a different structure that makes the planned approach inappropriate, but making such decisions based on significance is obvious p-hacking.",NA,NA,NA
"271","Part of publishing is telling a story and including extra information that doesn't give any insight can throw the story off track. *However* researchers that don't report the results of specific tests should always include the total number of tests they have done, or the total set of variables they explored so we may know how to interpret the pvalue.","So I am not entirely sure what you mean here, but if you put a covariate in a model you need to present the results for that covariate even if non-significant.  Particularly as order of covariate deletion in model simplification can affect outcomes.","Science is surprising and exciting, and we should recognize it more honestly, it would make our results even more attractive to the general public. Personally, I used this by-pass at the beginning of my career, at a time when I was not confident enough to admit that I had not thought in advance to a possibility. The ""publish or perish"" is so intense, and the canalisation to a unique, stereotyped style, in publications is so strong that, honestly, you can not go against the flow, especially at this time of your career.","See my previous answer. Again, more tests mean that the nominal significance level is distorted. /  / Earlier in my career, as a field ecologist, I was not aware of the consequences of dredging (or even that term) or of data snooping. (As an apology for earlier behaviour.) / ","Ridiculous practice, as P-values are random variables and known to be poorly replicable. A P-value of 0.054 does not expres anything biologically different than values of 0.03 or 0.08. The binary thinking with 0.05 as threshold is outdated.","This is disreputable and I have seen it done.","Should report results of initial analysis. But if results are ambiguous or suggestive, idea of additional study seems fine to me. Analogous to stopping points for clinical trials, etc.","The main problem is the use of significance testing. Then these kinds of things can happen.",NA,NA,NA
"272","Positive bias in publication is a significant problem, as it leads to researchers 'chasing p-values' and also leads to inefficiencies in the sense that people replicating studies that been undertaken previously but not reported in the literature.","Some do this through the practice of dropping non-significant variables from models as part of model fitting. I don't subscribe to this approach but others deem it legitimate ","See Peter Medwar's, the great lie of science. ","see my responses on not reporting non-significant co-variates","Rounded values are easier to read, fine to do it as long as stated clearly in material and methods; it should not be done to increase the number of significant findings. You hope that authors and readers do not focus blind on p values anyway, and rather look at effect sizes in full awareness of items like false discovery rates","This is fraud; not just taking too much freedom in reporting, but actually changing the data (set).","Since collecting more data doesn't guarantee significance, going back to the field/lab to collect more data sounds OK to me. ","The match between analyses and data should be the only criterion for choosing a specific test.",NA,NA,NA
"273","Pretty different - not reporting studies that fail to get p<0.05 is reasonable enough given the cost of publishing a paper. Omitting to report variables within a study that don't reach p<0.05 is borderline dishonest.","Sometimes it's not relevant to the story but frequently the most interesting outcome of research is which predictor variables are not related to the response.","Setting up a hypothesis about an result unexpected to the researchers in the introduction/background makes sense if it gives this result more context for the paper's readership - though if this cannot be achieved (i.e. there is little to no background literature) there is no point in presenting an unexpected result as expected.","Selecting a model can be seen is a type of fitting, and thus, overfitting is possible. For example, if I fit 1,000 models , I may find one fits the data well just by chance. This must thus be reported and accounted for in the research design.","Rounding-off a p value will depend of the kind of analysis we are dealing with. Most useful is to provide a sensitivity analysis to show the robustness of the threshold.","This is highly misleading and is tantamount to research fraud. If the impact of a particular data point is of interest, results should be presented both with and without that point and the impact explicitly described.","Since p values strongly depend on sample size, this practice is quite common. I would emphasize effect size more than p values; if effect size is high, then it may be good idea to increase the sample size. ","The nuance here is again that cherry-picking methods is not OK, but validating ambiguous results across many methods is often necessary. For example, if the first analysis is p=0.08,.it might be appropriate to run many others to see if this appears to be a real or spurious trend. But the full range of analyses must be presented.",NA,NA,NA
"274","Prevents meta analysis.","Sometimes one engages in 'exploratory data analysis' and every step of that cannot be reported.  Of course it is always important not to pick and choose the analysis that supports your bias, but there are legitimate uses for EDA (see Sokal and Rohlf, and others)","Setting up the hypothesis in this way can be more clear for readers and is more likely to be accepted by journals","Seriously skews interpretation of results--this is basically p-value fishing.","Rounding 0.054 to 0.05 is not going to make much difference to the interpretation because p=0.05 is not necessarily strong evidence for a relationhship - depending on sample size and model. Rounding to 0.01 is also not going to make much difference to the interpretation.","This is just wrong. It's fine to remove outliers for non-statistical reasons (e.g., looking at the data and realizing that a library prep for sequencing failed).  However, if you remove points based on stats, you affect all downstream stats.  That's bad karma.","Small datasets often lack power to detect true relationships. Additional data, gathered in an unbiased fashion, is the best solution to such limitation.","The only reason I'd decide to change a statistical analysis is if its assumptions are not met and there's a more adequate alternative. But this decision would be independent of the original p-value obtained.",NA,NA,NA
"275","previous answer applies to this too - the value of a study is not contingent on the resulting P-value","sometimes, we are testing some alternative hypothesis (or better: some potentially confounding effects) with only very low support - let's say: just to be sure that there are indeed no notable effects. / In such cases, I have sometimes refrained from mentioning the testing of covariates reflecting such potentially confoundign effects in case they were not statistically significant. Reason: to not to inflate the paper by hardly (a priori) supported, alternative hypotheses.","should be used when necessary: we need to make paper sexy..","Shoud be avoided. The FULL set of models explored is a component of the final evidence presented, hence a careful choice should be made a priori and the full analysis, including multiple competing models, reported. Performing many models for exploration is fine, but this should be declared.","Rounding a p-value to reflect an appropriate number of significant digits is important and entirely ethical, and under those circumstances, one might round 0.13 to 0.1, but rounding in order to give the impression of greater statistical support than is actually present (e.g. rounding 0.54 to 0.5 to make it appear to match a threshold) is deliberately misleading and should not be done.","This is malpractice. Outliers must be thrown out before you do the statistical tests and not afterwards.","So long as your behaviour in terms of collecting further data is not affected by the result of the test, then this is fine. If you stop only when you have a significant result, then this is not ok! Entering and plotting and even analysing your data as you go along can be useful, so long as you are wary of this problem.","The P-value is not a criterion for determining the quality of an analytical approach",NA,NA,NA
"276","problem: often you cannot studies without significant results!","Statistical association is not a cause-effect association. If some covariates covariates fail to reach statistical significance, why do you need to report them? They don't help in the abduction process to generate an explanatory hypothesis. They don't help either in pointing to any possible association. You should rather go back to the blackboard and rethink your analysis, experiment, etc. ","Should: sometimes reviewers will suggest alternative approaches to investigating data or experimental designs","Shoudl be used only if models tested but not reported are equivalent, for instance same model but different packages in R.  /  / If models are really different, results should be reported because it is interesting to compare several models whatever their results. ","Rounding is fine as long as it is to the relevant digit. 0.054 equals 0.05. This is mathematics, not statistics... However, rounding is for everything, not only ""o meet a pre-specified threshold."" There is no point in saying p = 0.23412734618745618974561093847145, so round it p=0.23!","This is manipulating the data to meet a statistical threshold and therefore is very bad scientific practise and should never be used. Outlier data points can be included but only with strong justification for this.","some experiences are labor-intensive. This might be worth then to conduct a small-sized pilot study to explore whether we might be correct in our predictions, before running a full-size study.","The parameters are not suitable or untenable.",NA,NA,NA
"277","Regarding not reporting variables, I think this is rarely practised. Regarding non-significant studies - it is impossible to distinguish between lack of evidence and evidence of lacking, both of which can lead to non-significant results, such studies are, with the best will in the world not generally accepted for publication in ecological journals. My personal preference would be to have some system for reporting non-significant results along with sample size, but I doubt journals would devote expensive journal space to this","Stepwise regression in all its forms is biased and unreliable, but very widely used. AIC-type approaches are also problematic because the weight given to the number of variables is arbitrary. It is not always necessary to present every detail of every covariate, but it is crucial to give an accurate broad picture of what covariates were tested and how, and how this affects results. It is not OK to omit some analyses from presentation just because they don't fit with a narrative.","shouldn't used because: / - it encourages to publish (as usual) only positive results, or if they are not positive, to present them as if they were / - it closes rather than opening discussion / - it encourages lying","Should be used when: looking for generality in statistical models / Shouldn't: when trying different models to find one that is significant","Rounding is not adequate when we are evaluating a threshold.","This is most certainly wrong and I doubt many scientists do this.","some statistics needs big data. If more data could be collected, it should try to collect more data.","The rationale for the methods used should be clear from the start. ",NA,NA,NA
"278","report everything!","That's almost exactly the same question as before. /  / The point is again experimentor's degrees of freedom, which inflate type I errors beyond the nominal level.","Shouldn't: a priori instead of a posteriori hypotheses","Should be used/should not be used is a rather irrelevant question I would say. To me, it is a way of data exploration and a way to understand what really are the driving variables in your model(s). I.e., if the coefficients change a lot when a new variable is introduced, something is going on. / Now, I mainly use model averaging so I do not consider this issue much.","Rounding is OK and I've surely done it, but rounding in order to change the apparent qualitative conclusions should never be done.","This is obvious p-hacking. However, if such outliers are biologically explainable, presenting results with and without such an outlier may be acceptable. However, such outliers should have been caught and a removal decision made for severe outliers at early data-inspection stages (normality, skew, etc) before significance was known, to minimize motivated behavior.","Sometimes one begins with insufficient data.","The statistical method needs to fit to the data and the research question. If the method is chosen based on the results it produces majorly inflates Type I error rates and leads to spurious results.",NA,NA,NA
"279","Reporting bias is a definite problem, although from the perspective of an individual researcher it can makes sense not to invest time writing and submitting a manuscript that, because of insignificant results, is unlikely to be impactful. Additionally, I believe it is common for researchers to perform tests that ask questions beyond the scope of their original hypothesis, to help generate future hypotheses and/or refine the questions they are asking. In these cases it may not be problematic to avoid reporting negative results when they do not have a bearing on the interpretation of the reported findings.","The core principle at stake is whether the covariate was part of the initial hypothesis, or a nuisance variable. If part of a core hypothesis obviously all hypothesis tests must be reported. However expecting a researcher to report the significance and effect size of every nuisance variable, or interaction effect with nuisance variables in the course of model selection in many cases would be extremely cumbersome.","Some studies are pretty dynamics and developing, so could be the expectations....","Should not be done, but on the other hand it is normal to explore some ideas at the beginning before realising they were not good enough to be included and discussed in a paper.","Rounding numbers is common in science and not limited to statistical testing.","This is obviously a case of massaging the data. Data points are the only 'true' thing in statistics and so should only be excluded if they are justifiably some logistical mistake. ","Sometimes some effect size is smaller than predicted before and to actually capture that effect it is necessary to increase the sample size. ","The stats should be pre-planned and modified only if the data do not conform to assumptions (e.g., normality and homoskedasticity)",NA,NA,NA
"280","Reporting both significant and non-significant help to understand better the empirical patterns or the process to infer from the empirical data","the covariates have to appear in appendix at least","Some times we write a good story - includes the introduction setup & prediction - to fit the results because that will help it get published.","Shouldn't be done if you are hiding something, but if a range of models were explored, they all showed the same thing, no need to burden the reader with everything that you tested. Also, usually I say something like ""preliminary analyses"" when I'm testing different models, but I don't necessarily say exacting what all the models were that I tested preliminary, unless its pertinent. Can you imagine how long the stats section of the methods would be if people reported every little thing they tried and discarded as they evaluated the data!?","Rounding of data summaries to reflect precision is acceptable, so it should be for P values.","This is pure and simple p-value inflation. Getting rid of outliers can be justified, but should be taken very, very seriously.","Sometimes this is certainly worthwhile, but not merely to increase sample size or reps.","The type of analysis should be proposed at the design stage, and any changes should only be because of assumptions of the model (eg transformations of the data to meet assumptions of regression analysis), not to try to find statistical significance.",NA,NA,NA
"281","Reporting studies that fail to reach statistical significance or some other desired statistical threshold contributes to scientific progress through minimising unnecessary effort and contributing to meta-analytical approaches. However, researchers may struggle to publish such studies which editors perceive as of low impact.","The full set of tested covariates should always be reported for repeatability and transparency.","Sometimes hindsight is 20/20, and important variables were missed because we had a weak understanding of the system. But if you find papers that have seen these effects, than it is important to bring these into the Introduction of the paper.","Shouldn't to the extent the practice hides analyses supporting an alternative conclusion /  / Should to the extent researchers apply models hat turn ou to be inappropriate, based on their or reviewers' expert judgement","Rounding off p-values to me implies data cooking which is clearly unethical.","This is scientific fraud.","Sometimes, pilot experiments are set up in order to refine the techniques or to get a hint of what the treatment effect might be for only after perform the experiments with a full set up","The type of data to be collected and analyses to be performed should be decided before starting the experiment. Changing tests after the fact should only occur when a more appropriate analysis is discovered, which should not be an issue if due diligence is done ahead  of time.",NA,NA,NA
"282","Reporting unsuccessful experiments is uncommon.","The lack of a relation is also information, from that perpective it should always be reported, so other can take that into account when designing their experiments. And if their was a power issue, the amount of variation explained by covariates in multiple studies could be considered together to get better estimates of the possible predictive effects of that covariate. / Yet for the sake of simplicity, I assume that authors sometimes omit these results or preferable move to supplementary materials","Sometimes it is only unexpected for you because will are not aware of all literature in the subject. Once you review the literature, you discover that you could have expected the result. Then, you should review your introduction to make the result expected.","Similar to previous questions about covariates and variables that were included in studies but were not reported if they did not attain a P &lt;0.05, the practice of not reporting alternate models that were tested but had poorer fits or larger P values obscures the process used by the researchers as well as potential alternate models that are also fit to the data set in question. Earlier in my career, I was told that testing multiple models and reporting only the preferred one was common and acceptable. Since that time, I agree that this is a common practice but disagree that it's acceptable. If researchers wish to test multiple models, they should specify them in advance, test them all, and report why they settle on one over the others. As an alternative, researchers can use an automated stepwise or piecewise fitting procedure as long as they report the estimation methods and what statistic (e.g., AICc, BIC, R2...) was used to decide which model was the ""best fit"".","Rounding p-value or other quantities to meet a pre-specified threshold misrepresents statistical outcomes.","This is scientific fraud.","Statistical significance is not the point.  Effect size is the point.  One may decide to gather more data to get a more precise estimate of an effect size.","The type of statistical analysis should be chosen as the most appropriate one considering the data and the question. It is ok to change if appears to not be the most appropriate way (under recommendation of a reviewer for example). But it shouldn't be motivated by the (absence of) results.",NA,NA,NA
"283","Researchers are only interested in positive results (as are journals) but negative results are clearly key to informative meta-analysis. Everyone has had studies fail, why would they report them?","The lack of significance, despite a good experimental design, is also a result and it's important to publish it!","Sometimes the author is honest about this (mentioning in the introduction that the aim was to describe patterns) butreviewers or editors ""push"" to present study objectives as more ""hypothesis-testing"", leading to present the results as having been predicted. I think it is better to stick to the facts, ","Since I am not aware of a better parsimonious way.","Rounding the numbers is not allowed here.","This is scientific misconduct.","Strictly speaking this isn't done (and isn't), but in ecological research it is very common to do something which is very similar to this: collecting pilot data and conducting power analyses to determine the sample size that would be needed to detect an effect ","The type of statistical analysis should only be changed if the data do not meet assumptions of the models that were selected a priori.  This should not be based on p values.",NA,NA,NA
"284","Researchers conduct many types of analyses on data. Some of the connections or correlations we test for do not lead anywhere. I usually report only about 1/3 of the tests I have done in a published paper. Often I figured out another way to analyze and present the data that made more sense. ","The primary covariates of interest should always be reported.  Other, ""secondary covariates"" often arise as people analyse their data in different ways (i.e., exploratory analyses); these will always be reported if something interesting arises, but will not be reported if they do not prove useful.  In a perfect world, all of these aspects would be reported, but space and the need for a concise narrative will always get in the way.","Sometimes the initial hypothesis cannot be tested with the collected data and has to be reframed - non-significance for example does not nessesarily mean that there is no Effect it only means that you did not find one (hard to publish). ","Some models are bad!","Rounding to meet a pre-specified threshold is dishonest. If one has to, round up, not down.","This is scientific misconduct.","Sure, basically every pilot study is based on this. Using a small sample - check and see if there is any response or trend. It doesn't have to be statistically significant at this point, just an indication that something interesting might be occurring. And then plan a larger study.","There are correct statistical tests for certain experimental designs and if a few methods test different kinds of null hypotheses they should all be included. If instead the authors want to use a test whose assumptions they violate just to get a different p value, this is obviously not a correct analysis and would (hopefully) be picked up in peer review.",NA,NA,NA
"285","Researchers might choose to invest time writing up studies that are more groundbreaking/imminently useful, and on average this may be more true of studies that reject the null hypothesis. In an ideal world it would all get written up, but there is demand for impact from research and limited resources.","The question is a bit unclear; this could form an element of a model building strategy but should then be stated clearly. / ","sometimes the unexpected result it a highly interesting outcome. Focusing the abstract and introduction on the original topic, but then moving the more interesting result would also be wired.","Some models maybe exploratory","Rounding to the lower value when the digit is under 5 is usual mathematical rule. In terms of statistics, from my point of view p=0.05 or p=0.054 are similarly marginally significant and should be considered with caution. The p value only indicates a tendency, and is not the god truth.","This is simply fabrication of results. It is completely wrong.","Testing power again after a sample data set is obtained is probably appropriate but not really common in my field","There are often multiple ways of testing the same data. Sometimes issues with one test become evident after seeing a significant/non-significant result, which indicate that searching for an alternative test might be useful. Generally speaking such a method should not be used, because if results are borderline significant in any given test, and the test is also appropriate, reporting incorrect results is likely. A better solution here can be to report results from several analyses in supplementary if all analyses are equally valid.",NA,NA,NA
"286","Results that support conclusions are not always unanimous but that does not mean that the conclusions are false. It is the researchers responsibility to report limitations of their results but that is all.","The question is ambiguous, as it's not clear what would constitute ""reporting"" the covariate, and I think that depends on the study. If it is part of the design of the study or there's a theoretical reason to include it, a covariate shouldn't be excluded from a model. But is it ok to exclude something you measured ""just in case"" but didn't consider theoretically likely to have an effect (and you should still report it).  ","Sometimes this arises from the need to focus the introduction of a paper - the narrative simply wouldn't 'work' if unexpected or counterintuitive results were presented.","Some models, especially in the case of data mining, are not really worth publishing. However, in most cases one must refer to these group of models as being inferior to the models that have been taken into account for the analysis. In rare cases, one can omit these models if this would simplify the story of the paper (e.g. if paper is already too long, complex, ....).","Rounding values is normal practice, which is done (if properly) according to a standard procedure (as shown in the examples in the question) and to avoid reporting overly long numbers. This is especially true for values that can have many digits like p-values. Therefore, no one can be misled by a rounded p-value or other type of datum. / In the end, I do not see much difference reporting a p = 0.055 or a p = 0.045 as 0.05, as in the end the significance threshold of 0.05 is just a convention with little value, and we should pay more attention at the overall magnitude of the value.  I do not find using p = 0.05 (or other value) as a sharp cut-off very useful nor informative, and we should start getting rid of such easy but ineffective ways of interpreting our data.","This is straight-up selective editing of data and is unethical.","The distinction between a pilot study and full study can sometimes be blurred. However, as long as the researchers make it clear how the data were collected that's ok. boosting sample sizes are just as likely to make the result go the other way, so it's not as thought his is always a bad strategy.","There are reasons to try other statistical methods that are non-redundant to explore data, but p-value shopping is not one of them.",NA,NA,NA
"287","same answer as before","The readers need full information to come to their own conclusions.","Sometimes this helps tell the story better, but I'd say it's fairly rare. More common is to state that the result contrasted with predictions, or was unexpected.","Sometimes (often at the beginning of model building/testing) one makes ridiculous mistakes by using very inappropriate models. Reporting all of them is beyond the scope of a paper and wouldn't give the reader useful information.","Rules on rounding are clear and allow for no ""interpretation""","This is very bad. Dumping data points purely to alter the stats is terrible.","the fact to add data or not has nothing to do with the significance of the results. ","There is some nuance here: there is often no single correct test, and a battery of related tests with different types of assumptions can yield complementary insights. However, if the motivation is the pursuit of significance then this is wrong.",NA,NA,NA
"288","Same answer as previous","The results are biased and overoptimistic.","sometimes you don't really know the predictions from the outset, its OK to change your predictions to match the data.","Sometimes a model is run before you realise that it isn't specified correctly or has far too many interaction terms included. ","Seems disingenuous, but I don't think it's that big of a deal statistically.","This is very common. The correct practice is to exclude outliers prior to statistical analysis, however in practice I find this is almost never done.","The grant funding systems in most countries actually require you to do this practice. Grants are unlikely to be funded without preliminary data supporting the hypothesis. This practice of needing to do some of the research before you can get funding to do the research is ridiculous for more reason than one. ","There might be situations where the initial analysis has neglected an important aspect of the data at hand (e.g. nonlinearity). In these instances, changing the analysis is valid in my opinion, assuming that this process is reported openly.",NA,NA,NA
"289","Same question as earlier.","The significance of any term in a model is contingent on what other terms were included in the model. This is important for both reproducibility and for interpreting results.","Sometimes, research is by its nature exploratory. In this case, unexpected results are the focus of the research- the null hypothesis is that unexpected patterns don't exist. This approach makes unexpected results a prediction.","Sometimes data sets don't behave the way the researcher anticipates and it takes testing numerous models before finding a statistical framework that provides good model fit. Many editors do not support pages of descriptions on all methods that have been used and I have received criticism for over-reporting statistics from editors and reviewers. The motives should always be to find a best fit model and never to explore p-values of different model types.  ","Seems that statistic is ""subjective"". Can this change? Maybe consensus in thresholds?","This one, if I knew about it, would irk me a ton. Blatantly dishonest, but I am sure some have engaged in this practice. Just kind of a bummer to think about, like why are you even scientists at that point?!","The initial sample size may have been too small.","This biases the reported significance levels of the final analysis",NA,NA,NA
"290","Same response as the last","The statement here again raises questions about the boundaries of a publication. At one extreme, I once performed an analysis that gave no statistical significance, so I didn't waste time trying to ""report"" this. Does that count as ""not reporting covariates that failed to reach significance""? (I wonder if ""covariates"" here means secondary variables, or quantitative variables.) In another case I have tried to publish a paper reporting a high proportion of non-significant covariates and it was rejected. There are also difficult cases where, because of missing data, including certain variables in an analysis reduces the number of available data - so I may drop a ""non-significant"" covariate without discussing it in my paper, because using it entailed analysing a different dataset.","strictness in writing schemes sometimes forces this practice","Sometimes models are built that violate assumptions. Filtering out misleading models is important and it could be confusing or misleading if all these models were published.","Seriously?","This practice can be used, if there are good reasons for selecting data, e.g. source of data, quality of data, observer's skills, etc.","The magnitude of variation, not statistical significance should guide such sampling decisions.","This can be appropriate if the rationale is based on examination of the underlying distribution of the data and the assumptions. Of course, if these assumptions are not met, this should result in changing the analysis method regardless of significance of the original model. ",NA,NA,NA
"291","Science progresses from negative as well as postive results. /  / FYI - I am unable to answer Question 1 so left it at ""0""","There's no reason not to comment on co-variates in this way - if they are introduced as part of a planned design, then they should be commented on.","Studies should be planned in order to understand results easily. When surprises appear, it would be equally important to show them as they are. / When you ""adapt"" the surprise as planned, it means you don't really thought about it well from the beginning.","Sometimes the initial models used are not the best in terms of their assumptions or how they parameterize variables. Sometimes also I (and likely others) have misunderstood how the initial model works, hence switching to a more appropriate one.","Setting aside the fact that P-values in isolation are useless, adjusting them to meet pre-specified thresholds to reach an arbitrary level of statistical significance is likely to be an attempt to make results look more real than they are.","This practice has two sides and the value and legitimacy depends on the intent of the exercise. It is inadvisable to discard observations in an attempt to reach statistical significance, but excluding observations that may be disproportionally contributing to significance can be useful in understanding the robustness of a statistical result. ","The repeatability of a result is paramount to its importance in biology.","This can only be used if the initial test was not actually appropriate for the data",NA,NA,NA
"292","Scientists need to use their judgment about when non-significant result are informative or not. For example there is no need to include the results of experiments that were screwed up in a known and understandable way (e.g., forgetting to add a reagent, boat running over your experiment)","There are only a few exceptions when not reporting insignificant fits is acceptable, such as when multiple covariates are highly correlated and you have already reported at least one of them as insignificant.","Supposedly, first is the hypothesis and then you get the data. / From what I see, people just take data and then try to find an explanation of whatever interesting story they find from the exploratory data. / This is a big problem in science and must change.","Sometimes there is a very large number of model combinations that one can use. Some of these are not relevant, so they are not worth testing. Moreover, in Bayesian phylogenetic models, some aspects of the hierarchical model do not affect some of the estimates. For example, some assumptions about population size dynamics have no effect on estimates of the rate of evolution, so they are unnecessary to report.","Should give a least 2 significant digits for a P value.","This practice is not inherently unethical, as sometimes the removal of outliers can be a subject of interest, or if there are reasons to believe that one or several outliers may hinder the robustness of analyses or models. It can be used as long as the exclusion of outliers is reported and supported with reason.","The results are invalid.  / Train people. / Punish people.","This creates the opportunity for authors to opt for whatever analysis gives their preferred result (possibly instead of whichever analysis gives the most reliable result).",NA,NA,NA
"293","see my comments before","There are tons of statistical methods that have been developed to help one weeding out insignificant covariates. Ecological data analysis contain high-dimension information. You need to find the principle ones so to be able to understand the system. Unless I misunderstand the question, I do not think this is a question at all for ecological data analysis. ","Taking the reader through the history of the writer's thoughts might be interesting from a biographical standpoint, but will often make for convoluted prose that does not bring conceptual clarity regarding the actual finding or conclusion. ","Sometimes we test models that has no support in literature or empirically or does not make sense, considering logic rationality. Thus, we can remove this model. ","Should only be used if all values are rounded to that decimal level.","This practice is only justified to identify 'true outliers' or measurement errors (e.g. some individuals measured in inches, rather than cm). While I think it is not justified, on a case-by-case basis this could occasionally be justified.","The way you phrased it, it's actually ok. The problem arises when you do more samples when it's p>0.05 by a small margin, but not when it's p<0.05. This bias in whether you add more samples is what inflates the FDR. I think it might be justified to inspect the results, notice that your effect size CIs are larger than you'd like, and then do more samples (particularly where pilot studies to determine the variance are not feasible).","This is a byproduct of complex multivariate analyses when variables have very different types of statistical distributions. It may not be possible to be sure in advance which statistical assumptions might be violated by a specific form of analysis until the analysis has been run. ",NA,NA,NA
"294","See previous answer","These deserve at least a brief mention -- the initial model(s) should be described in the methods, and if covariates were not significant and thus dropped from the model (through simplification) then this process should be reported.","The answer to this question depends on how wise the eresearcher is! i.e. how well they know the discipline they are researching. If they knew it better, then they may well of predicted the effect. By going through the research discovery process they have learnt more about the organism, and so refined their understanding and the relevance of different literature bases and potential alternate hypothesis. This comes down to the exploratory nature of much ecology.","Sometimes when different tests are used you see one was inappropriate based on other concordant results.","should report the actual number and if you do round - do so conservatively","This practice may be ok IF one reports the test results both including and excluding the outlier. This is an approach which I have sometimes used. ","There's nothing wrong with collecting more data!","This is a gray area.  I think that you want to use the proper analysis for your data.  Probably the best practice in this situation is to report both analyses, as the lack of significance in the first analysis does not necessarily negate the finding in the second.  That being said, one should not go fishing around for just the right analysis to give a significant result.  One should use the best analysis, given the assumptions.",NA,NA,NA
"295","See previous responses - it is a fantasy to think we are simply reporting a series of experiments/tests. Articles are narratives in which we choose how to frame, which information to use, which references to cite and which statistics to support our work. Stats are supporting material (as are citations) for the narrative - of course they are cherry picked!","These non-signicant factors are important as well and help us understand the full picture of a given phenomenon.","The idea that an unforeseen result was known/predicted from the start is simply false. You should misrepresent your findings in this way.","Sometimes, exploring trends in data is useful/informative before a complete set of models is formally tested.","Shouldn't be done; At least we should stick to the methods we have. /  / Although we all know that a=0.05 is kind of arbitrary. And the use of permutation approaches for p-value calculation will give you a bit of another view towards the meaning of P=0.049 or p=0.051.. / ","This practice should never be used except in a sensitivity test. In other words, present ALL the data, present the statistical analyses, and then show the new statistical analysis when the data points are removed. Let's be honest--some data points are really garbage and don't belong in any analyses! How to correctly include/exclude these becomes a grey area, but I think data exclusion has a place at some times in some studies. ","There are many reason why more data can be needed, one is that the range is not appropriate (eg two small). But this can be costly, and most of the time not possible.","This is a more tricky one. Obviously one shouldn't do it, again for all the reasons mentioned before. However, I consult on statistical questions and often find people to start with an inadequate or at least insensitive method. When recommending something more sensitive (e.g. going from a simple to a multiple regression), I pretend not to know the results (indeed, I often interrupt the person before they tell me whether the result was significant or not; also, they often come to confirm their approach, even if it did yield their hoped-for significances). / In teaching I emphasize the role of thinking before doing, but it is hard to preach water when the supervisors drink wine.",NA,NA,NA
"296","See response to Q1","This can potentially be done if an exploratory analysis is being done to select the variables entering the analyses. If there is no knowledge regarding potential predictions preliminary  analyses (e.g. correlations) can be used to select an initial set of variables to enter statistical models. / However, if these covariates were already believed to have some importance and were considered in the initial hypotheses being tested, then I would say that not reporting their statistical ""insignificance"" is not of good practice","The Mexican grandmother problem: If you sift through data enough you will find significant results by chance","Sometimes, if you are restricted in how many candidate models you can compare in set (if you believe model number should be restricted by sample size), it helps you personally understand and be confident in your results if you try a few different candidate model sets.","Shouldn't be rounded down obviously. ","This practice should never be used. An alternative would be to run an outlier analysis before testing data to create a uniform method and cutoff for eliminating data points that are questionable. However, this approach must only be undertaken if there are legitimate reasons to assume data points are outliers for some reason other than natural variation (e.g., a measurement issue).","There are some valid reasons for inspecting data part-way through a trial - for example, to understand inter-individual variation and determine the sample size required to achieve a high level of statistical power.  Nevertheless, this should not really be a determinant of whether more trials are run AFTER the planned trials are completed, and it should not depend on the P value.","This is difficult to answer because researchers may try other tests because they believe certain assumptions or criteria were not met.",NA,NA,NA
"297","see response to question two previous from this one.","This depends on the covariates have been included, particularly regarding the original hypotheses. If there are covariates used as a fishing exercise, they shouldn't go in.","The only reason to use it is to help publication in journals that demand unequivocal a priori null hypotheses","Sorry, your questions are really complex and hard to digest and I may misunderstand them. If I understood correctly, attempts at treating data with different models should always be reported. My % estimates are not taken to be serious. Frankly I have no idea how frequent that might occur. ","Shouldn't be used since 0.05 is the standard and 0.051 is not 0.05. On the other hand, I'd be happy to see 0.051 reported and feel good about it being a hair away from ""Significant""!","This practice should not be used in order to obtain a certain result. Yet, ""outliers"" or highly influential observations can happen (e.g., based on Cook's distance and leverage). One may then want to perform the analysis and report the results with and without that or these observations.","There are ways of doing this, and re4asons for doing it, that are innocuous, but these clearly are not what the survey authors have in mind.","This is difficult to say as very often it is not clear-cut which method to apply and several methods have been tested. And of course the p-values etc change from analysis to analysis and a better suited method may also give better p-values (eg. non-linear regression often fits better compared to linear because of higher flexibility, but there may be good reasons to choose non-linear regression...).",NA,NA,NA
"298","Selectively reporting of p-values makes the scientific literature uninterpretable.","This depends on what you're doing and what you mean by 'reporting'. Sometimes covariates are planned from the start (e.g. block designs) and others only become apparent during or after data collection (e.g. a realization that an environmental gradient may exist and confound data). In the latter, sometimes checking to be sure these aren't confounding your data is something good to do, and then if they aren't moving on with your planned analysis. It's always preferable to explain all of the steps of your analysis process, however. Often pressure from journals (reviewers/editors) to reduce the length of methods sections by removing such 'ancillary' steps in the name of brevity or concision means this information is omitted from publications.","the problem is reviewer bias against ""found by chance"" type of result","Sounds like data dredging to me. However, exploring data (e.g. using additive models and multiplicative interaction models) is fine - as long as you report when there was no significance (e.g. due to large numbers of coefficients in higher order interactive models)","Shouldn't: it's fraud","This question is distinct from removing outliers however","There is a subtle difference here between validly conducting a trial study to determine whether a study is feasible or an effect is likely to be of interest and a practice used more often in other fields of collecting data until the results are significant, rather than pre-determining the sample number required (e.g. through a power analysis).  The latter is a form of p-hacking and should not be used.","This is forgivable only if the analysis is being improved and one does not stop once an arbitrary threshold is reached. Like the last question, such a procedure will result in too many false positives and should never be performed.",NA,NA,NA
"299","Seriously who wants to see a list of &gt; 100 mutation sites that are irrelevant?  Having said that I will sometimes include the whole thing as a giant supplementary in a paper ","This depends on your definition of a covariate. If a covariate, in the sense that I use it,  does not even have P&lt;0.05, that fact should be mentioned if there was an a priori reason to believe that the covariate should matter. ","The problem is that the unexpected findings could remain unpublished as the reviewers usually requires to be predicted from the start despite these results might be very important for the field of study. Often researchers find something purely by chance.","Stating that you have presented the complete set of tested models when you have not is a falsehood.","Sig figures are always an issue...If the assumption says p&lt;0.05, then anything that is below 0.05 is less than 0.05. Rounding shouldn't change anything","This seems like clear 'p-hacking' and never justified.","There is often no way to collect additional data (PhDs, Masters) due to very limited time. In case there is a clear working hypothesis and study/experiment can be rerun and so add new data I have no objections. ","This is kind of immoral.  More thought should be given to the model and the model's fit before interpretation of results. /  / My first boss told me to check the model's fit before looking at p-values.  I thought (and still think) that this is sage advice.  If the model fits (after careful initial consideration), then the inference is robust irrespective of what it says.",NA,NA,NA
"300","Should be avoided to avoid biases","This doesn't really fit with my work (hence the 'never'), but everything is correlated with everything to some extent, so there is no reason why a researcher should report everything. I have more concerns about whether a P cut-off should be used to make this decision (as opposed to an effect size or something), but there's necessarily going to be some sort of decision threshold.","The purpose of hypothesis based research is that you are testing an assertion made prior to analyzing the data. Even in the peer-review process, there is no need to misrepresent hypotheses as this can be fully addressed in the evaluation of the results.","Statistical modeling requires decisions about appropriate sampling distributions, link functions, etc., that do not warrant explanation (other than specification that the choices are appropriate for the data).  Representation of which independent variables were considered is a different matter.","Significance levels are already questionable. They should always be transparently reported. The ""less than or equal to"" statement should be preserved.","this seems like data manipulation unless the excluded data are true outliers","There is probably limited dispute for this practice as researchers usually rely on pilot samples (and power analysis) to decide on the efforts for the next stage of sampling. Sometimes it could be interesting when the collections of more data render the results insignificant, indicating other important variables, processes and spatio-temporal variability which might have been overlooked.","This is not necessarily as dishonest as it sounds.  Different statistical methods with different underlying maths, might reveal different patterns and relationships due to the difference in the method.  That means that the conclusions will be different but so long as the interpretation is presented in the correct context, this could be a reasonable approach.  Particularly if the non-significant result is reported.   ",NA,NA,NA
"301","Should be used when the negative result is of interest. Should not be used when reporting will only clog the paper with a result of interest to no-one.","This gives a false impression of what is or isn't important. A simple line saying that variables x-y where tested but not found to be sig in any cases is all that's needed. ","The question is not totally clear to me.","Statistical models are rapidly advancing and ecologist are in a process of learning them all the time. There are often many different ways of implementing models and it may take long time and many (statistically incorrect) models until one may reach/discover the appropriate/stable implementation. Its literally impractical to report all the process.","Simply because you lose information. I do not think there are great differences between a test that provides p = 0.054 and a test that provides p = 0.046. In both cases, the risk of type I error is rather high and I think this should be taken into account.","This seems to me like rather serious manipulation of data in favor of a desired result.","There is probably not much to be gained by adding new replicates since that introduces new biases into the analyses (temporal blocking, etc.) that would have to be addressed","This is one of the ""arts"" of statistics - different tests have different power and if you learn of a better  or more appropriate test, use it. E.g., using GLM with different distributions rather than transforming data. ",NA,NA,NA
"302","Shouldn't: hinders scientific progress","This is contextual and depends upon the study design - e.g. for testing very specific hypotheses about phenotypic traits, it would be most informative for a reader to know all the non-significant tests, whereas for testing a much larger number of hypotheses in a genome-wide association study or machine learning model, reporting every non-significant variable is not necessary or expected","The scientific method must be presented accurately, including unexpected results.","Statistical models should be used along with and to support empirical data.","Slavishly adhering to a specified value is unhelpful in interpreting results. Look at the pattern/trend in the data, look at sample size, and decide whether it's meaningful, not whether you hit just the right p-value","This should never be done.  It's okay to look for outliers and points with high leverage, but it should never be done based on 'significance'.","This can be done, although never good, if you forget to do a power analysis initially, and conduct it afterword to prove that you should have collected more data to begin with.","THis is part of the multiple models issue!   If you yous two approaches, report both and discuss how the data meet/violate the assumptions of each.",NA,NA,NA
"303","Significance testing is bad. ","This is justified when the procedure of rejecting non-significant covariates is described in the methods and the number of rejected covariates is reported","There's nothing wrong with identifying an outcome as unexpected. There's something wrong with pretending you knew something you didn't.","Sure, I know of studies or laboratory groups that have a ""pet"" model that is their go-to model. But this may also mean that they never even tried other models. SO maybe they are not being dishonest, just a bit lazy. Again, with modern machine learning techniques, it's possible to take all of these models and compare/contrast/ensemble, but I think this practice has been slow to penetrate into the mindset of lots of lab groups.","sleezy","This should never be used, it is manipulating data without a justification","This can be justified if statistical power/sample size is very low, when it is clear that larger samples would be more conclusive either way.","This is probably a question more relevant to research based on statistical tests than to studies relying on information criteria or just focusing on estimated effect sizes. /  / Rather than changing test, perhaps changing the alpha level would be simpler? / ""No scientific worker has a fixed level of significance at which from year to year, in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas. Further, the calculation is based solely on a hypothesis, which, in the light of the evidence, is often not believed to be true at all, so that the actual probability of erroneous decision, supposing such a phrase to have any meaning, may be much less than the frequency specifying the level of significance.  (Fisher 1956). / ",NA,NA,NA
"304","So-called ""negative"" results are important to know. If they're not reported, then somewhere down the line someone is likely to ask that same question, conduct that same study, spend money doing it, and then probably also not report the results. It creates a system where money is wasted, where the same questions are asked over and over, and also where we have a poorer understanding of the systems we work in because the full results are not reported.","This is not such a big deal, as we often have to deal with a large set of potential explanatory variables, many of which are not informative at all. Removing them will not have much consequences for the final results.","There a different levels of ""unexpected"". So its not a very good question. Some are unexpected at the time of exploratory analysis but seem obvious on second thought or consultation with the literature. This doesn't mean the result was happened upon by pure luck which is what the question seems to include. I have never been so lucky, so my responses to above treat ""unexpected"" as a result contrary to the result we would have predicted most likely. ","surely this will also include running models that you later realize were inappropriate for the data","Some journals require this. It is best, however, to report the p-value to the appropriate level of precision. Some make the opposite error - too many decimal places, exaggerating precision.","This should not be used. This is tailoring your data and that is also lying. The only way this is okay is if you have severe outliers or data points that were obviously measured improperly.","This can be useful as a test of how much power you have, but should be paired with an analysis to point out that less sampling would have given a different result.","This is tricky. There are purely ""bad"" reasons to do this, and I think the blame rests with editors/reviewers who insist on p=0.05 when people just try everything and pick. However, there are often a few totally appropriate ways to test the same data (and these may all be imperfect). In that case, picking the best fitting model seems fine to me, and if that's also the one with the lowest p value, so be it. ",NA,NA,NA
"305","Some studies or variables simply are not interesting, or has been used already. Thus sometimes it does not make sense to report.","This is one of the ways we invalidate science.  One of the worst culprits, I believe, is data visualisation - peeking at potential relationships that then informs future ""tests,"" which are then strictly invalid.","There are very different levels of inference associated with exploratory studies and planned tests. It is misleading to report unexpected findings as though they were planned tests.","Testing many models is OK, indeed it can be good statistics. Not reporting is is not.","Some people round to two figures - but I'm assuming from your wording that you mean someone does it in order to obscure the marginality of the result, intentionality, which would be dishonest. Any poor ethical practice is unacceptable, whether or not it has a real ""meaning"" relative to some fundamentally arbitrary choice about acceptable alphas.","This should only be done if it is apparent that the data that are excluded were errors or could be done if the data were considered to be outliers and both analyses are reported (with and without the outliers).","This can help getting a more representative sampling, and prevent false negative/positive.","This practice can be used if the change was made because new data have been added or some changes in the empirical data were resolved.",NA,NA,NA
"306","Some studies with negative findings may need to be reported if some value can be obtained by reporting such studies. ","This is prevalent - people test a few different co-variates, then tend to not report those that are not statistically significant. It leads to biased inference.","There is a lot of grey here, not just black and white. Without a prespecified analysis plan, it is sometimes a little hard to tell what was expected and what not.","Tests may be used exploratively, and sometimes one realizes post hoc that they are not appropriate. Inappropriate tests need not be reported.","Statistical significance is not equivalent to biological and/or ecological significance.  We should always be careful about using statistical significance to equate the biological or ecological significance.","This should only be used if there is some justification for removing the data and the results are reported both with and without the data point. This is commonly done when there are1-2 clear outliers, and then the results both with and without those outliers are reported.","This invalidates the p-value. Vegas wouldn't allow you to keep spinning the roulette wheel until you win on a single bet.","This practice increases the risk of Type 1 error and publication bias. It is only OK to change the statistical analyses if there are good reasons for doing so that are independent of the p-values.",NA,NA,NA
"307","Some things aren't relevant. ","this is tricky: because in the Results section there might not be enough space to go into details, many people, including myself do not discuss those results. But as long as they are [presented in atable and referred to, then I believe it is acceptable pracice ","There is no need, studies are even more interesting when results don't match predictions.","The absolute set of models tested when conducting due diligence is often huge, and the exact formulations can be trivially different. The exact formulation for random effect structures regarding nestedness versus non-nested random effects, random intercepts, random slopes, are often enormous, but make absolutely no difference to the core hypothesis tested. If under the course of model development these changes in structure and funcitonal form make a difference for the hypothesis test, and the models are of roughly equal likelihood then this finding must be reported. If the quest for a model that well describes the data is robust, and the results of the key hypothesis test unaltered, then presenting the absolute full set of queried alternative functional forms doesn't serve anyone but ink vendors. Sets of alternative models that represent distinct hypothesis tests should be reported however.","Statistical significance thresholds are arbitrary","This should only be used with full disclosure, and when justified by a priori logic/biological complications","This is a reasonable practice, if this implies doing a preliminary analysis of results during an ongoing study, and the intention is to collect more data regardless of outcome.  /  / However, this is not appropriate if the decision is Stop when ""significance"" is reached. ","This practice should not be used if the statistical analysis was initially chosen due to its fit for the specific purpose (i.e., belongs to the experimental design). Changing the analysis only to obtain a different p-value will then pursue to obtain (generally slightly) lower values, which, as I mentioned in my previous answer, is worthless.",NA,NA,NA
"308","Sometimes a negative result may occur when you suspect the study was performed poorly (e.g. the data is low quality) and so the project is abandoned. Negative results should clearly be reported but there is little incentive to do so as these typically end up in low impact, low status journals","This kind of thing has not arisen in my work, and hence I am not sure about how important or not this is.","There is no point. These findings can be presented and discussed as surprising, and including why it is surprising is informative in itself.","The full candidate models should be included in the Supplemental Information, in my opinion","statistical thresholds are personal decisions","This was done in a situation where an anomalous result was obtained. Review of the data showed a trial where controlled conditions varied from all others. This was removed.","This is a total no-no. I hope no one engages in this behavior.","This practice should not be used to get the results that you want... but if you learn of a better technique (e.g., more appropriate vs the data structure, such as potential autocorrelation problems or controlling for some biases), then one may decide to reanalyse his or her data...",NA,NA,NA
"309","Sometimes I have tried a number of moderately interesting ancillary analyses alongside my main ones, and if they yield nothing interesting I don't report them. I think it's good to be thorough, but in my experience reporting loads and loads of NS tests is a good way to get your manuscript rejected, so it's easier to pretend that I never even tried the ancillary tests. I do not leave out main study questions that came up NS, however. ","This practice doesn't bother me, so long as the covariate represents 'noise' in the data that researcher wants to be sure isn't skewing the results, rather than being a variable of direct interest to the hypothesis. If the covariate is not significant (and particularly if the p value is relatively large), this suggests it is unimportant and can largely be ignored for the purposes of interpreting the studies results. Consequently, omitting this result should generally be fine.","There is nothing wrong with results that reveal explanations different to hypotheses. ","The issue here is the pressure of journals to be as concise as possible and thus, exploratory analysis are considered irrelevant to the major point of the topic","That's is unethical. One thing is to round p = 0.051 to 0.05; another is to round any value from 0.0525 or above to 0.05 just to fit in your argument. That is malpractice. ","This will definitively bias the analysis towards some potential favoured result. ","This is a vague question. P value is highly dependent on sample size and statistical power. If you can see a difference, but you need a larger sample size, then it is entirely appropriate to collect more data to see if the trend is significant.","This practice should only be used if an incorrect statistical test was initially applied, in other words, if the researcher has learned more, and discovered a new statistical test that is better suited to the question. ",NA,NA,NA
"310","Sometimes lots of data are collected and tested. Often non-significant variables are thrown out if they're not integral to the story. I think this is okay.","This practice should be avoid, but we all know that it is easier to present significant results so we always select the best (more meaningful) variables","Things don't always go as planned and sometimes you can't answer the question you set out to answer. To succeed you must still publish and unexpected findings can advance science.","The line between exploratory and final analyses is not very clear. Many models belong to the exploratory phase. Besides, very often, slightly different models answer to slightly different questions. If the question is crystal clear and very well defined in final phases of the study, it may be not so relevant to report all the other models that answer to a slightly related question.","That's just silly","This would simply cheat with the results. Unfortunately I am convinced that a small number of us do it, hiding behind the elimination of ""outliers"". But I'm convinced that even outliers have something to tell.","This is also fraud. ","This Q doesn't distinguish the motivation. I often change analysis methods when I realise that there is a more appropriate way to address a dataset.",NA,NA,NA
"311","Sometimes the data is too exploratory","This practice should be avoided in order to avoid data dredging issues (i.e., trying a bunch of different covariates and only reporting the ones that are significant). However, it can be useful in the context of model selection if the models containing the non-significant covariates are explicitly described.","This activity is most commonly used, in my experience, to reduce the burden of reporting and discussing why a result was not anticipated or was unexpected. I feel that full transparency in why data were collected, what the intended tests were from the beginning, and why exploratory tests were undertaken should be reported primarily to minimize P-hacking and to make sure scientists are assessing their bias throughout the process.","The list of candidate models should be listed and if only one is reported a single sentence (supported by supplementary material if necessary) can describe why only one was reported.  Most journals demand economy in reporting results.","That is dishonest","To caveat: if it is used, then models with and without the data point(s) should both be reported.","This is basically the definition of a pilot experiment, no?","this question is ambiguous. I have used multiple ways of analysing data when the first one is non-signficant, when I wanted to show the lack of effect was real and not due to the specific test I was using. So in this case I think it fine to use a variety of tests.  /  / It is fraudulent to test data with a battery of tests and only report the significant one, and ignore the others.",NA,NA,NA
"312","sometimes there are no space on articles to discuss a lot of variables and tests, so you have to chose what to report. In those cases, it is obvious people are going to report only the statistical significant results, leaving the non-significant variables behind.","This practice should never be used because it is associated with higher risk of Type 1 error and publication bias.","This approach is similar to data mining, and should be used with caution.","The other models should be reported on at least briefly or included in the Supplemental Information.","That is not rounding off, it is rounding down! If the cut-off used is 0.05, then that means values below that threshold (without rounding).  ","Totally dishonest, negates the reason why we do this job. Why gather data in the first place if it is to do this afterwards? This conduct is similar to data invention.","This is common practice in science. It is called ""pilot study"". See, for example, Leon, A. C., Davis, L. L., & Kraemer, H. C. (2011). The Role and Interpretation of Pilot Studies in Clinical Research. Journal of Psychiatric Research, 45(5), 626–629. http://doi.org/10.1016/j.jpsychires.2010.10.008.","This should be done only if/when there is supporting reasoning that the initial analysis might have been flawed or unsuitable for the type of data or hypothesis being tested. The choice of a different, new analysis needs to be justified and validated",NA,NA,NA
"313","Sometimes you realize that the variable is simply correlated with another variable that you are reporting on, so it is not particularly contributing to the story to report both (or several correlated variables).  Journal space/reader patience is limited.","This practice should not be used because P-values are a continuous measure of compatibility of data with a null hypothesis. The 0.05 threshold is arbitrary and meaningless. Statistics to report in an experiment should be chosen a priori and then reported in full, irrespective of P-value and effect size. Only this enables unbiased meta-analtyical accumulation of evidence on a phenomenon.","This approach should not be used using in conjunction with hypothesis-testing statistics. However, if the original prediction was wrong, but this was only discovered afterwards, then there's something to be said for this (given pressures to publish and clarity of manuscripts). But this should only be done by reporting correlations.","The poorly-fitting models also contain important information. However, sometimes for lack of space or a desire to make things less complicated, these will be left out (or at least, fewer details provided on the model fitting).  In addition, sometimes two kinds of models are about equally as good, and reporting the same result, so it seems less crucial to include all models in that case.","That seems straight out dodgy","Unethical and misrepresents findings. Outlier tests should be done, but results with and without outliers should be reported.","This is context specific. Often the collection of more data , especially in field-based ecology experiments, is not possible","This should not be used. This is essentially lying about your results. The only way this is a valid approach is if you find a model or statistical technique that better represents or encompasses your data set (i.e if you were using something like a linear regression and then decided that a mixed effects  model was better because of grouping or levels in your data). ",NA,NA,NA
"314","Statistical association is not a cause-effect association. Statistics are most of the time exploratory in Ecology. It you don't find association between two variables, they help little in the abduction process leading to the generation of new explanatory hypothesis. Why should you report everything you do to explore your data?","This practice should not be used because there is risk of presenting an inaccurate one sided story. With the use of online appendices, there is really know reason why messy results that are not very interesting can't be placed here.","This cannot occur if an a priori hypothesis is developed honestly and rigorously. / ","The problem is in reporting it as the ""complete"" set when that's not true. People explore data, but hiding model testing inflates significance of results.","That was done more often in the days when people used tabulated P-values; now that more precise values are reported by nearly all statistical software, people report the values that they get. ","Unless it can be justified that exclusion is based on clear outlier property (e.g. errors in the measurement device) this procedure is not at all scientifically valid and acceptable. Sometimes, however, it may make sense to report statistical tests both including and excluding specific data points but this must be done for a good reason and openly.","This is good practice. It forces researchers and readers to think beyond statistical significance (which can always be achieved if we collect enough data) to scientific significance.  The only case where it should not be used is when a proper power analysis has been performed at the outset, and the planned sample size has been achieved.","This should only be done if you have a robust reason to switch, such as realizing the data is not normally distributed, or needs a log transformation or something.",NA,NA,NA
"315","Statistical significance is not the only way to judge the merit of a study. If the result is not significant, it still worth to be reported if the effect size is large, same to the variable. ","This practice should not be used once a hypothesis has been fixed upon, but I can see how it inadvertently happens when people are just playing with data before they have have perhaps decided what they will try and test.","This doesn't bother me-if it is unexpected  and can explain it. Who cares if someone says they knew that at the start.","The question is a bit confusing. If I understand it right, you're talking about running tests and then not reporting them.  If so, then that practice isn't very sound.  They should at least be mentioned - if only briefly - in the supplementary materials.","the actual p-value should be reported! let the reviewers/readers decide how to interpret. ","unless such observations are statistical outliers, this is bad research practice","This is not ideal but sometimes I have collected more data to increase my power to detect if marginally (non)significant effects are important or not","This should only be used as part of exploratory analysis, where the non-significance is combined with an assessment of whether the test is meeting the relevant assumptions. If the test is the most appropriate for the data then it should stand - it is unlikely that an alternative test would be significant anyway.",NA,NA,NA
"316","Statistical thresholds should be used for hypothesis testing under appropriate experimental design and statistical models should be used for observational studies common in the field of evolutionary biology rather than threshold ","This practice should not be used. Goes against rigorous reporting of results. Can bias effect sizes (e.g., for meta-analyses).","This has to do with structuring a paper to make it more readable. I don't see why this can't be done. ","The question really hinges on the term ""candidate model"".  It is very common, and I believe acceptable practice, to conduct simple preliminary analyses (e.g., simple parametric analysis in lieu of analyses that correct for spatial auto-correlation or random effects), and then to conduct and present for publication analyses that are fully correct but more complicated.  It is not acceptable practice to engage in statistical ""fishing expeditions"".  I frequently engage in former sort of practice in the course of research.","The actual values does not matter very much as they are very similar. HOWEVER, people tend to interpret p-values and black-and-white and thus such practice should influence many people and, in this sense, it is not advisable.","Unless the points are obvious errors, this is dishonest","This is not statistically sound to conduct several tests on data sets that are not independent from one another","This should only be used when that previous statistical analysis was objectively incorrect. Even then it should be reported 'vaguely' in the discussion of the paper.",NA,NA,NA
"317","Studies that do not show conclusive or significant data should be published but researchers often hold off publication until they can repeat the experiment with a larger sample size...which is often what I do, but then fail to ever publish the study.","This practice shouldn´t be used because hiring information on what was not significant increases the chance of statistical erros and interpretations.","This is a matter of reporting style. Some journals/referees demand that all results of importance should be given sufficient background in the introduction. As a result authors may be obliged to provide details of the unexpected results in the introduction, which may appear that one is providing a post hoc justification of the results. Perhaps a better approach is to report at the outset within the background that some unexpected results were uncovered during the study.   ","The reality of model fitting is it best laid plans often don't come to fruition.  There have been many times when I had to reconsider the model set because a certain variable or set of variables was not fitting well (i.e., not converging, not yielding standard errors, etc.).  I am not sure whether it is worth noting these minutia or if the average reader cares.  I believe we all realize the best way to go about it, and also realize the reality is that isn't always possible.","The exact probability should be given or at least say p&gt;0.05 but not p=0.5 if it is in fact p=0.054","Unless the points are outliers and the authors clearly state they remove (all) outliers according to a specific existing criterion for a specific reason. But I wouldn't be surprised if the criterion is chosen after inspecting the results...","This is often criticized, but I feel unfairly.  There are two reasons why a test may be insignificant - it's not true, or the data lacks statistical power.  In the latter case, adding data should allow you to distinguish between these two options, and assuming the new sample comes from the same distribution as the old sample, this should not bias the P value.  That said, going after P values doesn't make much sense generally.  In a Bayesian setting, you might just get large CRs, so adding more data points would just reduce these. Adding more data points seems less fraught in that setting.","this should only be used when the lack of significance is reasonably thought to be due to power limitations in the non-significant analysis",NA,NA,NA
"318","Studies that don't have statistical significance often just don't get published. Where specific variables are tested for significance and they are found not significant that should be reported if its a salient part of the research question.","This provides additional information about the system under study and should be published together with main results.","This is a personal bug-bear for me and one I rail against in my stats classes.  Unexpected findings from exploratory analyses can and should be reported, but they should be reported as exactly that: unexpected exploratory findings. Post-hoc hypothesis maniuplation is probably a major contributor the replicability crisis in science. ","The reason it is difficult to report is that results from many a models may not make any sense and the !@#$%^&* reviewers then get stuck on those and refuse to see the ones that do make sense. So if you do not curate your results, you run a very heavy risk of getting rejected. It is a sad but true state of affairs. Fortunately, I do not do multi-model inference, but I would sympathize with some one who has to do it.","The exact value of p isn't important. It's a bit ridiculous to report p to more than one significant figure.   Giving more digits clutters the discussion, it's a distraction.  /  / The only time rounding is important is around the magical p value of 0.05. ","Unless there's a legitimate reason to suspect something may be an outlier, it's borderline research fraud. ","This is often done in pre-tests to check whether experimental designs are ok. ","This should only occur if the first analyses were justifiably not the best test",NA,NA,NA
"319","Studies that fail to show a statistically significant effect are much less likely to get published (even if the power of the test is high) - this is unfortunate, as it biases the literature. /  / Conversely, when variables fail to achieve statistical significance (within a study with an overall significant result), I believe these are usually reported.","This question is similar to a previous one: no result is also a result and should be mentioned to prevent publication bias.","This is an interesting question that I have several thoughts on.... /  / Firstly, whether a researcher presents unexpected results as consistent with there prior expectation or not does not effect what the research result actually is. It may only alter how the researcher is perceived by other scientists...so I see this as a form of dishonesty as only carrying quite mild consequences, from that perspective.  /  / Secondly, it is, however, dishonest and offers insight into the researchers values. Specifically that the researcher values their appearance more than their integrity. We may wish that our fellow scientist (and ourselves) were more unwavering in there commitment to honesty and transparency. /  / Thirdly, I think this behavior actually achieves little for a person. Unexpected results can be fun and exciting. Why not bring that forward and let your love for discovery and your curiosity be what you share. These are some of the most inspiring qualities. /  / So, to answer your question more directly.....The practice should not be used because it is dishonest and compromising to a persons integrity.  /  / Hope this was helpful to your study! / ","The reported results are almost always a biased selection of the full set of results. Therefore, it shouldn't be used becasue it will contribute to biased reporting of evidence.","The example is a bit trivial, but the precision of the measures used also dictate the precision of the probabilities that can be calculated. Just because I can calculate a p-value of 0.000346 doesn't mean that my small sample size really has that amount of power or precision. ","Unless there are compelling reasons that indicates that data points are suspect (eg known problems, typos, etc) they should not be excluded.","This is often problematic. In my own work, I have done this with ""blocks"" - e.g., I collect 10 more data points. Mate-choice work is notorious for collecting data 1 point at a time, and that seems horrible to me. However, there are ethical issues when using live animals. For example, I have studied cannibalism among live siblings, and it would be irresponsible to run substantially more trials than needed in that context. It would also be irresponsible (and disrespectful to animals already used) to leave a question essentially unanswered (e.g., p=0.08) when collecting 10 more data points would resolve the matter one way or the other. ","To be used only when there's a good a priori justification for the second analysis over the first.",NA,NA,NA
"320","Such a practice dramatically biases our view on the relevance and importance of certain ecological patterns or processes relative to others.","This question strongly resembles the first question in this series.  I can't estimate the proportion of people who do this.","This is discovery! Why would you report it as an a priori assumption?","The results are biased and overoptimistic.","The first is clearly wrong.  I'm more ambivalent about the second, as people usually discuss 'significant' vs 'non-significant', rather than gradations below this.  That said, the whole 'P = 0.05' threshold is clearly a bit dodgy.  Again, I'd rather see emphasis on effect sizes, or the whole problem reframed, and analyzed in a Bayesian setting focusing on certainty of model parameters, or something.","Unless there is a justifiable reason (equipment malfunction etc.) this is obviously completely unscientific.","This is often useful when running exploratory experiments, to see if it is worthwhile to continue. Provided the additional data are collected under the same conditions as the initial, then combining the two would seem acceptable. If the data are not under the same conditions, the new data should constitute the true and full data set.","To check the robustness of the results of the first selected test",NA,NA,NA
"321","supplementary material for all tested results not in the manuscript","This shouldn't be used to ensure full transparency in the model building process.  / I don't believe it is necessary that a huge amount of detail is required for non-significant variables in a paper, where there may be many covariates. Simply reporting that the variable was generated, tested, how it was tested, and the alpha threshold (e.g. p&lt;0.05), and the estimated p-value should be stated. / If seen as overly cumbersome, perhaps adding these details to the supplementary material could be a solution","This is dishonest and cannot be tolerated in the scientific community because of 1) principle of integrity; 2) risks of false positives. / I think it is okay to not mention whether a result was expected or not; but in general better to be clear about what was expected. Actually, these results going against initial hypotheses are a major spice of scientific investigation, and it is sad not to report them as such.","The use of this depends on the statistical methods. When trying to predict a factor based on several environmental covariates and their interactions, I think it is reasonable to present only the models that provide the best fit (and of course, explain this in the data). /  / Also when statistical models have been improved, the presentation of earlier models is unnecessary. ","The idea that P values are meaningful to 3 decimal places, or that the magical threshold of 0.05 is anything other than arbitrary, is all a bit silly, so it's a fairly innocuous practice in the grand scheme of things (though still dodgy).  ","URGH - again I was taught to do this as a student in stats class. Excluding outliers. NOPE. Whether you exclude them because they just look odd or whether it's because you know their impact on the stats - it shouldn't be done unless it's part of a carefully conducted and transparently reported sensitivity analysis.","This is sometimes difficult to do in ecology as your study area or studied organism is thousands of kms out of your desk. In any case, new data, if they are incorporated to the old ones, I consider it is correct to do it as it multiplies the replica. It should not be used for filing your results.","Trying a range of methods seems appropriate given the number if methods available these days",NA,NA,NA
"322","Sure - some things you try turn out to be not significant and maybe it was because of an experimental design flaw, and then the results truly aren't interesting. If the experiment was fine, then it becomes a file drawer problem, BUT it's hard to publish non-significant results, so many, many studies end up unpublished. (I think this is a flawed question, because it's a different matter with variables - those should be reported).","this skews the statistics significantly. ","This is flatly wrong. ","The way you formulated it, it sound like cheating. One thing is not to report all your previous explorations. Another thing is try to convey that your set of models is complete when you know it is not. ","The idea that something is significant with a p value beyond some arbitrary value is absurd. You should quote the p value and let the reader decide its significance. ","We justify it by saying that are outliers with very little significance on the overall results but that introduce too much variation","this is the point of a pilot study. I see nothing wrong with it.","unfortunately this practice is likely more common that expected, especially when performing multivariate analyses. Not a good practice; one should go with the most suited, powerful, and conservative method, not the most ""p-value &lt;0.05"" friendly ",NA,NA,NA
"323","That will be a new approach to solve the problems!","to be used only if the covariate is also biologically negligeable (not only statistically). / sometimes it has a biological meaning to mention it even when not significant","This is highly unethical and reflects a profound misunderstanding of the scientific method.  People do it all the time, but I think one needs to be extremely diligent to describe the motivations for a study as they truely were in an introduction, and then in the discussion one can rethink their understanding based on unexpected results.  ","There are an infinite set of possible models in most cases. All of them could never be tested and that is not an expectation of most readers. ","The importance of this may be over-stated, since the differences in weight of evidence associated with p=0.051 versus 0.049 is very small. However, I'd prefer authors report the actual value, or, if they do round off, that they describe significance as ""marginal."" ","Well, an outlier might tell a story of its own, and if excluded should be always at least mentioned in the methods.","This is the standard solution for failing to reject a hypothesis, collecting more data. ","Unless there is a sound theoretical reason for the null effect, I think its entirely reasonable to adjust or apply new tests to suss out a signal, particularly if subsequent analyses treat the data more rigorously (e.g., modeling temporal or spatial non-independence, non-linear effects, etc.)",NA,NA,NA
"324","the answer is basically the same in 1 (but I'm trying to avoid P-values...). ","to reduce size of result section","This is how we are trained to frame our science and subsequent papers; here is the theory, here is what is expected, here is what we found. But sometimes you really have no clue what to realistically expect although you may have some ideas on what could affect your response variable. Sometimes you need to try something that is different and see what happens. This is the exploratory side of science. I would love to write a paper saying ""I had no idea what to expect because this systems is so new/so strange/ so amazingly weird that I HAD to try something"". Also, sometimes you have a lot of predictions (like the direction of effect can go either way depending on the mechanisms driving the response). And then, you may have many response variables as well, and hence many predictions. I do think if you truly see something unexpected then you should report it as such, but sometimes I can see how a scientist may look at their results and go ""oh, well that is different, but I may have/ should have expected that"". I think the point that I am trying to make here is that sometimes it may be difficult to interpret your results and sometimes it may not be entirely clear if you should have predicted those results, or if they are truly unexpected. I feel that designing and conducting an experiment sometimes leads a scientist (and their team) in logic circles, and in the very long process from experiment conception to peer-review publication, a scientist can get confused on the some of the fine divide between predicted with confidence and alternate predictions.  /  / ","There are clear statistical concerns about testing models and only reporting some results. However, with complex datasets it is sometimes necessary to try a few approaches before settling on a final set of candidate models for results of the study. I think if researchers are aware of p-hacking and other potential pitfalls, this process can work, but I agree that in a perfect world it would be nice to present all models tested in the reported results.","The most important is to report and show the data without any transformation. If the data shows a clear pattern, the p-value is just the result of the statistical analysis.","Well, this is fraud - in some cases, however, it is useful to check whether statistical significance is driven by just one or two points, which would cast doubts upon the results","This is tricky, because it is often difficult in evolutionary biology studies to conduct accurate power analyses that will let one determine ahead of time what the sample size should be. At the same time, conducting studies can be expensive, so one doesn't want to collect more data than are needed to reveal an effect, if one is present. I think this is why people will sometimes test whether they have a significant effect yet, and then decide whether to collect more data. Obviously, if no effect is there, collecting more data is unlikely to reveal an effect. The danger is rather that people will stop collecting data prematurely and report an effect that would disappear if they collected more data.","Unlike my general sort of 'it depends' attitude towards your earlier questions, unless you can justify a different approach because the first was inappropriate or less appropriate, I don't condone this behavior. / I've had a colleague ask me for advice about this. They said they had a stats question and described their data. I said I suggest you do this. They said, I did, but the result was not significant. My jaw hit the floor. But then they said they needed another publication that year, it's important for tenure, etc. It did not excuse their inappropriate analysis, but did explain their motivation. ",NA,NA,NA
"325","The fact that non-significant results are not as often published surely create biase. Usually, if I have clear hypothesis, and funding received I of course report non-significant results, but in less prestigious journals, maybe. When testing novel idea, I admit that idea might have been abandoned if data is not supporting the idea. It's not ideal, but since publications are the merit one tends to focus on those studies that one knows to ger published.","to save time and publication space","This is interesting...It may be someway ""forced"" by a trend towards reporting studies always as being ""hypothetical-deductive"" when, in fact, many are exploratory, frequently in evolutionary biology. Some journals ask for hypothesis and so on, and authors may be constrained to do so even in exploratory studies that may be important.  /  / One should ""assume"" that an study is exploratory, but the issue is that sometimes this is interpreted (wrongly, in my opinion), as bad science.","There are usually multiple ways of testing an idea. After trying several ways, one usually descends on sets on models that seem to give the best representation of the data. I think it makes sense to report only that set. I stress that the 'best' set is not always the set that gives th emost support to the author's favourite hypothesis. ","The p-value (and rounding-up/down policies) should be specified in advance. Results that meet those values are significant, results that do not are not significant. There should be no massaging after the fact.","When an outlier have been properly identified (and defined) one may exclude it to learn about the rest of the data. This practice should always be reported.","This is useful in pilot studies or short term studies when results from the first year might be ""significant"", but the researcher is unsure whether the pattern is representative of larger scale trends. The key is to not change any of the ways the data is collected.","Unsure what is precisely covered by the term 'switching to another type...'. I have, for example, switched from a multi-factor analysis to a single-factor analysis when the multi-factor analysis wasn't giving any insights in to the variable of interest. However, I did report that I did this, and reported the outcome for both tests.",NA,NA,NA
"326","The file drawer is no doubt full of non-significant ecological results. I have a few analyses that I conducted that were quite exploratory and that did not yield anything significant. Those have been written up in part, but are very low priorities on my to do list.  I have considered simply posting PDFs of pre-prints to a pre-print server regardless of my intention to submit them to a journal just to get those ideas out there...","Transparency in what was tested is required, so that readers can see what parameters and therefore hypothesis have been eliminated","This is just a story-telling strategy as far as I'm concerned. The goal is to communicate a real, empirical finding, and often that is best done in a conventional story-telling format which begins with a set of predictions and then tests them. If you begin your story with a set of predictions that are orthogonal to your main results, that will probably just confuse the reader unless you are a particularly gifted writer.","There is a thin line between taking the time to explore your data and selecting the analysis that best fits your hypotheses. The first is acceptable the second not. It is always best to present all models, unless you have good reasons to exclude one.","The p-value carries little information (in particular none about the strength of an effect). Rounding it thus hardly does any harm. Following the rounding convention should be abandonned because it is an uncritically followed convention with no benefit, but negative side-effects, should the data be used in meta-analyses.","when case is well supported and improve model fit (residuals). Outlier for biological reason (i.e. female not investing in gonads growing larger but being this outside the scope of the study)","This is very tricky terrain. Repeatedly collecting small batches on a small study until the p-value wanders across some threshold is obvious p-hacking. However, if your study was underpowered because the effect size was much smaller than initially predicted, collecting a large amount of additional data all at once (say, doubling the sample size of the study) is fair if this is explained transparently in the resulting publication. This would be unlikely to generate a false positive, and the effect size estimate can be compared between the first and second batch, and the combined batches (say, using meta-analysis approaches within your single study).","Use of multiple analysis methods is fine, but should have nothing to do with results of an initial analysis (my opinion)",NA,NA,NA
"327","The issue here is that studies with insignificant or inconclusive findings are hard to publish, so are often not reported. Of course, it would be better if we published all studies' findings (at least somehow), and, more importantly designed studies in the first place to have the statistical power to detect the effects we are interested in.","Transparency. Covariates that may influence the response variable can be tested and removed from the models, but that process should be reported clearly in the methods section.","This is poor statistics but has often been forced on me by senior co-authors and editors/reviewers.","There is always more than one way to analyse any dataset. Being wedded to any single statistical method is equally as inappropriate as trying them all until one yields the results one wants. A good and fair analysis should be willing to throw away models based on how well the data meet the model assumptions, but crucially this decision should NOT be made based on the significance of model variables. ","The p-value should be presented in full, or with the corresponding value of the test statistic. Although the author writing the paper selected the alpha value a priori, printing the raw p-value would not actually diminish the findings, except for not being able to use the words ""statistically significant.""","While data should never be removed to increase significance, it is extremely good practice to remove data points that are evidently outliers and could lead to falsely significant results","This practice is acceptable in cases where the first dataset may function more as a pilot study.  The researcher may have a) limited funds/resources and/or b) limited understanding of sample variability and upon collection of an initial dataset that is statistically insignificant, have a better understanding of the sample size required to obtain sufficient statistical power.  ","Used it because results were contradictory with biological facts. Did it after additional thinking about the adequatie of the method used first. / have no idea on how other collegues proceed. so left the % on 0 ",NA,NA,NA
"328","The more tests you perform the higher your chances of getting significant results by chance. Also negative results may also be interesting.","Two reasons. (1) It leads to publication bias and (2) for multi-covariate analyses, it is often better to use information-theoretic approaches (i.e. no hypothesis / significance testing in any case.","this is post-hoc story telling, which is fine if it is describe as such, by making the prediction and then shown the result is much more sexy.","There is no one approach to analyzing a relationship between a set of variables (e.g. regression, vs maximum likelihood models vs ordination, etc). Sometimes it takes some trial and error to figure out the best approach for a given data set. Similarly in field data independent variables are not always truly independent from each other and one needs to run some analyses to figure out which of two or more correlated variables to include (e.g in a habitat model do you use maximum daily temp or average daily temperature).  Given the emphasis put on brevity in academic journals, one should report only the most insightful analyses, although one should ideally mention in the methods section that others were tried and give the reason why the one reported was chosen for the final analysis. ","The rounding is trivial. But the underlying assumption that a specific p-value threshold determines 'significance' is highly troublesome.","While some of the reasons for excluding the points may be legitimate, this ultimately devolves into p-hacking.","This practice is problematic as it increases the risk of Type 1 error and publication bias.","Usually if a simple model does not fit well I would develop a more complex one with covariates ",NA,NA,NA
"329","The problem is word limits that prevent a complete discussion of all the tests applied ","Under a framework of hypothesis testing it should always be reported. However in some modelling studies it is normal (and legitimate in my opinion) to do some data exploration which is not reported in the final article.","This is post hoc generation of hypotheses & predictions. It's poor experimental design.","There is no reason to limit data analysis in this day and age, with functionally unlimited capacity to report and store online data and analyses via Supplementary Material for most journals. Selectively reporting statistical models should only be done in a transparent manner (i.e. state that you have only shown one model even though others were tested and justify this approach). ","The threshold is a thumb rule anyway. So no point in rounding it off. That is stupid.","Why not save time and money and just make up all the data? Terrible, terrible practice. Only the most banal of human detritus would do this. ","This practice leads to statistical significance overshadowing effect sizes and biological significance. A tiny effect size will be significant with enough data collected but may not be biologically that relevant.","Usually if the original analysis doesn't come up with strong results/patterns subsequent analyses won't either- it's a practice I have abandoned. ",NA,NA,NA
"330","The problem with this is that reporting negative results is quite difficult to achieve as editors and journals are reluctant to stop at the negative result and is always looking for alternative explanations which in practice push researchers to publish only consistent data","Unsignificant results are also results. ","This is tricky - finding an unexpected result can be genuine eureka moment of great insight. / However, in my experience, these findings can also represent genuine errors in recording or analysis. /  / I think unexpected findings, that appear to be genuine from the dataset collected, should be reported. However, the results should really be verified with future additional follow-up studies. Perhaps that unusual finding did occur, but it was extremely rare, and perhaps not meaningful from a population perspective. /  / I think there is a problem in ecology where the 'sexy' findings are promoted, and they often find their way into high ranking journals. Other solid pieces of work often languish in 'specialist' journals.    / Open access journals which do not accept on the basis of the 'significance' of the findings is starting to change this problem - in saying that, I don't think one would get in Nature with a 'solid but unsurprising result'.","There should be complete transparency and repeatability in any analytical work. It is normal to try out a set of different models but you would report the fact that you had done this, and not hide it","The threshold isn't important - it doesn't matter if the p value is 0.056 or 0.053 - because the statistical meaning is essentially the same (i.e. if you ran the experiment 100 times, assuming the null distribution, you would get the observed result 5.6% or 5.3% of the time, respectively). However some reviewers or editors may stick to the old idea of 0.05 being an important cut-off. ","Will bias results ","This practice should not be used because the meaning of the p-value is then denaturated.","Usually, all statistical tests are required to be reported by reviewers (particularly those commonly used in the field). Therefore, I think it is acceptable to try multiple different statistical tests - none of which is a panacea - as long as all of the results are reported and evaluated in the discussion. I do not agree that a test should simply be replaced by another (it was difficult to capture this nuance in the (a-c) answers of this question.",NA,NA,NA
"331","The proper question isn't whether or not it SHOULD be used, but rather when does it make sense to use it and how is the information that it is used described in the article.  p-values have limited meaning and obsession about their use can be counter productive.  ","Using significance testing for model selection is not good in the first place, and again one should report what one did. ","This is tricky. Again, reviewers want you to go back and add different variables to your existing models, test new models and theory, etc. So, while one may have principles of best methodological and statistical practice, several rounds of review and the need to publish often trump the best of intentions. And in some cases, it may be a year or two from the time when a manuscript is first submitted to one journal to when it is actually accepted in the third or fourth journal. None of the coauthors remember what the initial version was like.","this approach should only be used when the other (unused) models were discarded because they weren't appropriate analysis tools. ","The threshold of 0.05 is human-made. Reporting the actual value lets the reader decide on the statistical support. ","you can't pick and choose which data to include","this practice would arise when sample size calculations cannot be reliably done before undertaking the study (in order to calculate power) through a lack of knowledge of the system.  This is unlikely to occur when studies are funded through time limited grants - there simply is not enough money to do this.   / I am involved in several long-term studies and we undertake analysis and publish results when funds become available for this.  It is a consideration of funding more than statistical significance that dictates when write-ups occur.","usually, I do this if the results are close. Sometimes I see if different data transformations that BOTH make the data more normal and provide a better model fit are useful, especially if our p-values are circling 0.10 with raw data. ",NA,NA,NA
"332","The question is ambiguous. I suspect you mean publishing work but failing to report results or variables that didn't reach significance. This is BAD because it's concealing the truth and will bias the literature. However the question could also be interpreted as failing to publish an entire study where nothing reaches significance -- this is bad, and leads to the file drawer problem, but you may have no choice if journals reject your paper because they have their own agenda to only include 'exciting' results (or you just haven't got the time to write up work that has the conclusion ""we're not sure""). That's why I've answered ""It should only be used rarely"" -- you shouldn't ever do this, but you may be a victim of circumstance and time pressures.","We all look at teh data but don't necessaril need to report all teh ways we looked at it","This looks like deception to me","This can be useful for validating assumptions and fitting a proper model. But it should never be used to test a model that just happens to have a low p value. P hacking is a really problem. ","The way this question is phrased is prejudicial. p&lt;0.05 does not imply p&lt;0.050. Significant digits should be considered. They question implies that reporting of only 2 significant digits is an attempt to ""meet a pre-specified threshold"" which is not true. It is a numerically accurate practice.","You can remove data points but justification should be based on the questions being asked and the validity of the data. However, some people may coincidentally test both data sets and that may be unavoidable.","This questions strikes me as odd because I think most people STOP collecting data if they find a significant relationship!","Usually, there are different statistical alternatives for doing a given analysis, and each have their pros and cons. I see no problem in testing different statistical methods, also as a mean to understand how they behave with real data.",NA,NA,NA
"333","the question is misleading: you've probably have meant the issue of multiple testing, whereas as stated it is a question about not writing a paper if there is nothing significant","We dont see negative results Published","This might be OK if it was predicted by others.","This crosses a line from framing a story to lying about the methods.","There's no need to do this, and I always state p-values to three significant figures. I get suspicious when I see it done and discourage my students from doing it (and I know they are taught to do it by some staff...)","You cannot clean up your data to better fit your initial expectations. You are cheating yourself and the community. ","This really depends on the hypothesis and the lab resources. Many cannot afford to continue sampling if results are not the expected. this is not ideal, but its unfortunately the truth. On the other hand, I always thing that collecting more data is positive, regardless of the statistical output","we may force our data to meet ""statistical support"" but the sample sizes we use are always well below of the real populations and thus we must be extremely cautious when supporting hypotheses",NA,NA,NA
"334","The question is perhaps mis-worded, as it doesn't distinguish between analysis that is exploratory vs academic dishonesty. I am answering assuming the former. While I don't think this ""should"" be done, it is an inevitable consequence of research, even when being careful. I cannot report every time I push enter in R and find a non-significant result while I am figuring out how to analyze my data. That simply isn't possible. I strive to be honest about so-called ""fishing expeditions"" and I would never bury a negative result that I felt was representative of honest exploratory effort. But lots of times I find a negative result, realize I did a test wrong, and change to another test. If I did a study that involved a lot of exploration, I would report the extent of exploration/fishing.","We have to avoid the relation significant = interesting and, of course, not significant = not interesting. This is more or less related to the preceding question and the previous understanding of your study.","This practice can sometimes allow for clearer communication of results.","This depends largely on the nature of the tests. Cherry-picking results to find the model supporting the preferred answer is clearly problematic. However, when dealing with complex data sets it can often be useful to try out various models to get a sense of how well they handle certain features of your data, and what variables might be important to consider. Under this situation (and assuming a certain level of caution is used), it can be beneficial to try multiple statistical models with out necessarily reporting those that work poorly and are uninformative.","There are better mathematics than simply the use of a p value. It is stretching the truth to modify the value to a better outcome for the authors. ","You need to have a very solid and justified explanation why values should be excluded. In my opinion it can only be done when the value can be excluded due to methodological reasons, e.g. obvious errors etc. But it cannot be done after looking for the significance level first...","This seems okay as long as the experimental design and analysis are still appropriate to the particular research question. Often a power analysis may reveal that sample sizes are too much and so an experiment gets repeated, or more sites added, to provide further support one way or another for a marginal p-value.","We should understand which test is the best one for our data a priori. And then take the output as the correct one. / The problem is that normally people does not understand the purposefulness of the test, and if the result doesn't like them, then they change to another test. / A more deep-understanding of statistics can maybe solve this (at least partially)?",NA,NA,NA
"335","The question is why a study failed significance. Maybe the original hypotheses were flawed or the methodology inappropriate. If the lack of significance indicates it could be either of those cases it may be best not to report in sight of the given uncertainty. Other times, journals based on peer reviews may simply not accept this type of studies as relevant.","we should have standard to have judgement ","This practice is misleading as it implies that a finding was based on a pre-determined hypothesis when in reality it was an unexpected outcome.  I think it is useful to report and consider such findings, but only in an honest context.","This does include things like choosing a transformation based on the data rather than a priori considerations of the biology. This is not great (inflated type I errors again) but isn't reall a sin if you say why you did it.","There are far more important things to worry about than the degree of rounding","You should always present the result on the entire dataset and then, eventually, present the results after eliminating some cases. Of course, you should motivate your choice of eliminating these cases.","This should never be used but always will be in ecology where a single person typically collects data over multiple years.","Well it depends why you made the change. Perhaps the original test turned out to be completely inappropriate. Yes, you saw the P value but you probably aren't making your decision on that basis. Furthermore, if you think a result will be significant from visual analysis and then it isn't, then you might look deeper into the technique and realize it is not the correct one.",NA,NA,NA
"336","The results are biased and overoptimistic.","When analysing data, you need to decide whether a question/variable is meaningful before looking for significance.","This practice is not really a problem. Though in reality the more direct statement that the researchers were surprised by the findings is a viable alternative.","This is a hard question - what does it mean to present the tests as ""the complete tested set""? I have not always presented all tests conducted. In most of my articles, this would be literally impossible. There are thousands, and we don't keep strict track of all of them (for example, if we try something and it is clear that the specification didn't work). However, I always make it clear that there are more tests than can be presented. Some journals make this hard by not permitting us to say ""data not shown."" However, I am very careful not to pick and choose the results so as to present a specific story that would not clearly emerge if all the results were considered together. I view it as part of my work as a researcher to conduct large numbers of tests and distill them down to their core message for readers. I am frustrated by the standard article format which seems to force me to present the research as if I had clear hypotheses going in - most of my research is exploratory. But I don't think anyone reading my papers would ever feel I was the least bit dishonest or misleading if they had access to every single analysis conducted.","There are rules for how many digits to use, mathematically. They should be followed.","You should jsutify why you exclude data independent of its statistical effects. ","This should only be used if there is a cost-benefit concern and the initial data collection was part of a pilot study","Well when there are several reasonable methods, it's OK to try them all, but I would say if some find significance and some don't, you should report them all.",NA,NA,NA
"337","The set of variables with a small test statistic (a big p) is very very much larger than the set with a large test statistic. I will not report all the variables with a small test stat.  /  / Statistics is done in science as support  tool for exploration and phenomenological understanding","When reporting a multivariate analysis, it is important to specify all the variables that were tested, and which were significant and which weren't. That said, I have done modelling analyses, where preliminary runs included everything including the kitchen sink. The final models presented in a paper might only include the significant variables. I think I have usually  reported the non-significant variables not used in the final model.","This practice is very problematic as the probability of rejecting a significant p-value is zero.","This is as much a philosophical problem as it is statistical.  / Is there only one 'best' model? Are there many competing 'good' models to compare and contrast? Do we care about informative, but low ranking models? / Working between two field - ecology and vet epidemiology - I see very differing perspectives on this problem. In ecology, there has been a recent trend in multi-model inference, presenting a number of highly ranked models, and the use of Information Criteria (AIC, BIC, QIC, DIC) to rank models.  / It may be impractical to report all models run - in fact, with a large multi-variable analysis with interactions, you may end up with hundreds or thousands of competing models.  /  / In epidemiology, there appears to be a more conservative, 'traditional' approach where parsimony is highly regarded, with approaches like forwards, backwards and stepwise model building approaches are still used.  Nested models are compared with likelihood ratio tests, and a small set of competing models maybe compared with AIC/BIC etc. However, generally the 'best' final model (or perhaps, the top 2 or 3 models) may only be reported. This appears to be to do with the goal of the analysis.  /  / Sometimes, I think [this is just an opinion], ecologists might be happy to be exploratory and open ended, whereas veterinary epidemiologists may desire to be more 'definitive' in the outcome, due to the pressures of the demands of their stakeholders [e.g. governments, vets, health professionals, farmers]","There are rules of how to round number. So follow them. However, in the text it has to be clarified that the values were rounded to so and so many digits.","You should think first about what to include in an analysis. Excluding a posteriori *because* you are not happy with the test result (if you have a better reason that you should have noticed a priori, but failed to, then that is okay, but must be explicitly reported) is plain p-hacking.","This to me seems not that unusual and close to doing a pilot study first, followed by a power analysis to determine desired sample size. Of course, stopping to collect data as soon as statistical significance is reached is more problematic, and more likely going to bias your results.","Well, the converse has also been true in my case; where we had significance using a certain test, but then applied another test and found results to not be significant. SO, I think this is okay as long as 1) the most appropriate test is ultimately converged upon, and 2) you don't have any asymmetry as to which direction you decide to explore the data (i.e. ""Let's stop as soon as we have significance!"")",NA,NA,NA
"338","The used variables will be always reported in the Methods, but then is possible to do not show in outputs of our results if not reach statistical significance","When space/time is limited, only the best stuff can be presented.","This practice makes it easier to read the paper. I do not think this practice is favorable, but I cannot come up with a reason against this practice.","This is cheating should not been used. If a subset of model is presenting it should be advertise and the full set need to be specified at least in the appendix","There are several published analyses that suggest this is rather more common than we would hope. e.g. Ridley et al. (2007) JEvolBiol 20 1082-1089",NA,"To increase the statistical power after any kind of a pre-study is not fault.","when the tests initially used are not the good one and after discussion with statisticians",NA,NA,NA
"339","Theoretically, ""non-significant"" effects are interesting, but it is hard to build a paper based on ""non significant"" effects only, except in situations where failure to reject a specific hypothesis is important. I have never failed to report ""non significant"" effects in a paper when other effects where ""significant"", but I have decided not to write a paper based on ""non-significant"" effects only. There should be space in journals, or a specific place for such results though.","When the covariates are not ecologically or biologically significant and are not primary study interstests - e.g., study sites (or replicates), no need to report them if no significant differences are found.","This practice should be usted because opens doors to new questions.","This is clear lying - stating the test set was one thing when it was another. I think this might be rare simply because few researchers are statistically literate enough to discuss the concept of a candidate model set (though I work in evol biol not ecology).","There is a threshold for a reason, and once it is exceeded IT IS EXCEEDED. / I do not support the use of strict p-values, or such arbitrary thresholds, so I think this is a bit of a silly question.  If you're going to work within a framework, you should work within the framework.",NA,"To make previous exploratory studies should be used frequently, not exactly to know if results are significant but to understand the system and classify potential influencing variables. The problem here is the difficulties to obtain a budget for this kind of assessments.","Who is to say that the first model was correct (i.e. least wrong)? / The whole point of statistical analysis is to find the best (whatever criteria is of interest) model to your data. So not trying several models is unwise. However one could question what evaluation criteria is used. E.g., just looking at coefficients might not be useful, but assessing different methods for fit could be useful.",NA,NA,NA
"340","There's a huge differences among studies where the experiment didn't work so you don't write it up (why bother wasting your time?), studies where you get a null-result (depends on the hypothesis), and reporting a non-significant variable in the model presented (should always do).","When there are many predictors and collinearity among them, some initial analyses might use some predictors that are, at the end, neither used nor reported. This might partly justify this practice. I used to do that occasionally when I was younger and inexperienced, but I try now to select my predictors very carefully before running any analyses, and then stick to them for the report of results.","This practice should not be used as it undermines the scientific method altogether - sometimes we find things we did not plan to find and yet they change our view of a field. Also, it sets a bad example to early career scientists.","This is dishonest and clearly wrong. ","There is always some rounding involved in reporting p-values. The number of digits varies by researchers. I would not round 0.041 to 0.05. I generally report about three digits after the decimal point.",NA,"Totally OK. Just makes sure the non-significance is not itself just a random fact.","Who says that the initially chosen analysis was the best way? ",NA,NA,NA
"341","there are a lot of potential derived metrics from your average data set, you start of with the big obvious ones (say abundance) and then move onto the ever more derived ones (e.g. rarefied SR, eveness, etc).  If all we ever did was look for the obvious and didn't fish around a bit the subtleties of how systems work would be missed on us.  The problem is that for your average paper the journal does not want 10-20 identical tests of subtlety different metrics so you have to pick and chose what goes in.  you try and be representative, but normally it will not be everything.  Likewise with any exploratory analysis, you present the one you ended up using, not every stage to get there.","Whether it should be done depends entirely on the nature of the question. For example, it the question is meant to be hypothesis generating, and you are searching through many variables, it is not necessary to list all possible parameters and statistical values. On the other hand, if the covariates were part of a pre-determined set of variables that were being tested as part of a larger theory, then all results should be reported. That is, the answer is very much context dependent. ","This practice should not be used because it does not reflect the ""optimal"" honest and rigorous process that should underly science, inasmuch as it increases the likelihood of spurious findings, biased effect sizes. Yet, there is a pressure to publish... and journals are more likely to publish success stories...","This is equivalent to dredging through data - statistics should be based on a-priori hypotheses and pre-defined experimental design, which includes the statistical model. ","There is no reason to omit the extra information.",NA,"Typical frequentist problem. Significance depends on sample size. We know that small samples don't say much. We find no significance, it could reflect sample size that's not large enough, so we change it. P-value inflation, but at the same time it can emerge from a real concern about sample size.","Why shouldn't one use different approaches? I do not see the point. Humans fail, or an expert (peer) tells me I could do better... /  / I may discover that I used a model that is not appropriate for the data (classical case try a linear regression, while the data are non-linear) ",NA,NA,NA
"342","There are a number of reasons not to publish negative results. Probably the most common one is that such results can be hard to get published unless they contradict previously published studies or widely accepted hypotheses.  Another reason is that perhaps the study did not have sufficient statistical power to detect a significant difference between treatments, in which case it might make more sense to wait until more data can be analyzed.","Whether or not this is appropriate is hugely dependent on the model selection framework that the authors choose. Whether model selection is itself appropriate, and under what conditions, is an almost philosophical problem - I'm not sure there can be a universal answer to whether this is right or wrong.","This requires some more nuance, I think. /  / Firstly, I find it wrong (and I don't think ever engaged in it) to sell a post-hoc result as a priori question. I see it justified by colleagues as ""you have to tell a story to lead the reader"". I agree about telling a story, but the story should not lie about the true train of events. /  / Secondly, I find it justified to add speculations and post-hockery to the original analysis -- as long as it is clearly marked as speculation and is placed in the discussion, not the results. But that is my interpretation of the role of the discussion. I think in the Good & Hardin book (Common Errors in Statistics) they write that it is the role of the statistician to ""forbid"" post-hockery, but the duty of the biologist to engage in it to further science. Just not sell it as a priori expectation.","This is one form of p-hacking. Unless there is very good reason to do this, it should be avoided.","There is nothing wrong with rounding 0.054 to 0.05. That's how rounding is done. What is wrong is to use an arbitrary criteria like 0.05 for acceptance. ",NA,"Typically it's just not practical or desirable; one might re-design the experiment with larger N, but it would typically be a total re-design and not just a repeat at higher N","Will bias results ",NA,NA,NA
"343","There are already too many papers out there, most of which people do not read before embarking on a study. However, sometimes the aim of the study to show that there has been no impact, in which lack of effect is important.","While I usually keep the covariate in the model no matter what-  I think this is fine to do as long as in your methods you specify that the covariate was in the model and you dropped it from the model if it wasn't significant. There at a long of spurious variable that are good to collect just to ensure they aren't modifying your results but if they don't I have no problem saying that and not wasting a lot of space reporting them. /  / (I also could be reading this wrong and ""not report"" means that the covariate is never even mentioned as being collected v.s. just not used in all the models in which case I haven't done that)","This seems disingenuous - also it does not benefit the paper. It is interesting when results are unexpected and presented as such.","This is part of the data mining, it is useful to get a feeling for your data set when trying lots of diffent ways to connect your data.","There might be cases I am aware of where researchers will in inaccurately portray p values in a less devious way, such as when p = 0.049 reporting p &lt; 0.05. While it is not incorrect, I think researchers may want to overemphasize significance but just reporting ""less than a threshold"". Of the two evils, I suppose this is the lesser. In any case, I know of no cases in my own work or colleagues where p values were purposefully rounded to meet significance. Although I suppose this would be hard to know unless you were actually involved with the stats!",NA,"Unless the authors account for multiple testing on the data, then this is not warranted. ","With discretion - if the model is not a good way to test your data, then it is good to figure out the best test. Particularly when learning new stats, it is easy to not use the proper model at first. ",NA,NA,NA
"344","there are many situation where this is important to not report such results. Notably when the data/experiment lacked the required statistical power. else we would just publish noise. And there is enough noise published. Otherwise, occasinaly this is really a strong negative result, which may not be super interesting (not challenging anything), but not publishing it may result in a publication bias having problematic consequences for metaanalysis. Hence, it looks like there is no general rule, and there is also the issue that publishing negative results is usually much more difficult in most journals (and for little advance or novelty)  ","Whole models should always be reported as well as the way to simplify them.","This should be always avoided, as any lie about the scientific process. If there is formal hypothesis testing, it also contributes to inflation of positive results. In some cases, changing the initial assumptions won't affect the conclusions, and it is more a question of getting the right logic to go through the publication process, a kind of ""white lie"". To be avoided but will be hard to get rid of.","This leads to inflated type I errors. One should always state what variables were assessed. / ","These are really arbitrary thresholds in the main, so this is fairly pointless stuff that just indicates some dishonesty in trying to make results appear more 'significant'",NA,"We are all time- and resource-limited.  Comparing results from different sample sizes can indicate / whether the 'negative' results are sample size-dependent.","would be if the test wasn't appropriate rather than if the test didnt show significance. / also maybe if you used conservative non-parametric tests & then found a way to do less conservative parametric tests",NA,NA,NA
"345","There can be good reason not to publish such results, for example if there are good reasons to think (in hindsight) that there were problems with the methodology or the design. Many researchers won't publish non-significant results based on solid methodologies and designs because they are perceived as harder to publish. This practice is problematic as it leads to publication bias.","Will bias results ","This turns more on personal honesty—as phrased. I think it is fine to suggest in an introduction that a prediction CAN be made (that wasn't initially made at the start of the study), but to actually state that the finding WAS predicted initially when it wasn't is dishonest, in my opinion.","This practice could be used in very binary type situations where there is hard biological evidence (e.g. genetics, development, etc) that there are few options. It should not be applied to behavioural sciences, e.g. behavioural ecology. ","These thresholds are arbitrary anyway, so no need to tune the p-values to meet these artificial thresholds.",NA,"We cannot publish without using this practive when we work on long term dataset (like bird survey).","You have to be careful of doing this, but data analysis is a lens or filter of the raw data. Disclosure needed.",NA,NA,NA
"346","There is a difference between studies and variables. The latter face the same issue as in Q1. As for studies, I have always been able to publish almost everything somewhere eventually. The key is making negative information interesting. ","winner's curse -&gt; false positive rate increased","This will take out the fun of being a scientist. I like surprises, which always lead to new discoveries. ","This practice has the potential of inflating type I error and false discovery rates. This is particularly true when corrections for multiple tests are required.","This is a difficult formulation of the question because there are appears to be two questions: is rounding done numerically in a way you suggest and is the reason for this to obtain specific threshold. I feel that researchers are not rounding p-values in order to meet a specific threshold but rather to follow established rounding rules (though also the first option may naturally happen). So, I think this latter type of rounding must happen often but not for the reason that is indicated in the question. I do not know how to answer in this case.",NA,"We regularly conduce pilot studies to test ideas and then follow up with a full study design based on those pilot results. I think pilot studies are invaluable.","You need to have very good reasons to change the statistics during the study and it should be based on the sampling design. Sometimes, you find new statistical tests which fit better to your sampling design.",NA,NA,NA
"347","There is a general bias in reporting positive rather than negative results, the latter are more difficult to publish. ","You can't avoid it. Without being aware of it, you naturally look for putative covariates explaining secondary patterns.","Those findings are really interesting, but should be reported as exploratory","This practice is just plain corrupt.  See Burnham and Anderson's treatise on model selection.  Any modeling effort has to begin by assembling _all_ the plausible models; likewise the report of that effort.","This is about the same as lying about results, so I hope this a very, very rare practice among scientists...",NA,"well look, significance depends on sample size, so it'll often make a stat bigger but without any real meaningful science associated with it. /  / Of course we get more samples, especially if sample size is small. If we think a certian process is occuring but n = 10 doesn't do it, well let's try more. But if I use 1000 and don't get it and go to 10,000 well forget it. That's like not a meaningful effect","You should explore our data in the better away. P value over rated. Other trings are more important",NA,NA,NA
"348","There is no obligation to report all your findings. If you're deliberately concealing information that contradicts a hypothesis or otherwise would be inconvenient, that is wrong. However, simply deciding a finding isn't worth pursuing or publishing because it's not interesting or would require too much time and money in terms of follow-up experiments to make it worth publishing, is fine and normal.","You can't test things and not report them.","To write a compelling paper, you don't tell the reader all the false starts you took. You tell a straightforward compelling story that focuses on the point of interest.","This practice is misleading as it does not provide information on all models considered in the analysis.  ","This is again a consequence, in my opinion, of a misguided understanding of alternative conceptual/epistemological framework in data analyses. Although we learn in graduate and undergrad courses all about hypothesis testing and P-values, we never learn about likelihood and Bayesian approaches of model selection, for example (this is changing...). Anyway, our mind always work like the last, but we report results (artificially) as the former...This is a nice issue.",NA,"Well, if there genuinely wasn't much data in the first place then fine. But otherwise it is a dishonest practice and an attempt to fit the data to the researchers' hypotheses. ","You should test different models when available and not just settle on the first one",NA,NA,NA
"349","There will be situations where you have limited power, in which case a non-significant result isn't necessarily informative. These are things that I would flag for ""further study"" or suggest ""gathering more data"" rather than publishing outright.","you have to test something to know something","Type I error rate is correctly estimated only if we test a priori predictions","This practice should be used, of course. / ","This is cheating the results. ",NA,"Well, this could be classified as curiosity driven research. Of course, the danger is that not more data are collected when p=.049, but only when p=.051, leading to a small bias. This small bias, however, should also be weighted against having more data, which is generally a good thing, provided that the extension of the study is done diligently.","You should use the appropriate statistical test, not switch to a less-appropriate test to mine for significance.",NA,NA,NA
"350","These data are just as important as the statistically-significant findings.  Also, given the somewhat arbitrary nature of the cut-offs for significance testing, reporting all values can allow readers to better assess the relative importance of all variables.  ","You should always be completely transparent about the full analysis process underpinning the results you are reporting.","Unexpected discovers are a great source of new knowledge. The illusion that science mostly proceeds down expected paths creates an expectation in the public and politicians that research is only valuable if outcomes can be predicted beforehand. ","This practice should not be used because it increases the risk of Type 1 error and publication bias.","This is clearly unethical.",NA,"Well, with really expensive assays, one is always looking for the minimum effective range. Sometimes, if there is a trend towards significance, but the sample size is too small, I have added more samples to see how the data shapes up.",NA,NA,NA,NA
"351","These results are as important than those reaching the threshold. Unfortunately, many journals are not willing to publish studies that show no relationship between variables. But, if we cannot get these results published, other researchers might waste their time and effort by studying the same questions again. ","You should report all results, even non-significant ones, in the main or in the supp. However, I don't think this practice always result from a willingness to mislead (e.g.article length limit).","Unexpected findings are findings anyway and, if they fit in the study aims, I see no reason to report it. However, care must be taken as this practice shouldn't be the rule. In general, I believe ecologists shall try to encompass most of their ideas under clear hypothesis/questions. ","This practice should not be used. Yet, models have to be fitted in order to assess if their underlying assumptions are met. If the assumptions are not met, or collnearity problems are found, it could lead to some models being left out... Although these should be mentioned in the text.","This is dishonest and should never be done. It must be said, however, that the obsession for statistical thresholds and the naiveness of some referees might push dishonest people in doing that.",NA,"What should be used is an inspection of the confidence interval. If the confidence interval is adequately tight - and ideally that is predetermined a priori - then stop. ",NA,NA,NA,NA
"352","This amounts to a fishing expedition.","You should report the effect size and p-value of all parameters in your model","Unexpected findings could bring new research ideas!","This statement raises questions about what work a scientist reports in a given paper. The models that I report in a paper are informed by my entire life-experience, including experiences of fitting exploratory models to the data I'm reporting in the paper, models I fitted to before deciding exactly which filters to apply to the data featured in this paper, and models I may have fitted to similar sets of data.  It seems wise to strike a balance between describing all these vaguely-related models, and only describing the best-fitting model(s). /  / As in the previous Q, there seems to be a suggestion here that some dishonesty is involved, which in my opinion confuses the interesting issue.","This is especially useful when p values appear too strong, e.g. 0.000001",NA,"when testing methods it should be OK.",NA,NA,NA,NA
"353","This biases our entire literature. ",NA,"unexpected results should be reported and discussed so other researchers can pursue or verify them.","This strategy is lying to yourself. I don't see any point in doing this.","This is fradulent",NA,"when there is a clear cost of every replicate (e.g. genetic work to be done) and no prior data on the power analysis, I think it may be acceptable (clear 7:0 outcome from the first 7 replicates - enough; 5:2 - worth exploring further etc.)",NA,NA,NA,NA
"354","This happens in my filed when people measure many different variables. Many related variants of a property might be calculated, and only one selected for reporting in the paper.  I don't know, but strongly suspect, that significance in statistical tests would play a role in deciding which one to keep.  Although i can't remember specifics, I wouldn't be surprised if I did this at least once, in some of my earleir papers. ",NA,"Unexpected results that emerge from exploratory analysis could be presented as formal hypothesis anyway.","This type of analyses should be unfavorable to test an a priori set question. But it is one aspect of nature of practical research to apply various statistical models to the data to understand the characteristics of the data. So reporting results of hypothesis testing based on this practice is not favorable, but it is difficult to avoid this practice...","This is hugely misleading, of course. ",NA,"When this type of testing is done in pilot studies it allows for the use of power analyses to determine how many replicates should be used to determine if a difference exists between groups or treatments. This approach can save time and money while possibly helping scientists to avoid issues with coming to the end of a large study only to find the results less interesting than hoped for and subsequently engaging in activities that introduce bias like P-hacking or rounding P values. If this activity is used to force a result to be ""significant"" and not done in conjunction with power analyses however, I would not support the effort because adding reps or data to simply adjust a P-value will lead to Type 2 errors in many cases.",NA,NA,NA,NA
"355","This is a difficult one - in many studies, multiple variables are tested and found to not be significant, but they are omitted from papers as it would fill the paper with too much detail to put everything in. However, I also believe this information is important and useful in that it communicates to readers what else has been measured and tested... I expect that editors or reviewers would find it too cumbersome to include though so it is often excluded.",NA,"Unfortunately this is the kind of thing that referees don't like: if you find something odd that you didn't expect or predict then that is potentially interesting. That is how we got penicillin! But referees would hate a paper along the lines of ""we looked at the data and found this crazy thing""! ","This was a difficult question to answer. There is a trade-off between statistical best practices and preparing a publication that is practical and clear to read (no editor or reviewer wants to read a diary of all the statistics someone did before hitting on a final presentation version). ","This is just misleading",NA,"when we think that the results need to be reproduced and that the targetted revue asks generally more replicates...",NA,NA,NA,NA
"356","This is a hard question to answer - sometimes you wonder if you tested something incorrectly, designed the project improperly, or this is not worth reporting. However, my professor tole me ""The best paper highlights such problems and puts flashing red lights on them so it is good to report unexpected results.",NA,"Unfortunately, defining strating hypothesis regarding the results is an usual practice. In some specific case it can help clarifying the message of the study, but it is mostly a twisted hypothetico-deductive approach","To avoid statistical biases.","This is just silly even despite my qualms with p values in general",NA,"Whenever possible to add new data, if statistical power is the issue.  ",NA,NA,NA,NA
"357","This is a loaded question, methinks. /  / For excluding covariates, I would do it only for predictive/exploratory studies.  For designed experiments, I would never do it. /  / My answers above are for MET type analysis where the results of previous studies are analysed.  In this situation, publication bias is huge (especially in ecology).  If a result is not significant it is not published.  So, in my opinion, anybody who conducts a MET analysis is inadvertently performing this...",NA,"We learn more from testing predictions and finding they are not supported than from making up a prediction after the fact","To explicitly say that the reported tests were the complete set when there were others seems dishonest. However, when multiple tests are conducted but only one or some are discussed, without an explicit statement of the presence/usage of other analyses, I think that is common and simply the necessary nature of analysis with complex data sets. ","This is kind of fudging the numbers, but on the other hand having a magic threshold for what is significant or not is pretty arbitrary too...",NA,"While it is not ideal, if the variance is large than expected then quitting before you have significant data to draw a conclusion would seem stupid. Having said this continual checking of the p value until a significant result is found is obviously wrong.",NA,NA,NA,NA
"358","This is a tough one. Ideally we would know exactly what we need to measure/test before we run the experiment and then only measure those particular variables.  But we often don't know exactly what we need to measure (i.e. we want to measure 'body size' but do we do this by measuring weight, length, tarsus length, etc?) so we measure everything and then report the variables that seem to be the most predictive (or adhere to our story the best).  I think it is okay to omit non-significant variables from reporting as long as this doesn't change the interpretation or outcome of your study. ",NA,"We should report all results, particularly those unexpected.","To me this is the same as scenario 1.","This is not a big issue, but it should be simply written as it is.",NA,"While this can be a problem, and can lead to the manufacture of significance, it can also be used appropriately. I have no problem with people finding a significant p value of 0.048 and then deciding to get more data to see if the effect really exists (or a non-significant one of 0.052- readers of the paper are going to want to know if this is a real effect or not, and more data is needed to test this, so repeat the experiment). This is an important part of science. The key thing is to admit it in the MS.",NA,NA,NA,NA
"359","This is also very poor practice, but can be justified in the case of correlated variables, and in the case of variables failing to meet statistical requirements, like normality. In this case, the rejection of such variables should be noted in the paper, but the variables might individually be left out of the paper.",NA,"Well, it might not be a problem to present results form exploratory analyses, but why not simply state that correlations were tested.","Transparency on what was tested is required, unless the testing was only for the purposes of eliminating correlates of terms that remain. ","this is not accurate to the meaning of the threshold.",NA,"why not",NA,NA,NA,NA
"360","This is an issue with journals not accepting null results.",NA,"Well, the problem here is distinguishing between a totally unexpected result and one that causes the researcher to slightly change the initial hypothesis - in many cases, the difference is not clearcut.  Especially if the novel result then leads to a new prediction which itself can be tested and lends support","Typically I test multiple models when I'm getting a feel for my data set. If the models agree with one another, I choose the one that most accurately reflects the structure of the data collection and organismal natural history. ","This is really splitting hairs.",NA,"Will bias results ",NA,NA,NA,NA
"361","This is cherry picking, and it is not a honest report of the truth.",NA,"well, this is a difficult one - in the statistical sense, this should not happen, but in current times scientists are forced to market their work as best as possible and this is one way to make it more publishable.","Using different approaches of equal validity is obviously useful. Not reporting concordant results is probably mostly for the sake of brevity. Not reporting discordant results is dodgy. ","This is scientific misconduct.",NA,"With Bayesian analysis, there is actually no problem with this practice. More data is always good. I think the field as a whole should move to more Bayesian data analysis. However, the primary reason for not collecting more data after doing some analyses is logistical. In most of my work, it would not be feasible to collect more data after beginning the analysis.",NA,NA,NA,NA
"362","This is dredging. ",NA,"when finally, we understand the result with an other point of view that we haven't see before...","We may test lots of different models that end up being useless and would involve longer methods sections with little interest.","This is slightly missleading",NA,"You're describing a pilot study???",NA,NA,NA,NA
"363","THis is similar to the answer to one of the other questions - ecology is a such-it-and-see discipline at times and we sometimes (particularly in student-projects) use a scatter-gun approach to sampling so the students have something to write-up. Hypotheses are honed through the process.",NA,"When running an analysis takes weeks or months, we seldom have the opportunity to build many models; we have to think very hard about the models we will build and we usually have to make hard decisions a priori. Every possible result is assessed before running models to make sure we don't waste weeks. /  I believe that this practice is more common in situations where people have small data sets and can build a large number of models. They might ""find"" interesting results by chance. / Also, I believe that your question strongly depends on the context: ""to predict or to explain"" (Shmueli 2010)? If the goal is to achieve a very good predictive power, perhaps ""unexpected results"" are not an issue? It is well known that the two approaches won't lead to the same set of ""selected variables"". / My answer is perhaps Okay outside the framework of data mining (which I have not used for my own research).","We must explore data to find interesting and potentially meaningful relationships, especially in large data sets. As well as to ensure the data are not confounded. It is not possible to publish a coherent paper that shows all the analyses explored. ","This is sloppy, incorrect, and can lead to false interpretation of results. This should never be done. When the reader sees a ""P=0.05"", they will conclude that the researcher was either sloppy, or tries to prove/disprove a hypothesis without the statistical backing up. / My earlier answers relate to the researcher having to strike a balance between what to present and/or not to present out of potentially myriads of research findings, and having to make an informed albeit somewhat arbitrary decision. With the present point, I believe there is a clear yes/no answer.",NA,"You can use a power test to decide if more data need to be collected. I don't do this often.",NA,NA,NA,NA
"364","This leads to publication bias. Researchers would like to publish these studies, but those without statisticallly significant effects are often rejected by journals.",NA,"While I have great sympathy for the temptation to restate results as expectations, it is somewhat misleading and in most cases unnecessary.  Although it may enhance the clarity and succinctness of the resulting MS, it also potentially misleads the reader into assuming a more robust test than is actually present.  That said, I'm not sure the difference is substantial and this appears to me to be more a question of editing and semantics than deception.","We should take enough time to think about the models to test and report all the results about them, instead of trying different models, fishing for significant results.","This is utter fraud.",NA,"You may need to increase sample size, or your first study was preliminary.",NA,NA,NA,NA
"365","This one is difficult, because we do so many pilot studies. If clearly and honestly identifued as such (I.e., with no intent that publish), thus is essential and fine, but otherwise it leads to lots of publication bias. ",NA,"While it can be tempting to present findings as part of a narrative in which those findings were predicted, transparency about predictions is important because it helps readers to evaluate the biases, methods, etc. of the author more accurately.","We should try to embrace the fact that different models provide different outputs, and consider them when reporting variable significance and effect size.","This is very poor statistics and should never be used.",NA,"You should always use pilot data (maybe a supervisors understanding of the variability in the type of data being collected) to inform a sample size, then maximise that and interpret in relation to an effect size.",NA,NA,NA,NA
"366","This practice may be used only if the failed studes are not experimentally flawed.  ",NA,"While we go into studies with a priori expectations, science is really about discovery. Sometimes you discover something unusual! then the first step is that you go back into the lit to see if anyone else has found something similar. usually, someone has. Then it makes more sense to write up your results from that perspective, even if you did not enter into the study with that in mind.   /  /  ","We test lots of models in exploratory analysis that would never be published, just to get an idea of what the data looks like. However, deliberately concealing models that are inconvenient is bad practice.","This just seems dishonest.",NA,"You should consider the error in the original estimate of P...",NA,NA,NA,NA
"367","This practice should not be used because it does not reflect the rigorous reporting of results that should underly scientific investigations, and even lead to biased reports of effect sizes (e.g., when performing meta-analyses). ",NA,"Why would anyone want to claim that they predicted something that they hadn't? Is it an ego thing? I seems like it's more exciting to find something you didn't expect! I can see how someone would do this though, in writing a manuscript and trying to build the story that supports the results, thus building the underlying information into the introduction. If you get something out of left field, do you build it into your original research idea or do you introduce it in the discussion? It's a tough, murky question to be sure...but I wouldn't want someone to claim they predicted it, especially if there is nothing that could have led to their prediction. I could understand a young scientist doing this (e.g., someone working on their master's thesis, with little experience and not having full exposure to the literature). Again, it's murky...","Well, particularly when a superior model may have been ignored.","This practice has been widely accepted and easyly compared with different researches.",NA,"You will always reach significance provided you continue to increase sample size. (if you are doing sequential sampling then there are analysis methods to deal with that, quatlity control type methods, say)",NA,NA,NA,NA
"368","This practice shouldn't be used mainly because it strongly affects the results of meta-analyses (publication biais, see Moller and Jennions 2001; Lortie et al 2007). ",NA,"Will bias results ","What I have done is not report in any detail any other models but have said that others were tested, how they differed from what was reported and whether they showed anything other than what was reported  (in that case they did not show anything different).  This gets to be a bit of an issue if you are considering different ways of analyzing results and both methods give similar results.  ","This practice is not fair. The exact p-value should be reported.",NA,NA,NA,NA,NA,NA
"369","This Q is hard to interpret. SOmetimes data are just messy and unpublishable.",NA,"With the new emphasis on data exploration, it is ok to see what the data tell you without making specific predictions in the first place. We need to realize that the philosophical underpinnings of science have evolved since Popper in 1912.","What researcher has not looked at his/her data and thought: the relationship is not linear, or the distribution of the error is not normal, or there is clearly spatial autocorrelation in the data, or ... etc. ...?    Does that not mean that s/he thus entertained and rejected a large number of models, without having formally proposed them?  ","This practice is sketchy though the p &lt;= 0.05 is arbitrary anyway so I don't feel that strongly about it. I prefer presenting probabilities (using Bayesian methods).",NA,NA,NA,NA,NA,NA
"370","This question covers a multitude of cases. One should report non-significance when one expected significance. And, if testing a variety of possible correlates, one should say which were tested as well as which were significant. This provides some perspective into which to set later work.",NA,"Working with large datasets it is impossible to anticipate all possible outcomes. I don't see a problem with benefiting from serendipity and following up interesting but unanticipated results.","When deciding on my models, I often try a bunch out (examples: sometimes try different nonlinear equations, or link function, or transformed variables) to see which ones converge properly and how each matches to the statistical assumptions before deciding on one or two types to test in depth (actual parameter selection and testing of random effects). The ones I test in depth are reported.","This practice mislead readers and should always be avoided. ",NA,NA,NA,NA,NA,NA
"371","This question is difficult, because ""studies"" and ""variables"" are so different from one another. With studies, it's very understandable not to try and publish some negative results, given the time investment required. For variables, see my response to last question.",NA,"yes very bad practice...","When dozens of models were evaluated, leave out the less predictive ones as long as the ones published are a fair representation of the variables examined.","This practice should never be used because p &lt; 0.05 or 0.01 have no special reality, and is just an agreed-upon construct. Obviously p = 0.051 is only slightly different than p = 0.049 and really no more likely to not represent the null hypothesis. Nonetheless while I have never rounded off a number to get to a threshold, I have spent paragraphs arguing why I think the p = 0.051 result is still important!",NA,NA,NA,NA,NA,NA
"372","This question is worded oddly, so I am not sure I understand it. By my understanding it is asking if I don't report non-significant results, so only report significant ones. I don't do this. It's important to report all your results, otherwise the context of the significant results is skewed. ",NA,"You might not have anticipated it but it was predictable if you had known the theory that predicted it. After getting a result that you did not expect, and conducting further research into why that result was obtained, you discover a theory that you had not known and realized that the result was, in fact, predictable.","When exploring the data, I often analyze the data using different linear models. I just tested a set of models with different measures (counts, proportions, etc.) used to define the factors of interest to see if results were consistent across measures. I see no harm in featuring one set of models and mentioning the others that were tested, as well as in explaining the results of model-building (using analyses of deviance) about whether or not the models were improved by adding other factors.","this practice should not be used because it is not honest.",NA,NA,NA,NA,NA,NA
"373","this question made me wonder do you mean whether in a paper published on a subject and there are results that are not significant and by being so these make your argument less robust - in which case I have not left them out but I suspect that others do and that is how I answered the question. / If you mean do you try to publish papers that are effectively zero results as in ""We tried it and it did not work""  then I most certainly do have things in my cupboard that fall into this category and I am betting most ecologists do.  Having struggled to get the time to write up the work that does show something I am sure we all let the ""It did not work"" section fall by the wayside.",NA,"Your asked to tell a nice story to reach high level publication ...","When I analyze data I made many models.  I think it is the nature of analysis to explore many different approaches. I sometimes make a model, but then decided that model is not biologically relevant, or is not testing what I want to test.  I do not include that model in the final publication.  I only include models that I think are relevant to the question.","This practice should not be used... if one adheres to a Neyman-Pearson view of inference. However, if one is more Fisherian in his approach, it should not have a big impact as there is no purpose in using such a significance threshold (and given that the precision of the of estimates we obtain is often not high enough to justify highly precise p-values). Anyhow, I believe that p-values should probably be used only in experimental studies.",NA,NA,NA,NA,NA,NA
"374","This question needs more focus to distinguish between planned studies that produce negative results and analysis of all sorts of ad hob relationships, correlations, etc. that might be done or considered in exploratory data analysis.  The former are important to report; there is no way to fully document the latter",NA,NA,"when some of the models explored during the data analysis are dead ends, particularly when they go too complex to have biological meaning. ","This practice would only be incorrect if rounding is only applied to situations where it resulted in the appearance that an alpha threshold had been reached. If on the other hand, this rounding was applied to all p-values then I don't see it as dishonest, and I have in fact been required by some journals to apply such a rounding practice to all p-values which did end up having the effect shown above.",NA,NA,NA,NA,NA,NA
"375","This really depends on the context - if the variable is part of a central hypotheses being tested it shouldn't be dropped",NA,NA,"When testing hypotheses, or doing e.g. forward or backward multiple regression analyses, you test lots and lots of models - the combinations of variables can be huge, and therefore the total number of candidate models. You often look at lots of models and see if they make sense - statistically, but also biologically. When writing your paper, you can't include all models tested - it would make the paper extremely long and very hard to read - note that as scientists, we write for the reader and to communicate our key findings - too much research is going on globally to allow us to publish all model combinations we have examined. This is why we often do not report all candidate models tested, but only the final and most plausible ones (and perhaps the null model or some other that contain useful information to interpret our findings). When selecting which models to present, we should led ourselves be guided by honesty, integrity, and common sense.","This question does not make sense. It does not make sense to round to the threshold as it is still not less than the threshold and thus not statistically significant. If alpha=0.05 and your p-value is 0.05 it is NOT significant so there is no reason to round.",NA,NA,NA,NA,NA,NA
"376","This seems akin to the previous question, although perhaps more pertinent to meta-analysis? In which case, it should *never* be used in meta-analysis",NA,NA,"when the previous tests are finally not the good tests to do","This question highlights a broader problem which is the focus on p-values and arbitrary ""significance"" cut-offs. I believe p-values should always be reported exactly and interpreted in a continuous, not dichotomous manner.",NA,NA,NA,NA,NA,NA
"377","To avoid accumulating too much information in papers, this may blur the main message of papers",NA,NA,"When using model selection, it is not necessary to report the results of every model, especially when undergoing multiple model selection procedures for multiple response variables.  This is especially true when performing model selection on variance terms in the model and not the fixed terms.  If model selection is being performed for only a single response variable and fixed terms are the subject of selection, then this is a case where you might want to report all candidate models.","This really depends on how the result is reported.  / If I set an alpha to 0.05 for my analysis, and I get a p=0.054, and my dataset is small (this is rare, as I usually deal with epidemiological/disease ecology datasets which are generally large), I may well report the P as 0.05. / However, in my interpretation I would highlight that this variable is a borderline case and the reader should be aware of the potential for falsely rejecting the null. / The threshold chosen and the reporting will depend on the dataset size - if one has a large dataset, then one perhaps should consider a more stringent alpha of perhaps 0.01.  /  / I think where this issue really arises, rounding off for significance, mostly occurs where the dataset is small. In such cases, readers should be worried in general about the validity of the findings, irrespective of the size effect. /  / Almost in all cases, however, I report p-values to 3 decimal points so this really doesn't arise in practice. ",NA,NA,NA,NA,NA,NA
"378","To me this again depends on the research question and the way a test/model is set up...I don't have a problem to report variables that are not significant and I don't mind if other researchers show such results (those often stimulate heaps of discussion).",NA,NA,"while model selection criteria have helped us a lot, sometimes one needs to consider correlated variables, or substituting a continuous approach for a discrete approach. For example, one might treat an individual tree as a random factor in a model, OR if one has a hypothesis that percent nitrogen in tissue  is the mechanism underlying variation in the response variable, one might use   a separate set of models in which pct nitrogen in tissue for each individual is a continuous covariate to explain response variable and not include individual plant. Then, we might present this as the model in the paper but state in the text that it was  a better model fit than the other (without necessarily presenting the second model in the supplementary info). ","This seems entirely reasonable given that some editors insist on using a set number of decimal places when reporting statistics",NA,NA,NA,NA,NA,NA
"379","To my mind, this is scientific malpractice. But try telling that to the reviewer whose sole comment on my manuscript was that most of the results in this paper are non-significant, the author should send this to Journal of Negative Results. For most journal, such papers would not even be reviewed, unless they are from very big places.  ",NA,NA,"Why bother using a statistical practice if you're not going to stick to it?","This shouldn't be used.  There are 2 reasons: / 1) it is 'stretching the truth' just to fit into some arbitrary values / 2) the significance thresholds are arbitrary.  In my view a p-value of 0.054 provides pretty well as much evidence as 0.049.  If a researcher does not realise this, and can hence write about it eruditely (perhaps even persuasively, then they need to take a step back and look at the broader issues.",NA,NA,NA,NA,NA,NA
"380","to save time",NA,NA,"Why would anyone do this? This doesn't even make sense.","This used to be much more common before modern software programs where p-value was estimated via a table. This was fine then, but there's no reason not to report the actual value now other than a perhaps false sense of keeping things simple and easy to interpret.",NA,NA,NA,NA,NA,NA
"381","To some degree you have to curate the data presented or it becomes untenable for the reader. If I am taking soil moisture measurements occasionally across the experiment to ensure that there aren't experimental artifacts in soil moisture, in the paper I am just going to state that it was monitored to ensure equal moisture not present a complex repeated measured model result that is non-significant for something that isn't the point of the paper but is just good experimental practice.",NA,NA,"Will bias results","Threshold-based inference based on p-values is antiquated and silly.",NA,NA,NA,NA,NA,NA
"382","Tricky question: perhaps your 'good' result is that two metrics are not statistically different..",NA,NA,"You have to report your practices accurately.","Thresholds are arbitrary, and whoever has an understanding of was p-values mean should not care much about the rounding.",NA,NA,NA,NA,NA,NA
"383","Typically I don't use frequentist methods so I usually report the credible interval or effect size of all variables, but have worked with people who only wanted to report the 'significant' effects.  I think in general all of the effect sizes should be available (non significant variables should still be reported in an appendix at least).",NA,NA,"You should always report the complete set of tested models, including both candidate and final models. You must report what you tried, even if it didn't work.","Thresholds are arbitrary, so are number of decimals to report.  / I will comment in the same way p=0.048 than p=0.052.  / Similarly, I am not interested if p=0.012 or 0.008 because the conclusion is similar.  / ",NA,NA,NA,NA,NA,NA
"384","Un significant results should be published constantly as a part of the manuscripts as long as they represent the study.",NA,NA,"You should always report the full candidate set of models.  Sometimes you may only focus on a subset of models because they of problems with convergence, or there was little support (e.g., AIC model weights) for other models, but this should be made clear.","Transparency, and because significance cutoffs are rather arbitrary. A value close to a threshold is still suggestive of a relationship/effect. Instead of rounding, authors should report marginally significant results and discuss sources of error/noise in the discussion that, if controlled for/addressed, could lead to better tests of a hypothesis. ",NA,NA,NA,NA,NA,NA
"385","Unfortunately, studies that result in no statistical significance are difficult to publish because they are hard to write (boring) and folks aren't going to be that interested in reading so it will get killed by reviewers",NA,NA,"you should report the complete set of models tested, even though you might not present detailed results for all of the variables used (previous question). In my opinion, there is nothing wrong in exploring the data using different models and perspectives.","tresholds are useless",NA,NA,NA,NA,NA,NA
"386","Unfortunately, these generally get trimmed out of manuscripts where supplemental information isn't available. It would be best to always report all non-significant results, but I feel space limitations squeeze these out. ",NA,NA,"You should report what you have done. It can be ok to test more variables than reported","Unfortunately, I believe that the question is not phrased very clearly. Rounding 0.054 to 0.05 is mathematically correct (.0 - 0.4 is rounded down, .5 - .9 is rounded up). If someone presents a rounded value of 0.05 it is clear that this value is between 0.45 and 0.549999... .The question however states that the researcher rounds ""to meet a pre-specified threshold"". This is the part my answers apply to, because mathematical laws of rounding should never be broken just to get to a threshold.",NA,NA,NA,NA,NA,NA
"387","Uninteresting results can't be published.  It's not about should or shouldn't.",NA,NA,"You should use different statistical models to see how the data fits. Ideally, we could report all the types of models that we tried in our papers, but this would never get past peer review. I think exploratory modelling is an informative and useful practice. I don't support misrepresenting statistics, or declaring that only a single model was used when many were, but I think everyone runs analyses that aren't in the final paper. That's not because people are dishonest, but because journals do not permit superfluous exploratory modelling to be included in manuscripts. I'd have no problem included mine there if it was possible.","Unless it is specified (""We rounded off all p-values at the closest...""), reporting a rounded value is wrong. With p-values it is particularly bad as it increases the risk of artificially reporting significant results.",NA,NA,NA,NA,NA,NA
"388","Usually manuscripts have length limits and often only the ""exciting"" results are ""publishable"". ",NA,NA,"Your underlying assumption that all relevant models are pre-identified prior to even seeing the data is incorrect.","unneccesary and misleading - just say it was almost significant, thats fine.  thresholds are arbitrary anyway",NA,NA,NA,NA,NA,NA
"389","Usually this is a consequence of journals not accepting null results rather than a willful decision by the researchers",NA,NA,NA,"Using p-values as a be-all-end-all cutoff doesn't make sense in the first place. Just report the p value and let the reader interpret it accordingly. p&lt;0.04999 is not better than p&lt;0.050001.",NA,NA,NA,NA,NA,NA
"390","Variables are aften related to specific hypothesis we want to test. Because space is very limited in regular papers, one must focus on testing some of the possible hypothesis. Also, the reliability of variables should be accounted for, because sometimes they fail to reach significance because they are too broad of incomplete",NA,NA,NA,"Usually, researchers will indicate a 'close' to significant at level of  XX when showing p=0.05.",NA,NA,NA,NA,NA,NA
"391","Variables versus studies are completely different - these should not be conflated. Not reporting variables in a study is a type of p-hacking, and there's usually no reason to not include these variables, especially response variables. Not publishing negative studies (especially exploratory projects) is a bit different - many small projects get abandoned because there is no pattern or clear result, or results were confounded, and these studies usually have too low power to be definitive ""negative"" results. As such, there is little reason to publish such results, as well as few journals to publish such results in. In ecology, this typically isn't being done because the study was robust but there's no significance, but that the study was messy and not informative.",NA,NA,NA,"We ALWAYS round off P values to significant digits so I don't understand this question at all.  Rounding begins with the outputs given by the statistical software.",NA,NA,NA,NA,NA,NA
"392","We always say that no result is a result, so no significant result should always be reported - if that is directly related to the tested hypothesis. / Though, I could understand that under certain circumstances not significant studies or variables would not be reported, e.g. if we have doubt about the data set, or about the methodological framework used to perform the analysis... Nevertheless, that is part of Science to be critical about our work, and criticism about the data or methods or results, is globally positive.",NA,NA,NA,"We have to have standardized practices so that everyone is 'playing by the same rules'.   A rounding / off is open-ended; at what point do you stop?",NA,NA,NA,NA,NA,NA
"393","We are lacking negative results!",NA,NA,NA,"we need to know the real P value not a rounded value even if it can be estimated based on the F and the degrees of freedom",NA,NA,NA,NA,NA,NA
"394","We need to know negative results as well as positive ones.  To withhold a result is to not report the full story.  Perhaps the PI needs to/ wants to repeat the study.  Then they should delay the reporting /  of all associated parts of the study too.  ",NA,NA,NA,"we should be as precise as we can",NA,NA,NA,NA,NA,NA
"395","We should report all results including no statistical significance",NA,NA,NA,"We should report p values accurately, and in conjunction with other metrics that can help interpret potentially marginal outcomes (e.g. in association with effect sizes).",NA,NA,NA,NA,NA,NA
"396","Well designed experiments are typically publishable regardless of results. However, they often have to be low-priority (e.g., we might run a pilot study or collect data to see if it's worth writing a grant on a topic - if there's nothing there, time is best spent elsewhere). Journals are often hesitant to publish these as well.",NA,NA,NA,"We shouldn't be using p values at all.",NA,NA,NA,NA,NA,NA
"397","Well, sometimes the sample size is just too low, or there were problems with the protocol. Happens with undergrad research. ",NA,NA,NA,"We sometimes use p-values to estimate standard deviations in meta-analysis and it's infuriating when people either round-off or dismiss p&gt;0.05 without any further details. I think the scientific community is equipped to understand the p-value thresholds, and those who don't probably shouldn't be looking at them. Definitely think this should be discouraged (it was encouraged when I was a student).",NA,NA,NA,NA,NA,NA
"398","When analysing complex ecological datasets (this includes naturalistic experiments etc where a lot of information has been collected) then including EVERY component that was analysed (non-relevant non-significant as well as significant results), in some journals, would reduce likelihood of publication and reduce readership due to making paper far-too dense. The increasing prevalence of large online supplementary materials should nullify this issue now (and so I think means that the results should be included in some situations), but it has certainly been an issue in the past, particularly with exploratory, rather than hypothesis-based, enquiry.",NA,NA,NA,"We tend to report the exact p-values, unless very small",NA,NA,NA,NA,NA,NA
"399","When conducting meta-analysis, this practice always conducts to over-representation of significant results. However, the absence of significant result is so hard to publish that people (me included) finally don't even try to publish it. The absence of negative result in publications doesn't indicate that it wasn't tested, only that editors don't select them for publication, because they consider them not enough sexy.  ",NA,NA,NA,"Well, you have to round somewhere, and on rare occasions, it might put you in this case (albeit hopefully to a lesser extend than in your examples, like 0.05001 ~ 0.05); but if you do it intentionally to rescue a significant result, that is dishonest and childish. / In general, I do not see why not to report p-values as any other quantity, with a fixed number of significant figures. I see p-values as continuous measures the weight of correlation/evidence, and I do not use hard-thresholds in my research. Therefore, excessive rounding artificially increase the weight of evidence; that is simply cheating and harmful to scientific research. ",NA,NA,NA,NA,NA,NA
"400","When the statistical test in question was post hoc or unrelated to the purpose of the research.",NA,NA,NA,"When I get a p-value like 0.0000000000009, I will write p-value &lt;0.001. Except for gene sequencing study, I do not see the use of reporting a P-value like 9 E10.",NA,NA,NA,NA,NA,NA
"401","when the variables don't present interest in the topic",NA,NA,NA,"When I read p=0.05, I assume that what is meant is p&lt;=0.05; rounding down is misleading.",NA,NA,NA,NA,NA,NA
"402","When used, it should never be BECAUSE it was not statistically significant.",NA,NA,NA,"Whether or not you round off is less important than the interpretation given in the text of the study, including assessment of effect size and generality of conclusions.",NA,NA,NA,NA,NA,NA
"403","When you are doing exploratory work, instead of the usual hypothesis testing it is easy to fall into this category and leave out non-significant results that do not matter or are no longer relevant to the other results that you have found. However, if you are following the hypothesis testing standard you should be reporting all statistics resulting from your specific research question.",NA,NA,NA,"Why bother rounding?",NA,NA,NA,NA,NA,NA
"404","While I think non-significant results should be published, many journals won't publish null results because they believe they are not of general interest.",NA,NA,NA,"Will bias results ",NA,NA,NA,NA,NA,NA
"405","While not all exploratory analysis can fit in most papers, selective reporting of variables in planned analyses can be misleading. ",NA,NA,NA,"You miss a category in c) Dosen't matter. / This level of detail adds little to the interpretation and conclusion of a study. The mathematical statistics are guides when interpreting the biological statistics/significance.",NA,NA,NA,NA,NA,NA
"406","Why should one not report n.s. data? I would certainly not fill a statistics results table with four p values and 20 n.s. entries. I would slim the table as appropriate and either add a foot-note or add a text statement that xyz data were tested not significant (data not shown), simply for space saving and readability. ",NA,NA,NA,"You should always report de exact P value",NA,NA,NA,NA,NA,NA
"407","Will bias results ",NA,NA,NA,"You should provide your p-value without rounding-off, and the effect sizes",NA,NA,NA,NA,NA,NA
"408","You should mention these variables, saying that they were not significant",NA,NA,NA,"You should report the results you get. P values have limited use but should be reported accurately within the correct context",NA,NA,NA,NA,NA,NA
"409","Your question doesn't permit us to give the circumstances. In my view, analyses should usually have two components: 1) testing a priori predictions and important variables; 2) Data-dredging in which other patterns are noted and commented upon if interesting (but the way in which they are discovered should be made clear). Variables under (1) should always be reported. It is not so crucial that variables under (2) are reported, as it depends on study goals.",NA,NA,NA,"You should try to represent your data as clearly and fairly as possible - you can't change standard rounding practices simply to reach the numbers you want.",NA,NA,NA,NA,NA,NA
"410",NA,NA,NA,NA,"Your wording is again critical: ""rounding to meet a pre-specified threshold..."", which I take to mean with the intention of meeting a specified significance criterion.  It is always the case that P-values and other statistics are rounded to some level of precision - and this is not just acceptable practice, but a logical necessity.",NA,NA,NA,NA,NA,NA
"411",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"412",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"413",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"414",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"415",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"416",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"417",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"418",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"419",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"420",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"421",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"422",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"423",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"424",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"425",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"426",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"427",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"428",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"429",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"430",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"431",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"432",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"433",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"434",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"435",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"436",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"437",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"438",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"439",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"440",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"441",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"442",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"443",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"444",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"445",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"446",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"447",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"448",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"449",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"450",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"451",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"452",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"453",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"454",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"455",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"456",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"457",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"458",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"459",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"460",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"461",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"462",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"463",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"464",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"465",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"466",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"467",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"468",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"469",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"470",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"471",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"472",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"473",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"474",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"475",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"476",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"477",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"478",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"479",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"480",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"481",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"482",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"483",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"484",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"485",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"486",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"487",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"488",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"489",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"490",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"491",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"492",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"493",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"494",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"495",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"496",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"497",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"498",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"499",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"500",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"501",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"502",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"503",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"504",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"505",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"506",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"507",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"508",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"509",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"510",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"511",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"512",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"513",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"514",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"515",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"516",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"517",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"518",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"519",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"520",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"521",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"522",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"523",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"524",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"525",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"526",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"527",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"528",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"529",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"530",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"531",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"532",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"533",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"534",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"535",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"536",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"537",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"538",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"539",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"540",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"541",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"542",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"543",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"544",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"545",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"546",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"547",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"548",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"549",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"550",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"551",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"552",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"553",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"554",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"555",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"556",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"557",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"558",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"559",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"560",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"561",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"562",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"563",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"564",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"565",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"566",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"567",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"568",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"569",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"570",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"571",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"572",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"573",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"574",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"575",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"576",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"577",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"578",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"579",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"580",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"581",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"582",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"583",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"584",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"585",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"586",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"587",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"588",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"589",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"590",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"591",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"592",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"593",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"594",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"595",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"596",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"597",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"598",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"599",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"600",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"601",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"602",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"603",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"604",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"605",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"606",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"607",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"608",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"609",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"610",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"611",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"612",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"613",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"614",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"615",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"616",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"617",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"618",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"619",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"620",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"621",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"622",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"623",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"624",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"625",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"626",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"627",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"628",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"629",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"630",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"631",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"632",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"633",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"634",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"635",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"636",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"637",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"638",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"639",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"640",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"641",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"642",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"643",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"644",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"645",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"646",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"647",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"648",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"649",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"650",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"651",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"652",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"653",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"654",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"655",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"656",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"657",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"658",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"659",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"660",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"661",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"662",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"663",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"664",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"665",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"666",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"667",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"668",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"669",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"670",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"671",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"672",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"673",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"674",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"675",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"676",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"677",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"678",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"679",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"680",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"681",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"682",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"683",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"684",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"685",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"686",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"687",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"688",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"689",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"690",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"691",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"692",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"693",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"694",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"695",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"696",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"697",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"698",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"699",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"700",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"701",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"702",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"703",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"704",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"705",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"706",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"707",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"708",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"709",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"710",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"711",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"712",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"713",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"714",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"715",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"716",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"717",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"718",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"719",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"720",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"721",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"722",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"723",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"724",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"725",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"726",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"727",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"728",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"729",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"730",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"731",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"732",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"733",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"734",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"735",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"736",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"737",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"738",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"739",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"740",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"741",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"742",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"743",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"744",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"745",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"746",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"747",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"748",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"749",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"750",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"751",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"752",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"753",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"754",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"755",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"756",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"757",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"758",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"759",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"760",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"761",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"762",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"763",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"764",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"765",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"766",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"767",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"768",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"769",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"770",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"771",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"772",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"773",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"774",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"775",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"776",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"777",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"778",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"779",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"780",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"781",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"782",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"783",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"784",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"785",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"786",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"787",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"788",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"789",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"790",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"791",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"792",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"793",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"794",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"795",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"796",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"797",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"798",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"799",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"800",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"801",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"802",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"803",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"804",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"805",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"806",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"807",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"808",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"809",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"810",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"811",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"812",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"813",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"814",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"815",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"816",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"817",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"818",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"819",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"820",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"821",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"822",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"823",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"824",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"825",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"826",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"827",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"828",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"829",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"830",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"831",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"832",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"833",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"834",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"835",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"836",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"837",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"838",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"839",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"840",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"841",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"842",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"843",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"844",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"845",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"846",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"847",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"848",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"849",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"850",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"851",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"852",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"853",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"854",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"855",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"856",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"857",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"858",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"859",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"860",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"861",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"862",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"863",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"864",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"865",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"866",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"867",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"868",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"869",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"870",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"871",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"872",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
"873",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
